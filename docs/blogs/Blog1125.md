# Blog November 2025

<style>
  .flex-container {display: flex; gap: 20px; align-items: flex-start;}
  .column {flex: 1 1 0; min-width: 0;}
  .column--right {border-left: 1px solid var(--md-default-fg-color--lightest); padding-left: 20px; }
</style>

**LINKS**

[Exploring Workflows](#exploring-workflows)  
[That Easy Fix](#that-easy-fix)  
[FluxGym with 16 Images](#fluxgym-with-16-images)  
[Flux Kontext Prompts](#flux-kontext-prompts)

---

[Main Page](../index.md)

<hr style="height:10px;border-width:0;color:pink;background-color:pink">








## Exploring Workflows

**02.11.2025**  
Generally speaking, I've had less than stellar experiences trying other people's workflows. If they're simple enough for me to get my 70-odd (emphasis on the '*odd*') year head around it, I can generally get things to work. But sometimes things are a bit too bespoke and I get lost in the weeds. Go to install nodes or models or stuff the workflow asks for and come to find out that I'm on the wrong version of Python.  
And basically stuff up my ComfyUI install and have to start from scratch.

ComfyUI *is* becoming more robust, so stuffing up one's install has become less of a problem.

---

Whilst the fact that 'my' [little workflow on Reddit](https://www.reddit.com/r/comfyui/comments/1okk9xg/persistent_consistent_characters_in_flux_srpo/) has enjoyed, to date, 3200+ views and 8 likes, no one is asking for help with it. The only comments on the page are mine. Which tells me: no-one is interested in static images anymore, they want video.  
Fine, whatever. Done my bit, shared with the community.


<a href="/assets/images/blog25/06-01VictGirl.jpg" target="_blank" rel="noopener">
<img src="/assets/images/blog25/06-01VictGirlb.jpg" alt="Victorian Girl" width="360" align="left"></a>

<a href="/assets/images/blog25/06-02VictGirl.jpg" target="_blank" rel="noopener">
<img src="/assets/images/blog25/06-02VictGirlb.jpg" alt="Victorian Girl" width="360" align="right"></a>


A lot of what I'm learning is on the ComfyUI subReddit. So, I found a new workflow for SRPO today, something I wasn't looking for, but -- **WOW!** -- am I glad I found this one.

It wasn't all that straight-forward going, gettting this workflow working. Had to install a few nodes -- *as you do* -- and then, tried to work out why 'Flash-Attention2' wasn't working. And found out the I'd need a more recent flavour of CUDA (the NVidia driver) and-and-and... nah -- not going there. Had another look at the options for that section of the workflow, and found something alternative to Flash-Attention2 that I already have.

Now, this workflow was developed **AGES** ago (which means a few weeks or months, not *years*) so a node has disappeared -- the ImageSmartSharpen node, used to be part of the ComfyUI Essentials nodeset -- so you can't even install it.

Here's where Reddit comes into its own. A user (*Nevereast*) posted this:

"*For anyone who also googles this: you can fix this yourself: you can browse the repo and view the history of images.py and find the missing component (ImageSmartSharpen) was removed in the latest release. I just copied the code block from the version prior to the release and put it in the corresponding file in my local environment. There should be three changes: one for the component, and two pointer variables overriding the core image smart sharpen. Hope this helps anyone else who has this issue.*"

'Nevereast' meant 'image.py', but anyway, followed the instructions, works a treat!!

*Explainer for Nevereast's post:*  
- 'browse the repo' = have a look in the original [GitHub repository](https://github.com/cubiq/ComfyUI_essentials)  
- 'view the history of images.py' = there's an **image.py**, download that file  
- 'copied the code block' = I didn't: I just went into the /ComfyUI/custome_nodes/comfyui_essentials folder, renamed 'image.py' to 'image.bak' and placed the file I download in its place as 'image.py'. Works after you reboot the ComfyUI server.

So, why all the bother? Was this all worth it?

I invite you to click on one of the above two images, save it, open it in your favourite image-editing software (or viewing, *peu importe*) and **zoom in**.

The quality is **next level**.

---

I'm revisiting all my old renders, doing some of them over. And of course, whilst I was going a dewy-eyed about this workflow, a new flavour of [Qwen Image Edit](https://huggingface.co/Phr00t/Qwen-Image-Edit-Rapid-AIO) -- All-In-One -- has dropped. It is impossible to keep up. Here I was trying to build this elaborate 2-character workflow, and Qwen already has it all mapped out: you can combine 2, 3 and even 4 images together. Makes my little dual-consistent-character effort obsolete. Qwen Image Edit All-In-One will be easier, higher quality and much faster to get an image.

Well, maybe not the last one. I think it will always take multiple renders to get the right results.

<style>
  .flex-container {display: flex; gap: 20px;}
  .column {width: 100%;}
</style>

<div class="flex-container">
  <div class="column">

    <a href="/assets/images/blog25/06-03Kaimu.jpg" target="_blank" rel="noopener">
    <img src="/assets/images/blog25/06-03Kaimub.jpg" alt="Girl at Kaimu" width="180"></a>

    <a href="/assets/images/blog25/06-04Kaimu.jpg" target="_blank" rel="noopener">
    <img src="/assets/images/blog25/06-04Kaimub.jpg" alt="Girl at Kaimu" width="180"></a>

    <a href="/assets/images/blog25/06-05VictGirl.jpg" target="_blank" rel="noopener">
    <img src="/assets/images/blog25/06-05VictGirl.jpg" alt="Girl at the Train" width="180"></a>

    <a href="/assets/images/blog25/06-06VictGirl.jpg" target="_blank" rel="noopener">
    <img src="/assets/images/blog25/06-06VictGirl.jpg" alt="Girl at the Train" width="180"></a>

    <a href="/assets/images/blog25/06-07VictGirl.jpg" target="_blank" rel="noopener">
    <img src="/assets/images/blog25/06-07VictGirl.jpg" alt="Mystery Girl" width="180"></a>

  </div>
</div>

<hr style="height:10px;border-width:0;color:pink;background-color:pink">







### That Easy Fix 

**03.11.2025**  
I'm so proud of Julia. We intend to lose the extra kilos we've accumulated over time (and through wrong eating and nil exercise): I'm a *sick* of being overweight! So this morning **we** got down and dirty with a bit of aerobics! **WooHOO**!!

We're also starting out day with 'swamp water' -- warm lemon water with a spoonful of psyllium husk -- and being careful about food choices. Pizza? Not for the next forseeable future. Steak with Diane sauce? Hold the Diane sauce, thanks.

---

I had to revisit my easy-fixed node this morning when ComfyUI pissed and moaned about not finding a LUT folder or something. The only change I had done was to that image.py node -- as in, replaced it with an older version that contained the 'ImageSmartSharpen' component -- and ComfyUI wasn't happy with that solution. So, I went back and did what Nevereast had suggested in the first place:

- backed up image.py  
- copied the class ImageSmartSharpen over from the original to the existing: line 1072  
- added reference to the class in IMAGE_CLASS_MAPPINGS: line 1718  
- added reference to the name in IMAGE_NAME_MAPPINGS: line 1763  

ComfyUI was happy with those changes. Of course, I've zipped up that entire folder again, because I might need to repeat the process when I do an update. Oh well. You pay a price for flexibility.

---

Having a go with this new Qwen model. The model download was **[Twenty-Nine Gigabytes](https://huggingface.co/Phr00t/Qwen-Image-Edit-Rapid-AIO/tree/main)**!!! Good job we have fibre-to-the-premises NBN (fibre-optic broadband). Julia can watch her Netflix programmes and I can download stuff: NBN broadband does it with ease.

Here is the workflow. To the uninitiated, it may look complex because of the 'noodles', but it is *actually* one of the simplest ComfyUI workflows to date.


<a href="/assets/images/blog25/06-11QwenAIO.jpg" target="_blank" rel="noopener">
<img src="/assets/images/blog25/06-11QwenAIO.jpg" alt="Qwen AIO" width="700"></a>


(*Click for an expanded view*)

The base images:

<div class="flex-container">
  <div class="column">

    <a href="/assets/images/blog25/06-08VictGirl.jpg" target="_blank" rel="noopener">
    <img src="/assets/images/blog25/06-08VictGirl.jpg" alt="Victorian Girl" width="180"></a>

    <a href="/assets/images/blog25/06-09VictGirl.jpg" target="_blank" rel="noopener">
    <img src="/assets/images/blog25/06-09VictGirl.jpg" alt="Victorian Girl" width="180"></a>

    <a href="/assets/images/blog25/06-10Train.jpg" target="_blank" rel="noopener">
    <img src="/assets/images/blog25/06-10Train.jpg" alt="Victorian Train" width="180"></a>

  </div>
</div>

A few notes on these images.

<a href="/assets/images/blog25/06-12Train.jpg" target="_blank" rel="noopener">
<img src="/assets/images/blog25/06-12Train.jpg" alt="Victorian Girl" width="180" align="right"></a>

The images are versions of previous renders. Same prompts. With the images of the women, all reference to a background was removed. For the empty train, I used Flux Kontext with the simple prompt to: *Remove the woman from the image. Preserve the textures.*

You can do some crazy stuff using the different models. The dress of the girl in the first image above -- let's call her 'Celeste' ... I do -- didn't seem appropriate for this scene. So, I loaded that image into my **Flux Kontext** workflow with the prompt:

*Change the hair style to tied back Low Updo hair. Change the dress to dark green Victorian dress with fitted bodice and square neckline with lace trim and the neckline. Preserve the woman's hands pose.*


<a href="/assets/images/blog25/06-13VictGirl.jpg" target="_blank" rel="noopener">
<img src="/assets/images/blog25/06-13VictGirl.jpg" alt="Victorian Girl" width="360" align="left"></a>

<a href="/assets/images/blog25/06-14Train.jpg" target="_blank" rel="noopener">
<img src="/assets/images/blog25/06-14Train.jpg" alt="Victorian Girl" width="360" align="right"></a>


And *voila*!  
Updated the scene with Celeste now wearing her Victorian-style (*sort-of*) green dress in my **UBeaut Qwen Image Edit AIO** workflow and got the picture on the right. Yes, the poses are different: I tried making the seed 'fixed' instead of 'randomize' in the KSampler and entering the previous seed. 

**1019212345419124**.

Nope, it didn't work.

Actually, my problem was that I had been *over-baking* my Qwen images by setting steps to 25. According to [this video by Sharvin](https://www.youtube.com/watch?v=rXQh1dHZSAo), steps is meant to be set to 4! Because of that change -- setting steps to 4 -- the composition of the image changed. Huge benefit is: renders are now at *least* ten-times faster. Got to admit: that was sloppy of me. Just like my 'ImageSmartSharpen' quick-fix, which didn't. Fix the problem, that is... instead, it created other problems.

---

Apart from that, the character-consistency aspect of Celeste and Moi-même seems to be eroding. Will LoRAs work in Qwen?

Time to find out.

Well, the LoRA loader hooked up fine, but workflow generated **2256** lines of errors telling me it wouldn't load the LoRAs. Come to find out: Qwen LoRAs exist, and the training is specific to Qwen. Meaning: I can't use a Flux LoRA in Qwen. Yet. Another observation: it seems the 5090 graphics card is becoming the default for anything serious you want to do.

<hr style="height:10px;border-width:0;color:pink;background-color:pink">






### FluxGym with 16 Images

**04.Nov.2025**  
I do depend a lot on "Emily" *(ChatGPT)*. Just read a bit more on FluxGym (for creating Flux LoRAs) and the article claim I should be able to *easily* throw a 60-image dataset at it. Well, my install kept pooing itself with as few as 12 images. Thought I'd try a different model. My [discussion with Emily](../visual/FluxGym.md) have resolved that issue, thankfully. I was able to create a LoRA *(moimeme4)* with 16 images. Yes, it takes a long time, but I'm pretty happy with the results.

Saying that, I had hoped to at last be rid of that confounded 'brow ridge'. Oh well, 'in for a penny'.


<a href="/assets/images/blog25/06-15VictGirl.jpg" target="_blank" rel="noopener">
<img src="/assets/images/blog25/06-15VictGirl.jpg" alt="Victorian Girl" width="360" align="left"></a>

<a href="/assets/images/blog25/06-16Office.jpg" target="_blank" rel="noopener">
<img src="/assets/images/blog25/06-16Office.jpg" alt="Office Girl" width="360" align="right"></a>


Where things have gone in such a short time. As mentioned in a previous post, a lot of archived images from Automatic1111 and early ComfyUI have gone the way of Blender renders. The images -- well, I suppose I should keep **some** just to chronicle progress -- are frankly an embarassment, now. Blender and Poser content-creation files and render are being deleted to make room for unet files (safetensors) and other models. I don't want to buy HDDs unless the old drives are showing signs of expiring.  
The latest Qwen Image Edit All-In-One was 29 gigabytes. This SRPO is 23.8 gig.  
Need the room, Poser/Blender.

Clicking on either of these images will open the image in a new tab. Click the image to zoom in. The realism is uncanny.

<hr style="height:10px;border-width:0;color:pink;background-color:pink">







## Flux Kontext Prompts

**06.Nov.2025**  
**To preserve dimensions in Flux Kontext**:

Preserve identity markers: Explicitly mention what should remain consistent

* “…while maintaining the same facial features, hairstyle, and expression”
* “…keeping the same identity and personality”
* “…preserving their distinctive appearance”

---

For Character consistency, you can follow this framework to keep the same character across edits:

* Establish the reference: Begin by clearly identifying your character
* “This person…” or “The woman with short black hair…”
* Specify the transformation: Clearly state what aspects are changing
* Environment: “…now in a tropical beach setting”
* Activity: “…now picking up weeds in a garden”
* Style: “Transform to Claymation style while keeping the same person”
* Preserve identity markers: Explicitly mention what should remain consistent
* “…while maintaining the same facial features, hairstyle, and expression”
* “…keeping the same identity and personality”
* “…preserving their distinctive appearance”

---

How far have we come with creating AI images?


<a href="/assets/images/blog25/07-01Office.jpg" target="_blank" rel="noopener">
<img src="/assets/images/blog25/07-01Office.jpg" alt="Victorian Girl" width="360" align="left"></a>

<a href="/assets/images/blog25/07-02Office.jpg" target="_blank" rel="noopener">
<img src="/assets/images/blog25/07-02Office.jpg" alt="Office Girl" width="360" align="right"></a>


The image on the left is as far back as I care to go. It was an early ComfyUI render in Stable Diffusion 1.5 on the 8th of August, 2023. The workflow is a bit of a dog's breakfast. Note: *what's really cool about images created in ComfyUI is that the workflow is embedded in the image, which is saved as a .PNG. To retrieve the workflow, merely drop the image on a running ComfyUI instance. Of course, be prepared to see missing nodes that no longer exist in the repositories.*

The prompt was classic ComfyUI/SD 1.5: *full body shot, (front view), elegant beautiful Nordic young woman, wearing (beige Rhea blouse), wearing (black silk) skirt, dark (short bob) hair, (open mouth), sitting, sitting at cafe table, head tilted, outdoor cafe, coffee cup,*

Cryptic. It was actually one of my initial efforts at "face-swap": and it failed. The person behind the main figure got the face intended for our main figure. The tech was called 'roop', now defunct. Didn't have LoRAs back then. As far as accuracy to the prompt, well, not stellar. And some of the background figures' faces were just hideous (guy in the jacket).

On the right, same basic image, but providing more detail: SRPO requires detail. More is better: *This photograph, full body shot, front view, features elegant beautiful Nordic young woman "moimeme4". She is wearing a beige Rhea blouse, a black silk skirt and beige high heels. Her dark brown hair is styled  a short bob hair. Her expression is open mouthed with her head tilted, holding a coffee cup. She is sitting at cafe table at an outdoor cafe. The scene is Au Deux Maggots in the Latin Quarter in Paris. It is a blustery November morning with lots of wind blowing her hair. The lighting is morning bright outdoor sunshine. Leaves are visible on the sidewalk outside the cafe.*


<a href="/assets/images/blog25/07-03Office.jpg" target="_blank" rel="noopener">
<img src="/assets/images/blog25/07-03Office.jpg" alt="Office Girl" width="360" align="right"></a>


Prompt adherance is, better. *Can't see her shoes, and no one asked for 2 coffee cups, but still.* No question that the **quality** is significantly better. I mean, that detail in the hair! Skin texture! A followup render finally yielded the hairstyle I was after and removed the superfluous coffee cup and glassware. That's the thing about having your own graphics card: 1 render or 500, *peu importe*. We have solar so our electric power cost is 5¢/hr.

Impressive about the lass on the right is her *nose*. Yep, that tracks: I can smell the coffee, in Brazil. When it rains, people ask me if they can stand under my... um, you get it (*Roxanne*, with Steve Martin). 

By the way, Emily *bless-her-pointy-head* suggested I take the denoise down to .5 or less. Um, no. I just get light-coloured mud. Or a closeup of psyllium husk desolving in warm water. So, no. .95 is as low as I dare to go.

One of the problems Stable Diffusion had back in the day (*just a few years back, really*) was hands. Too many fingers, not enough fingers, misshapen limbs... hey, early days. One doesn't see that hardly at all with Flux or Qwen.  

<div class="flex-container">
  <div class="column">

    <a href="/assets/images/blog25/07-04Office.jpg" target="_blank" rel="noopener">
    <img src="/assets/images/blog25/07-04Office.jpg" alt="Office Girl" width="180"></a>

    <a href="/assets/images/blog25/07-05Office.jpg" target="_blank" rel="noopener">
    <img src="/assets/images/blog25/07-05Office.jpg" alt="Office Girl" width="180"></a>

    <a href="/assets/images/blog25/07-06Office.jpg" target="_blank" rel="noopener">
    <img src="/assets/images/blog25/07-06Office.jpg" alt="Office Girl" width="180"></a>

  </div>
</div>

---

And for music, [this piece](https://musescore.com/the_alex/scores/6385914) arranged by Alexander Evstyugov-Babaev runs through my head on a continuous loop.

<audio controls="controls">
  <source src="/assets/music/Bellflowers.mp3" type="audio/wav">
  Your browser does not support the <code>audio</code> element. 
</audio>

<hr style="height:10px;border-width:0;color:pink;background-color:pink">





### Reasons for Art

The discussion on CrossDream Life has brought into focus -- made it clear and validated it -- why all this was so important to me. Indeed, even why I got into Poser and Blender the way I did: this art form let me hop into a reality I could control. "If you can think of it, we can make it happen."

Two years ago, I tried to make my own comics.

<a href="/assets/images/blog25/09-01MumDecides.jpg" target="_blank" rel="noopener">
<img src="/assets/images/blog25/09-01MumDecides.jpg" alt="Mum Decides" width="800"></a>

Keep in mind this was 2 years ago. Renders were still pretty ordinary, pretty sure the base moel was at the most a variant of SDXL. Implied are misshapen limbs, dodgy facial expressions... not great. What was cool about this was that the comic could all be done in a workflow:


<a href="/assets/images/blog25/09-02Comixwf.jpg" target="_blank" rel="noopener">
<img src="/assets/images/blog25/09-02Comixwf.jpg" alt="Comix workflow" width="800"></a>


What was sad about this was that my writing sucked. Sure, character consistency wasn't there either, but hoo-boy, not great writing. Maybe because the "wishing" part was/is too poignant, I didn't do that great with plot development.

Anyway, the discussion on CDL touched on copyright issues relating to AI. I think eventually this may be a bridge we'd have to cross, but given the tech and how images sort-of "grow", it's not like you're seeing a copy of some image that the model was trained on.


<hr style="height:10px;border-width:0;color:pink;background-color:pink">


I'll give you a case in point. I wish to create an image of a young lady at a train station in Victorian England. I use a prompt generator to 'have a look' at an image, a drawing I found online with roughly that scene. I won't show you it, as there may be copyright issues. The prompt generator (an LLM that can "examine" an image) has a "look" and produces a whole bunch of text, much of which accurately describes what I want depicting:


<a href="/assets/images/blog25/09-03VictGirl.jpg" target="_blank" rel="noopener">
<img src="/assets/images/blog25/09-03VictGirl.jpg" alt="Victorian Girl" width="360" align="left"></a>

<a href="/assets/images/blog25/09-04VictGirl.jpg" target="_blank" rel="noopener">
<img src="/assets/images/blog25/09-04VictGirl.jpg" alt="Victorian Girl" width="360" align="right"></a>


The photograph depicts a Victorian-era woman "moimeme4" standing next to an old passenger carriage. She has fair skin, light brown hair styled in a loose updo. Her dress is long-sleeved, black with intricate lace details on the bodice and sleeves and a square neckline, and it features multiple layers of ruffled fabric. The dress buttons up the front. Her facial expression is serious and slightly contemplative.

She holds an aged brown leather suitcase by the handle. The suitcase has metal clasps and a slightly worn appearance. The steam train carriage beside her has a dark metallic exterior with visible rivets and numbers "15" on the side. In the background, another car of the train is visible, painted in maroon with yellow accents.

The background shows a dimly lit train station with arched ceilings and hanging lights. The walls are lined with dark wooden panels and vintage posters. The platform extends into the distance, fading into bright light at the far end. A yellow safety line is visible near the edge of the platform. Fallen leaves scatter on the ground beside her. The overall mood is gothic and melancholic, with a sense of adventure. The photograph emphasizes textures such as the lace of her dress, the polished wood of the station, and the aged leather suitcase. The woman's expression is slightly pensive, adding to the somber atmosphere. The image combines elements of historical fashion with a modern photographic style. The overall color palette of the image includes muted blacks, browns, and grays, with subtle highlights from the metallic elements of the train carriage. The photograph has a slightly dark and moody tone, emphasizing the Victorian era setting. The textures in the image are rich, particularly in the woman's dress fabric and the old leather suitcase.

I exchange 'illustration' with 'photograph' and add my LoRA keyword and run the queue. And get:

At this point, the forensic AI lawyer is sobbing. But wait: it gets worse. How do you contest a pastel Flux Kontext drawing derivative of an SRPO photoreal derivative of an LLM derivative of a drawing?








### Tomorrow's Entry

<hr style="height:10px;border-width:0;color:pink;background-color:pink">

---

<hr style="height:16px;border-width:0;color:pink;background-color:pink">

[Main Page](../index.md)
