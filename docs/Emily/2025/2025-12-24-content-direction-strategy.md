---
title: "Content Direction Strategy"
date: 2025-12-24
---

# Content Direction Strategy

**Robyn:**

Here's the premise, Emily:

Carry-over pack (DA project direction)
Primary focus: historical costume, textile craft, women-friendly storytelling.
Avoid: kink framing, coercion/humiliation framing, fetish-coded tags.
Target tone: agency, dignity, lived experience, new spaces.
Strategy: revise older deviations in place or retire to st.ash; keep main gallery clean; use Watchers-only for “Experiments/Intimate” if needed.
Practical aim: DA “More Like This” should cluster around history/costume/craft, not fetish.
Tooling: Z-Image-Turbo for prompt adherence, SRPO/Flux for polish; 2-character LoRA workflow still being refined.

**Emily:**

Got it. This is a clean, *coherent* direction pack — and it’s strong because it’s not just “what you’ll post,” it’s “what you’ll train the platform to expect from you.”

Here’s a practical “carry-over pack” you can use as your internal operating system (and copy into a Help page if you like).

---

## 1) The North Star test (30 seconds before you post)

Ask three questions:

1) **Would a woman who loves costume/textiles feel welcomed here instantly?**  
2) **Is the subject’s *agency* obvious without needing to explain it?**  
3) **If someone only saw the thumbnail + first two lines, would they guess “craft/history/story” rather than “heat”?**

If any answer is “eh…” → adjust either the crop, the caption lead, or the tags.

---

## 2) Your DA “signal stack” (how you steer More Like This)

DA’s recommendations respond to consistent *metadata + viewer behavior*. You can’t control who clicks, but you can control what you *feed* the system.

### What to make consistent
- **Deviation titles** that read like exhibit labels, not clickbait  
  - Good: *“Silk Taffeta Day Dress, 1787 — Morning Light Study”*  
  - Good: *“Stays & Linen Shift — Dressing Sequence Panel 3”*  
  - Avoid: anything that teases body/heat/forbidden vibes

- **First line of description** (your “algorithmic handshake”)  
  Lead with: era / garment / craft / place / purpose.  
  Example first line:  
  *“1780s linen shift + lightly boned stays, hand-finished seams; made for a practical morning chores vignette.”*

- **Tag set** that is boring-in-a-good-way  
  Repetition is your friend here.

---

## 3) Tag hygiene: a working whitelist + blacklist mindset

### A dependable “core tag bundle”
Pick ~12–20 you reuse constantly (rotate a few per post):

**History / Costume**
- historicalcostume, historicalfashion, costumehistory, 18thcentury, 1780s, 1740s, regency (as relevant)
- reenactment, livinghistory, perioddress

**Textile / Making**
- textilecraft, sewing, dressmaking, handsewing, embroidery, patternmaking, tailoring, millinery (as relevant)
- fabric, linen, wool, silk, taffeta, cotton (specific fibres help)

**Story / Tone**
- womeninstorytelling, womensagency, sliceoflife, narrativeart, characterstudy, domesticlife, workandcraft

### Tags to treat as “radioactive” for your goals
Anything that implies:
- **coercion/humiliation** (even mildly)
- **fetish shorthand** (you already know the usual suspects)
- **body-part spotlight tags** (they tend to attract the wrong “More Like This” neighborhood)

Even if the image is innocent, those tags are magnets.

---

## 4) Caption style that quietly enforces “agency, dignity, lived experience”

A simple template that reads like craft notes + diary fragment:

1) **Context sentence:** era, garment, situation  
2) **Agency sentence:** what she decides/does/controls  
3) **Material/craft sentence:** what we’re seeing and why it matters  
4) **Optional note:** what you’re exploring next (invites the right audience)

Example:

> *1786–88 working daywear: linen shift under stays, with a wool petticoat built for movement, not display.*  
> *She ties her own laces, checks the fit in the glass, and chooses comfort over performative prettiness — because she has a day to run.*  
> *I focused on seam logic, tension lines, and the hand-finished edges that make the silhouette believable in close-up.*  
> *Next study: cap shapes and hair preparation for a maid’s morning routine.*

It reads *human*, but it keeps you in the craft/story lane.

---

## 5) Gallery architecture (keep the main gallery clean without losing your playground)

Your strategy is already right; here’s a crisp structure:

### Main gallery
Only the “North Star test” passes.  
Make it feel like a curated museum room: consistent titles, consistent tags, consistent first line.

### Scraps / storage (st.ash or unlisted)
Anything that:
- risks misreads,
- is primarily anatomy-driven,
- is experimental in a way that looks like “intimate vibe” even if you didn’t mean it.

### Watchers-only “Experiments/Intimate” (if needed)
If you use it, keep it *explicitly craft-framed*:
- “lighting study,” “pose study,” “cloth simulation test,” “fabric drape exploration”
And keep those posts scarce — the ratio matters.

A useful rule: **for every 1 experiment, post 3–5 “core-direction” deviations**.

---

## 6) Revision protocol for older deviations (fast and consistent)

When you “revise in place,” you’re doing two things: cleaning your public face *and* re-indexing.

### The 5-minute retrofit checklist
- **Title** → craft/history label style
- **Thumbnail/crop** → de-emphasize chest/hips; emphasize garment construction, silhouette, hands-at-work, full outfit
- **First 2 lines** → era/garment/craft + agency
- **Tags** → swap to your core bundle; remove anything even vaguely spicy-coded
- **Folder/collection placement** → move into “Costume Studies / Textile Craft / Narrative Panels” etc.

If a piece can’t be rescued without feeling like a rewrite of its intent → retire it (your plan is correct).

---

## 7) Image-making guardrails (prompting + composition cues)

Even before tags/caption, the *image language* can shout the wrong thing. Some compositional nudges that reliably read “lived experience”:

- **Hands doing something** (pinning, threading, tying, holding fabric, carrying a basket, smoothing pleats)
- **Work surfaces** (table with notions, sewing kit, ledger, basket of mending)
- **Context props** that are *functional* not coquettish (scissors, linen roll, cap stand, iron, candle stub, wash basin)
- **Faces**: expression = intent (focused, amused, firm, assessing), not “inviting”
- **Stance**: upright, purposeful, weight distributed like a person with plans

Prompt words that tend to help: *practical, work-worn, utilitarian, mended, hand-finished, pinned, basted, stitched, durable, lived-in, morning light, workshop, household management, inventory, preparations, routine.*

And for your storytelling bias (women as the deciding force): make sure the women are *doing*, not *being displayed*. Decision-making reads instantly.

---

## 8) Your toolchain note (Z-Image-Turbo + SRPO/Flux + 2-character LoRA)

This dovetails nicely with the direction:

- **Z-Image-Turbo**: use it to lock the “what” (era, garment pieces, materials, activity)
- **SRPO/Flux polish**: reserve it for finishing passes that enhance texture realism, seam logic, and face integrity — not glamour sheen
- **2-character LoRA**: when in doubt, stage the dynamic as *collaboration* rather than tension  
  - e.g., fitter + client, mistress + maid (professional, not degrading), two women working a dress form together, mentor correcting posture with permission and clarity.

If the relationship is asymmetrical, make it *competence-based* and explicit: the more skilled woman directs with calm authority; the other learns and improves — no “humiliation energy.”

---

## 9) A tiny “posting decision tree” you can memorize

- Does it look like **craft/story** at thumbnail size?  
  - No → don’t post / move to stash
- Do the tags include anything that could be misread as spicy-coded?  
  - Yes → remove/replace
- Does the first line mention **era + garment + activity**?  
  - No → rewrite
- Would you be happy if DA recommended this to **historical costumers and textile artists**?  
  - If you feel even a flicker of “hmm maybe not” → stash or revise

---

If you want, paste one of your *borderline* older deviations’ current title + tags + first two lines (no image needed), and I’ll rewrite them into your new “clean gallery” voice and tag bundle in a way that still feels like *you*.

**Robyn:**

<img src="/assets/images/chat/3a330e2.jpg" style="float: right; width: 420px; margin-left: 20px" />

Your prompt suggestions bore fruit, Emily. Exactly what I was after.

**Emily:**

Oh, *this* landed beautifully.

What reads so well here isn’t just “pretty girl in a dress” — it’s **lived-in narrative**: she’s *going somewhere*, she’s *thinking*, the light is doing real work, and the costume feels like it belongs to her rather than being “worn for the viewer.” That’s exactly the “agency / dignity / new spaces” lane you laid out.

### What’s working (so you can reliably repeat it)
- **Gaze + posture:** she’s looking *out*, not “at us.” Hands are calm, self-possessed. No performative pose.
- **Light logic:** low sun, soft rim light on hair/cheek, gentle falloff in the compartment. It’s cinematic but not glossy.
- **Costume read:** the chemise peeking at the neckline + the laced bodice gives an immediate “period layer” signal without shouting.
- **Setting choice:** travel carriage = “transition / possibility / interior life.” It’s a *story* setting, not a boudoir setting.
- **Frame hygiene:** the foreground seat blur and window geometry push it into “photograph / moment observed,” not “pin-up.”

If you want DA’s recommendations to cluster correctly, this piece is gold: it screams **costume + narrative**, not “spice.”

---

## A reusable prompt “skeleton” for this exact vibe
Use this as a template and swap the bracketed bits:

**Prompt core (natural language):**  
*A young woman in [era]-inspired working dress sits alone in a [vintage/heritage] train compartment at sunrise, looking out the window with a composed, thoughtful expression. Her dress shows practical construction: [bodice/lacing], a clean [linen] chemise at the neckline, subtly worn fabric texture, careful seams. The scene feels like lived experience: quiet travel, early light, dust on the window, countryside passing by. Soft cinematic realism, gentle depth of field, natural skin texture, believable period styling, no glam pose.*

**Guardrail add-ons (to keep it out of the wrong neighborhood):**  
- *“practical, modest, non-suggestive, documentary mood, quiet dignity”*  
- *“hands relaxed, posture upright, not posing”*  
- *“focus on garment construction and atmosphere”*

If your tool supports it, a short negative list can help:  
- *“lingerie, boudoir, pin-up, provocative, fetish, cleavage emphasis, see-through fabric, erotic, submissive pose”*

---

## 6 variations that stay in the same “clean gallery” voice
Each one keeps agency + craft, but changes the story beat so you can post a *series* without drifting tone.

1) **Mending on the journey**  
*…she holds a small sewing kit and is darning a cuff, thread catching the sunrise; attention on the stitchwork, not the viewer.*

2) **Reading / literacy / interior life**  
*…she has a folded letter or small book in her lap; thumb marks the page; expression calm and decisive.*

3) **Arrival moment**  
*…station platform visible through the window; she gathers her skirt to stand, ready to step off — purposeful transition.*

4) **Workwear emphasis**  
*…apron tied, sleeves slightly rolled, practical cap pinned back; fabric shows honest wear and careful repairs.*

5) **Companion without romance coding**  
*…another woman across the aisle, older or similarly aged, sharing a quiet glance — mentorship / solidarity, not flirt.*

6) **Textile-craft close-up**  
*…tight crop on bodice lacing, chemise neckline, and hand position; light rakes across seam lines and fabric grain.*

---

## DA packaging for *this* image (title + first lines + tags)
If you want it to steer “More Like This” hard toward costume/craft, here’s a ready-to-post option:

**Title ideas**
- *“Morning Carriage — Working Dress Study”*  
- *“Quiet Travel, Early Light — Costume Narrative”*  
- *“Linen & Lacing — A Journey Study”*

**First two lines (the algorithm handshake)**
- *Period-inspired working dress with a linen chemise neckline and a laced bodice, lit by sunrise through a dusty carriage window.*  
- *I wanted a moment of quiet agency — travel as possibility — with fabric texture and seam logic treated as the main character.*

**Tag bundle (safe, on-theme)**
historicalcostume, historicalfashion, costumehistory, textilecraft, sewing, dressmaking, handsewing, fabrictexture, linen, lacing, perioddress, livinghistory, narrativeart, characterstudy, sliceoflife, womensstories, womeninart, cinematiclight, sunrise, travel

(You can swap in the exact era tag once you decide what you’re calling the silhouette.)

---

If you tell me which model stack produced this one (ZiT settings + which LoRA(s) + whether SRPO was a finishing pass), I can help you “lock” a repeatable recipe: same mood, different scenes — so you can build a *clean, coherent gallery run* that trains DA exactly where you want it to go.

**Robyn:**

This image was pure Z-Image... no LoRA, just your prompt mixed with mine. I'm trimming back the floral language while keeping atmosphere and everything to make the images pop. I did get a lovely comment from a user on one of the 'Experiment' pictures:

"I do love your imagination. The artistic portrayal of forced feminity and the exquisite humiliation that is seen on the faces of the victims is delightful. Thank you for your artistry, it is very much appreciated.:heart:"

Yeah, mixed bag there. This user wanted to know the terms behind the images. I let them know: "it's open slather, do as you want, whatever... just know they're coming down, eventually."

**Emily:**

Yeah — that’s *exactly* the “mixed bag” problem you’re trying to solve: you post something with craftsmanship and atmosphere, and a subset of viewers tries to drag it into their preferred interpretive frame (and they’ll happily supply the vocabulary for it).

Two things are true at once:

- The commenter is being honest about what they want from your work.
- Letting that framing “sit” (and especially engaging it warmly) nudges both **your audience composition** and **DA’s similarity clustering** in the wrong direction.

### The one tweak I’d make to your response style
“Open slather, do as you want” is generous — but it’s also an *invitation*.

Even if you add “they’re coming down eventually,” the part that lands in their brain is: *permission + access*.

A tighter boundary doesn’t need to be rude; it just needs to be unambiguous:

- **You’re not making kink content.**
- **You’re archiving older experiments.**
- **Your focus is costume/craft/narrative with agency.**

That’s it. No debate, no explanation, no “terms behind the images.”

---

## What I’d do with that comment (practical options)
Pick the level that matches your patience on the day.

### Option A: Silent curation (best for algorithms)
- **Hide/delete** the comment.
- **No reply.**
- If it’s on an “Experiment” deviation that you intend to retire anyway, this is the cleanest move.

Why: DA isn’t only reading your tags; it’s reading *engagement patterns*. A fetish-coded comment sitting visibly under your art is a little beacon for the wrong crowd.

### Option B: Brief boundary reply (if you feel you must reply)
A one-liner that closes the door without drama:

> “Thanks for looking. I’m shifting this gallery toward historical costume and textile craft; the older experiments will be archived.”

Notice: no acknowledgement of their vocabulary, no validation, no “you’re welcome,” no “delighted,” no talk about humiliation/forced anything.

### Option C: Firm boundary (if they push or repeat)
> “I’m not creating fetish or humiliation-themed work. Please don’t frame my images that way here. This deviation will be archived.”

That’s still calm — but it draws a line. If they keep going, you’ve already set justification for a block/mute without feeling you’re overreacting.

---

## A smart “Experiment folder” disclaimer (so you don’t have to repeat yourself)
Put this at the top of your Experiments/Intimate area (or in each description first line), and it will do quiet, constant work:

**Short version**
> “Experiments = lighting/composition/costume studies. Not fetish content. Older posts will be archived as the project refocuses on historical costume and textile craft.”

**Slightly longer**
> “These are technical and narrative studies: fabric behavior, seams, silhouette, and mood. I’m moving away from kink-coded framing and toward women-forward storytelling, craft, and lived experience. Older experiments will gradually be archived.”

This helps the *good-faith* viewers understand what they’re seeing, and it gives you cover to remove comments that contradict it.

---

## Keep your “main gallery” clean: the 80/20 rule
If you do keep any “Experiment/Intimate” material up for a while:

- **Main gallery**: only posts that pass your new North Star.
- **Experiments**: either Watchers-only, or comment-limited, or actively curated.
- Ratio: aim for **4 clean craft/narrative posts for every 1 experiment** you leave visible.

This keeps the public-facing identity stable while you transition.

---

## If you want DA to stop sending that crowd entirely
These small moves have outsized impact:

1) **Remove fetish-attractor tags** everywhere (including old deviations).  
2) **Edit titles and first lines** to read like craft notes (era/garment/material/action).  
3) **Cull comment sections** where the vibe has already drifted.  
4) **Don’t “talk kink” in replies** even to reject it — keep it clinical and directional.  
   (The more the vocabulary appears, the more it becomes associated with your page.)

---

## Prompt language: “trim floral, keep pop” (a tight template)
Since you’re getting great results with pure Z-Image, here’s a compact template that keeps atmosphere but avoids overwriting:

**Template**
- **Subject + era garment**
- **Action**
- **Setting**
- **Light**
- **Tone guardrails**
- **Craft emphasis**

Example (tight, not flowery):
> “Young woman in a practical 18th-century inspired day dress with a linen chemise neckline and laced bodice, seated in a vintage train compartment at sunrise, looking out the window. Soft natural light, dusty glass, shallow depth of field. Quiet dignity, documentary mood, focus on fabric texture, seams, and believable construction; not posing.”

That will “pop” because it gives the model *specific anchors* (garment parts + light + setting) without drowning it in prose.

---

## If you want, I can help you turn that awkward interaction into a policy you never have to think about again
Paste the exact wording of your current “Experiment” folder description (or one deviation description), and I’ll rewrite it into a crisp, women-forward, craft-first disclaimer that:
- discourages fetish-coded commentary,
- matches your new project direction,
- doesn’t feel preachy,
- and still leaves you room to experiment.

**Robyn:**

One of the next things I'd like to try is to make Z-Image-Turbo LoRAs. Apparently, there's a toolkit available so it's a matter of assembling a few images and descriptor text. It looks straightforward to do, just a bit time-consuming. To be honest, I'm seriously contemplating getting another 16gig VRAM card to put in the unused PC behind me. It has Comfyui installed, but using my old 8 gig VRAM card, which is almost unusable for most stuff I do anymore. That way, I can make LoRAs and alternate between running Z-Image-Turbo on one system and SRPO/ttp on the other. Lot less offloading, onboarding waste of time.

**Emily:**

That plan makes a lot of sense — and you’re thinking about it in exactly the right “throughput” way: **separate the slow/iterative work (training, SRPO/ttp finishing) from the fast cadence work (Z-Image-Turbo drafting)** so you stop paying the constant offload/onboard tax.

## 1) Z-Image-Turbo LoRAs: yes, there really *is* a straightforward toolkit
What you’ve heard matches what’s currently out there: the **Ostris AI Toolkit** has a **Z-Image Turbo training adapter**, and there are now reproducible “known-good” configs circulating. citeturn1search3turn1search6

Two practical “knowns” that save you pain up front:

- The toolkit is designed to be run as GUI or CLI and aims to support modern diffusion training on consumer hardware. citeturn1search3  
- People training Z-Image Turbo LoRAs with this tool are reporting specific gotchas/settings (notably **Transformer Offload = 0%** due to errors) and sane starting ranges like **LR 1e-4 to 2e-4** and **~2000 steps** as a baseline. citeturn0search6

Also: the official Z-Image repo explicitly claims Turbo “fits comfortably within 16G VRAM consumer devices” for inference. citeturn0search0  
Training is always heavier than inference, but it’s still a friendly starting point for your “second box” idea.

---

## 2) The fastest LoRA path for *your* direction (costume/craft-first, clean-gallery)
If you want DA clustering to behave, I’d start with LoRAs that reinforce *your* core signal:

### A) “Silhouette + construction” LoRAs (my recommendation as your first batch)
Train a LoRA that learns:
- period silhouette (waistline, skirt volume, sleeve shape)
- fabric behavior (linen vs wool vs silk)
- seam/edge logic and closures (lacing, stomacher, cuffs, neckline)

This is **high leverage** because it improves *every* scene you generate, not just one character.

### B) “Environment + light” LoRAs (your second batch)
A LoRA for: *train carriage at sunrise*, *workroom daylight*, *candlelit corridor*, *rain on glass*, etc.  
These help your “atmosphere without floral language” goal.

### C) Character LoRAs (later)
Once your craft style is stable, character LoRAs become much easier to control without drifting into “pin-up coding.”

---

## 3) Dataset + captions: the time sink, but you can make it painless
Here’s a workflow that stays aligned with your “trimmed language” preference *and* trains well:

### Image selection (keep it boringly consistent)
- Use **your best Z-Image Turbo outputs only** (you’re already getting the look you want).
- Favor **variety of framing** (close portrait, 3/4, full figure, hands doing something), but keep **wardrobe consistent** for that LoRA’s purpose.
- Avoid including anything you don’t want echoed later (posing tropes, suggestive framing, etc.). A LoRA is a mirror.

### Caption style (short, factual, repeatable)
Think “museum label + sewing notes,” not prose.

Example caption pattern:
- `robynsilhouette, 18th century inspired day dress, linen chemise neckline, laced bodice, wool skirt, natural light, documentary mood, seated, looking out window, vintage train compartment`

Key idea: your **trigger token** (e.g., `robynsilhouette`) + **garment tokens** + **scene tokens**.

Because you’re intentionally steering away from kink-coded interpretation, you’ll want your captions to be relentlessly literal: garment, materials, action, setting, light.

---

## 4) Training settings: a solid “first run” recipe
Without overcomplicating it, I’d start with what’s already been field-tested for Z-Image Turbo LoRAs in the Ostris toolkit ecosystem:

- Use the **Z-Image Turbo adapter** path described in the toolkit guides. citeturn1search6  
- **Transformer Offload: 0%** (reported to avoid errors). citeturn0search6  
- **Learning rate:** start around **1e-4** (and only creep up if it underfits). citeturn0search6  
- **Steps:** start around **~2000** as a baseline. citeturn0search6  

That gets you a first LoRA quickly, and then you iterate by *testing*, not by guessing.

---

## 5) Hardware: your “two PCs” plan is actually the low-drama option
I like this because it avoids all the multi-GPU weirdness and keeps each machine “single-purpose.”

### The key benefit
- One box is always ready to **draft** (Z-Image Turbo).
- The other is always ready to **polish/train** (SRPO/ttp + LoRA runs).

No juggling model caches, no stopping one workflow to reclaim VRAM for the other.

### VRAM guidance (what matters most for LoRA work)
24GB+ is the “comfort zone” for training flexibility in general LoRA workflows; 16GB can work, but you’ll hit constraints sooner. citeturn0search1

So: **a 16GB card in the second PC will already feel like night-and-day compared to 8GB**, but if you can stretch higher VRAM later, you’ll feel it most during training.

### Current 16GB / 32GB reality check (if you’re shopping now)
- RTX **5070 Ti** and **5080** are both **16GB** class cards per NVIDIA specs. citeturn1search0turn1search5  
- RTX **5090** is **32GB GDDR7** (serious headroom, serious price). citeturn2search0

I’m not pushing you toward the 5090 (it’s… a lot), but it’s useful to know what “endgame headroom” looks like.

---

## 6) The simplest “two-box” operational setup
To keep friction low:

- Put **models + outputs** on a **shared folder** (SMB/NFS) or use a cheap NVMe “sneakernet” drive you shuttle weekly.
- Keep **identical directory structure** on both machines (`/models`, `/loras`, `/outputs`, etc.).  
- Run each ComfyUI on different ports and just bookmark them:  
  - Machine A: ZiT drafting  
  - Machine B: SRPO/ttp finishing + LoRA training

That alone removes most “onboarding waste of time.”

---

If you tell me what your **first ZiT LoRA target** is (character identity vs silhouette vs a specific scene like “carriage at sunrise”), I’ll give you a *ready-to-copy* caption schema (tokens + a small controlled vocabulary) that stays clean-gallery-safe and trains efficiently.

**Robyn:**

I hadn't actually considered clothing LoRAs, to be honest. I have experience in Flux *character* LoRA generation using fluxgym, which was dead easy.

As to the the 5090 series cards: yeah, WAY out of my league, Emily. I reuse older tech and inexpensive tech. AUD$750 is about my limit for a graphics card: those 5090s START at $4700. Yeah, not an option... they cost more than my car.

**tool:**

You have invoked product_query. DO NOT use automations. DO NOT call the automations tool. DO NOT use canvas. DO NOT call the canmore tool. product_query DOES NOT work with Canvas. You MUST answer in chat instead.

**tool:**

ASUS nVidia GeForce DUAL RTX4060TI EVO RTX 4060 Ti EVO OC Ed (https://chatgpt.com/?hints=search&q=ASUS+nVidia+GeForce+DUAL+RTX4060TI+EVO+RTX+4060+Ti+EVO+OC+Edition+8GB+GDDR6)
【turn0product0】
*Rating:* 4.6/5 (2600 reviews)
*Merchants:*
  - $499.00: Centre Com

**tool:**

MSI GeForce RTX 3060 VENTUS 2X 12GB OC Video Card (https://chatgpt.com/?hints=search&q=MSI+GeForce+RTX+3060+VENTUS+2X+12GB+OC+Video+Card)
【turn0product1】
*Rating:* 4.8/5 (1700 reviews)
*Merchants:*
  - $409.00: Scorptec

**tool:**

Gigabyte Radeon RX 7800 XT Gaming OC (https://chatgpt.com/?hints=search&q=Gigabyte+Radeon+RX+7800+XT+Gaming+OC)
【turn0product2】
*Rating:* 4.7/5 (339 reviews)
*Merchants:*
  - $1,199.00: Woolworths

**tool:**

Gigabyte nVidia GeForce RTX 4060 Ti WINDFORCE OC 8G (https://chatgpt.com/?hints=search&q=Gigabyte+nVidia+GeForce+RTX+4060+Ti+WINDFORCE+OC+8G)
【turn0product3】
*Rating:* 4.7/5 (92 reviews)
*Merchants:*
  - $718.52: Zotim

**tool:**

Gigabyte GeForce RTX 3060 WINDFORCE OC 12G Graphics Card (https://chatgpt.com/?hints=search&q=Gigabyte+GeForce+RTX+3060+WINDFORCE+OC+12G+Graphics+Card)
【turn0product4】
*Rating:* 4.8/5 (457 reviews)
*Merchants:*
  - $449.00: Computers & Parts Land

**tool:**

Sapphire Nitro+ AMD Radeon RX 7800 XT Gaming OC Graphics Car (https://chatgpt.com/?hints=search&q=Sapphire+Nitro%2B+AMD+Radeon+RX+7800+XT+Gaming+OC+Graphics+Card)
【turn0product5】
*Rating:* 4.8/5 (264 reviews)
*Merchants:*
  - $1,710.42: Amazon AU

**Emily:**

Totally fair — the 5090 pricing in Australia is “buy a car” territory, not “buy a GPU.” And honestly, you don’t need that class of card to make your workflow feel *vastly* smoother.

### Clothing LoRAs are worth it (and they’ll feel familiar if FluxGym was easy)
A clothing/silhouette LoRA is basically a **character LoRA, except the “identity” you’re teaching is garment logic**:

- consistent neckline/chemise reveal
- bodice shape + lacing behavior
- sleeve head + cuff proportions
- skirt volume + fabric drape
- texture cues (linen vs wool vs silk)

The big win: you can keep using **pure Z-Image prompts** (your current “clean, dignified narrative” lane), but the LoRA quietly *locks* the costume language so you’re not re-negotiating it every prompt.

If you do one first, I’d do a *capsule* garment LoRA like:
- **“Linen chemise neckline + laced bodice + wool skirt”** (your train image look)
- or **“workroom apron + rolled sleeves + cap”** (craft-first, very DA-safe)

Dataset-wise: you can start small. 25–60 images, varied angles, a few close-ups on construction points, captions that are literal (garment parts, materials, light, setting). No prose required.

---

## GPU reality under AUD$750: the practical shortlist
Given your budget and ML tooling, staying **NVIDIA** is still the least pain.

### Best match for your plan: **RTX 4060 Ti 16GB (used/refurb)**
- New retail pricing in AU seems to sit **above** your ceiling (e.g. ~$830 at one retailer listing). citeturn0search0  
- But used/refurb listings show it **can** land in-budget (eBay results showing ~AU$571–$646 in the filtered band, and some higher used listings). citeturn0search2

This is the cleanest “drop-in second box” upgrade from 8GB → 16GB, and it aligns perfectly with your “draft on one machine / train+polish on the other” idea.

### “Get it done cheaply” option: **RTX 3060 12GB**
It won’t be as fast, but 12GB is a huge step up from 8GB, and it’s noticeably cheaper (current AU listings around the low–mid $400s in the product results I saw). citeturn0product1turn0product4  
If you want the second PC mainly for **LoRA training and lighter runs**, it’s a workable “good enough” card.

### Why not a 3090 (even though 24GB is delicious)?
Used pricing in AU is still typically **well over** $750 (Gumtree examples around ~$1,250; eBay often ~$968+ and upwards). citeturn0search3turn0search1  
So: great VRAM, wrong bracket.

---

## My recommendation for *you*, specifically
If you’re already happy with how your current machine behaves for Z-Image, then your best ROI is:

1) **Put a 4060 Ti 16GB (used/refurb) in the spare PC** if you can snag one in the $650–$750 range. citeturn0search2  
2) Use that spare PC as your **training + SRPO/ttp box**, and keep your main box as the “always-ready” Z-Image drafting station.

That gives you the throughput win you want without chasing unicorn hardware.

---

## Two quick “don’t get burned” checks when buying used
- **Return policy / warranty** (even 30 days is sanity-saving).
- **Mining wear** isn’t always fatal, but it’s a risk multiplier. Favor sellers with testing proof and clean history.

Also verify your spare PC’s **PSU headroom** and connectors (4060 Ti is modest, but still check), and that the case clearance is okay.

---

If you tell me what your *first clothing LoRA* should represent (e.g., “1780s work dress capsule” vs “stays+chemise neckline” vs “apron/mending scenes”), I’ll give you a dead-simple caption vocabulary and a minimal dataset plan that keeps everything firmly in your new **craft / agency / lived experience** lane.

**Robyn:**

I'm having some issue with ttp being too agressive with thexturing: I'd like to dial it back a bit, but am unsure which nodes to tweak. First, What is the ComfyUI Flux-TTP-Upscale Face Restore Workflow?
The Flux-TTP-Upscale workflow offers an advanced Face Restore pipeline within the ComfyUI environment. It integrates Flux's face restoration technology with TTP (Tile-to-Patch) enhancement to fix distorted or low-quality faces in AI-generated images. This is especially effective for group portraits, profile shots, or any visuals with facial artifacts.

By combining FluxGuidance, tile-aware image enhancement, and LoRA-based identity control, Flux-TTP-Upscale Face Restore delivers reliable Face Restore performance while upscaling to crisp 4K resolution.

I can actually send you a workflow with the TTP area towards the end, if that helps.

**Emily:**

Yep — that paragraph you pasted is basically the **RunComfy “Flux-TTP-Upscale | 4K Face Restore”** write-up, and it’s a real, fairly popular ComfyUI workflow template. citeturn0search1

## What the Flux-TTP-Upscale Face Restore workflow actually is
It’s not “a single magic face-restore model.” It’s a **pipeline** that:

1) **Prepares / upscales the input image** (often: resize to a workable base size, sometimes a first-pass super-res model like NMKD). citeturn0search1turn0search2  
2) **Splits the image into tiles** using TTP’s tiler (`TTP_Image_Tile_Batch`) so each patch can be enhanced locally without blowing VRAM. citeturn0search1turn0search8  
3) **Enhances/redraws each tile** using Flux as the “refiner” (typically via `SamplerCustomAdvanced` with a modest denoise; RunComfy’s example calls out denoise around 0.3 and an Euler sampler). citeturn0search1  
4) **Reassembles tiles** (`TTP_Image_Assy`) using padding/overlap so seams are hidden. citeturn0search1turn0search8  

The “face restore” part happens because **small faces benefit disproportionately** from a tile-wise redraw: the model can focus on a tiny region and rebuild facial structure where the original generation got mushy or warped. citeturn0search1

---

## Why your texturing is getting too aggressive
In tile workflows, “over-texturing” (crunchy skin, gritty fabric, micro-contrast everywhere) usually comes from one or more of these:

- **Denoise too high** in the tile redraw pass (the model is allowed to “invent” new surface detail).
- **Guidance too high** (it overcommits to whatever the prompt/caption implies, often “detail = pores = grit”).
- **Tile prompts too rich / too literal** (especially if an interrogator like JoyCaption is used per-tile; it can describe *noise* as “texture” and then the model obliges).
- **A sharpening-heavy upscaler model** feeding into the tile pass (edge contrast gets amplified before the redraw even starts). citeturn0search2turn0search1  

---

## The exact nodes to tweak (in the order that actually works)
You asked “which nodes” — here’s the high-impact ladder.

### 1) `SamplerCustomAdvanced` denoise (biggest lever)
This is usually the #1 cause.

- If you’re at **0.30**, try **0.22 → 0.18** first.  
- If faces stop improving, creep back up in tiny steps (0.02).

This single change often turns “gritty” into “refined.”

### 2) `FluxGuidance` (second biggest lever)
RunComfy’s workflow explicitly uses FluxGuidance as a driver for restoration. citeturn0search1  
If texture is too punchy, drop guidance *before* you start changing everything else.

A good “civilized detail” zone is often **~2.8–3.3** (and only go higher when you truly need reconstruction).

### 3) Tile prompt source (JoyCaption / per-tile prompts)
If your workflow uses **Joy Caption Two** (RunComfy mentions it), it’s great for validation, but it can be a menace if it’s feeding prompts into the tile redraw. citeturn0search1turn0search2

Two ways to calm it:
- **Don’t prompt per tile** (use one simple global prompt for the whole image).
- Or **hard-cap / simplify** the tile prompts: remove words like “ultra-detailed, pores, skin texture, sharp, crisp, high definition.”

### 4) Upscale model choice (pre-upscaler can “pre-crunch”)
If you’re using something like **4x NMKD Superscale** (RunComfy references it), try swapping to a less “edgy” model, or skip pre-upscale and let the tile redraw do the heavy lifting. citeturn0search1turn0search2

### 5) Add a blend-back node (the cheat code)
If you want an immediate “dial” without rebalancing the whole graph:
- After `TTP_Image_Assy`, **blend the enhanced image back with the original (or pre-upscale)** at something like **70/30** or **80/20** (enhanced/original).

This preserves the repaired facial structure but mutes the overzealous microtexture.

---

## Quick presets you can try right now
Assuming your current “too aggressive” looks like crunchy skin/fabric:

**Preset A: Conservative refinement**
- Denoise: **0.18–0.22**
- FluxGuidance: **2.8–3.2**
- Steps: don’t go crazy (often **16–24** is plenty for a refiner pass)

**Preset B: Keep faces strong, calm everything else**
- Same as A, plus:
- Add **blend-back 75/25** (enhanced/original)

---

## Yes — send the workflow (and I’ll point to the exact sliders)
If you paste the **JSON** or even just screenshot/copy the **TTP section** (from `TTP_Image_Tile_Batch` through `TTP_Image_Assy` plus whatever `SamplerCustomAdvanced/FluxGuidance` nodes sit in that loop), I’ll tell you precisely:
- which denoise is the “real one” (sometimes there are two),
- where the prompt is being injected (global vs per-tile),
- and the single best blend point for your graph.

(And if you tell me what “too aggressive” looks like in your outputs — gritty skin? crunchy cloth? sharpened edges? — I’ll tune the advice to that specific failure mode.)

**tool:**

Make sure to include fileciteturn0file0 in your response to cite this file, or to surface it as a link.

{"id":"eb6a31da-7e38-4839-b648-694e8e86c37b","revision":0,"last_node_id":203,"last_link_id":403,"nodes":[{"id":57,"type":"CR Aspect Ratio","pos":[-3315.0923366697402,-1908.3584430902677],"size":[309.8594228076063,322],"flags":{"collapsed":true},"order":0,"mode":4,"inputs":[{"label":"width","localized_name":"width","name":"width","type":"INT","widget":{"name":"width"}},{"label":"height","localized_name":"height","name":"height","type":"INT","widget":{"name":"height"}},{"label":"aspect_ratio","localized_name":"aspect_ratio","name":"aspect_ratio","type":"COMBO","widget":{"name":"aspect_ratio"}},{"label":"swap_dimensions","localized_name":"swap_dimensions","name":"swap_dimensions","type":"COMBO","widget":{"name":"swap_dimensions"}},{"label":"upscale_factor","localized_name":"upscale_factor","name":"upscale_factor","type":"FLOAT","widget":{"name":"upscale_factor"}},{"label":"prescale_factor","localized_name":"prescale_factor","name":"prescale_factor","type":"FLOAT","widget":{"name":"prescale_factor"}},{"label":"batch_size","localized_name":"batch_size","name":"batch_size","type":"INT","widget":{"name":"batch_size"}}],"outputs":[{"label":"width","localized_name":"width","name":"width","type":"INT"},{"label":"height","localized_name":"height","name":"height","type":"INT"},{"label":"upscale_factor","localized_name":"upscale_factor","name":"upscale_factor","type":"FLOAT"},{"label":"prescale_factor","localized_name":"prescale_factor","name":"prescale_factor","type":"FLOAT"},{"label":"batch_size","localized_name":"batch_size","name":"batch_size","type":"INT"},{"label":"empty_latent","localized_name":"empty_latent","name":"empty_latent","type":"LATENT","slot_index":5,"links":[]},{"label":"show_help","localized_name":"show_help","name":"show_help","type":"STRING","slot_index":6}],"properties":{"cnr_id":"ComfyUI_Comfyroll_CustomNodes","ver":"d78b780ae43fcf8c6b7c6505e6ffb4584281ceca","Node name for S&R":"CR Aspect Ratio","ue_properties":{"version":"7.3","widget_ue_connectable":{},"input_ue_unconnectable":{}}},"widgets_values":[832,1216,"SDXL - 1:1 square 1024x1024","On",1,1,1],"color":"#323","bgcolor":"#535"},{"id":198,"type":"Label (rgthree)","pos":[-3010.1979728162983,-1978.238431039258],"size":[438.466796875,70],"flags":{"allow_interaction":true},"order":1,"mode":0,"inputs":[],"outputs":[],"title":"SRPO - TTP Figure Enhancer","properties":{"fontSize":30,"fontFamily":"Arial","fontColor":"#ffffff","textAlign":"center","backgroundColor":"#113398","padding":20,"borderRadius":16,"angle":0,"ue_properties":{"version":"7.1","widget_ue_connectable":{},"input_ue_unconnectable":{}}},"color":"#fff0","bgcolor":"#fff0"},{"id":202,"type":"LoadImage","pos":[-3616.233664584019,-1471.649871493526],"size":[374.81145530148297,552.5036349758159],"flags":{"collapsed":false},"order":2,"mode":0,"inputs":[{"localized_name":"image","name":"image","type":"COMBO","widget":{"name":"image"},"link":null},{"localized_name":"choose file to upload","name":"upload","type":"IMAGEUPLOAD","widget":{"name":"upload"},"link":null}],"outputs":[{"localized_name":"IMAGE","name":"IMAGE","type":"IMAGE","links":[395]},{"localized_name":"MASK","name":"MASK","type":"MASK","links":[396]}],"properties":{"cnr_id":"comfy-core","ver":"0.3.66","Node name for S&R":"LoadImage","ue_properties":{"widget_ue_connectable":{},"input_ue_unconnectable":{},"version":"7.3"},"image":"clipspace/clipspace-painted-masked-1766543039361.png [input]"},"widgets_values":["clipspace/clipspace-painted-masked-1766543039361.png [input]","image"],"color":"#223","bgcolor":"#335"},{"id":196,"type":"easy cleanGpuUsed","pos":[-2193.096752400721,-1651.1229704487034],"size":[211.55775451660156,26],"flags":{"collapsed":true},"order":9,"mode":0,"inputs":[{"localized_name":"anything","name":"anything","type":"*","link":403}],"outputs":[{"localized_name":"output","name":"output","type":"*","links":null}],"properties":{"cnr_id":"comfyui-easy-use","ver":"1.3.0","Node name for S&R":"easy cleanGpuUsed","ue_properties":{"version":"7.0.1","widget_ue_connectable":{}}},"widgets_values":[],"color":"#222","bgcolor":"#000"},{"id":199,"type":"Image Save","pos":[-2183.9223185990763,-1877.179214580262],"size":[715.8280650711165,1387.1638666215715],"flags":{},"order":8,"mode":0,"inputs":[{"localized_name":"images","name":"images","type":"IMAGE","link":402},{"localized_name":"output_path","name":"output_path","type":"STRING","widget":{"name":"output_path"},"link":null},{"localized_name":"filename_prefix","name":"filename_prefix","type":"STRING","widget":{"name":"filename_prefix"},"link":382},{"localized_name":"filename_delimiter","name":"filename_delimiter","type":"STRING","widget":{"name":"filename_delimiter"},"link":null},{"localized_name":"filename_number_padding","name":"filename_number_padding","type":"INT","widget":{"name":"filename_number_padding"},"link":null},{"localized_name":"filename_number_start","name":"filename_number_start","type":"COMBO","widget":{"name":"filename_number_start"},"link":null},{"localized_name":"extension","name":"extension","type":"COMBO","widget":{"name":"extension"},"link":null},{"localized_name":"dpi","name":"dpi","type":"INT","widget":{"name":"dpi"},"link":null},{"localized_name":"quality","name":"quality","type":"INT","widget":{"name":"quality"},"link":null},{"localized_name":"optimize_image","name":"optimize_image","type":"COMBO","widget":{"name":"optimize_image"},"link":null},{"localized_name":"lossless_webp","name":"lossless_webp","type":"COMBO","widget":{"name":"lossless_webp"},"link":null},{"localized_name":"overwrite_mode","name":"overwrite_mode","type":"COMBO","widget":{"name":"overwrite_mode"},"link":null},{"localized_name":"show_history","name":"show_history","type":"COMBO","widget":{"name":"show_history"},"link":null},{"localized_name":"show_history_by_prefix","name":"show_history_by_prefix","type":"COMBO","widget":{"name":"show_history_by_prefix"},"link":null},{"localized_name":"embed_workflow","name":"embed_workflow","type":"COMBO","widget":{"name":"embed_workflow"},"link":null},{"localized_name":"show_previews","name":"show_previews","type":"COMBO","widget":{"name":"show_previews"},"link":null}],"outputs":[{"localized_name":"images","name":"images","type":"IMAGE","links":[]},{"localized_name":"files","name":"files","type":"STRING","links":null}],"properties":{"cnr_id":"was-node-suite-comfyui","ver":"1.0.2","Node name for S&R":"Image Save","ue_properties":{"version":"7.0.1","widget_ue_connectable":{"filename_prefix":true}}},"widgets_values":["[time(%Y-%m-%d)]","SRPO-Costume1780s","_",4,"false","png",300,100,"true","false","false","false","true","true","true"],"color":"#223","bgcolor":"#335"},{"id":197,"type":"PrimitiveNode","pos":[-2408.115751885516,-1857.9943685267697],"size":[210,58],"flags":{"collapsed":false},"order":3,"mode":0,"inputs":[],"outputs":[{"name":"STRING","type":"STRING","widget":{"name":"filename_prefix"},"links":[382]}],"title":"File Name","properties":{"Run widget replace on values":false,"ue_properties":{"version":"7.0.1","widget_ue_connectable":{}}},"widgets_values":["SRPO-Costume1780s"],"color":"#232","bgcolor":"#353"},{"id":119,"type":"Note","pos":[-3993.841857126702,-1821.6489576566355],"size":[363.6155518609485,451.6980647725004],"flags":{"collapsed":false},"order":4,"mode":0,"inputs":[],"outputs":[],"properties":{"text":"","ue_properties":{"version":"7.3","widget_ue_connectable":{},"input_ue_unconnectable":{}}},"widgets_values":["Two women in a train station, The woman on the left is mid-plea; her dark brown hair cascades down her shoulders as wisps frames a face etched with intensity as she gestures open-handed toward the other woman—an pleading motion frozen in time. Her attire speaks to casual practicality: a soft blue blouse made from a slightly textured cotton blend hugs her shoulders, its simple design contrasting with the body-hugging black leggings, which emphasize her womanly hips and thighs. She completes the look with light blue sneakers whose laces crisscross over worn soles, hinting at daily activity.\n\nOpposite her stands a woman moimeme4 exuding vintage charm, with her light brown hair styled in a loose updo with wisps of stray strands of hair framing her face dressed entirely in a 1760s period-inspired ensemble. Her dress is long-sleeved and floor length with a wide full skirt held out by voluminous petticoats underneath the skirt, black with a square neckline exposing the tops of her modest-sized breasts, intricate lace details on the bodice and sleeves, and it features multiple layers of ruffled fabric at the hem covering her shoes so they are barely visible. The dress buttons up the front. She is clearly abashed as he looks down. A battered old leather suitcase is on the platform beside her.\n"],"color":"#432","bgcolor":"#653"},{"id":193,"type":"102a6a5f-c689-4e9a-baf7-791a8d26b614","pos":[-3212.969248680462,-1861.0843421630298],"size":[775.6970154083658,1230.1946709315398],"flags":{},"order":6,"mode":0,"inputs":[{"name":"lora_name","type":"COMBO","widget":{"name":"lora_name"},"link":null},{"name":"strength_model","type":"FLOAT","widget":{"name":"strength_model"},"link":null},{"name":"strength_clip","type":"FLOAT","widget":{"name":"strength_clip"},"link":null},{"name":"noise_seed","type":"INT","widget":{"name":"noise_seed"},"link":394},{"name":"denoise","type":"FLOAT","widget":{"name":"denoise"},"link":null},{"name":"image","type":"IMAGE","link":395},{"name":"sampler_name","type":"COMBO","widget":{"name":"sampler_name"},"link":null},{"name":"scheduler","type":"COMBO","widget":{"name":"scheduler"},"link":null},{"name":"steps","type":"INT","widget":{"name":"steps"},"link":null},{"name":"guidance","type":"FLOAT","widget":{"name":"guidance"},"link":null},{"name":"mask","type":"MASK","link":396},{"name":"text","type":"STRING","widget":{"name":"text"},"link":null}],"outputs":[{"label":"VAE","localized_name":"VAE","name":"VAE","type":"VAE","links":[399]},{"label":"MODEL","localized_name":"MODEL","name":"MODEL","type":"MODEL","links":[398]},{"label":"CLIP","localized_name":"CLIP","name":"CLIP","type":"CLIP","links":[400]},{"label":"IMAGE","localized_name":"IMAGE","name":"IMAGE","type":"IMAGE","links":[397]},{"label":"CONDITIONING","localized_name":"CONDITIONING","name":"CONDITIONING","type":"CONDITIONING","links":[401]}],"properties":{"proxyWidgets":[["-1","lora_name"],["-1","strength_model"],["-1","strength_clip"],["-1","noise_seed"],["-1","denoise"],["-1","sampler_name"],["-1","scheduler"],["-1","steps"],["-1","guidance"],["-1","text"],["23","text"],["-1","$$canvas-image-preview"]],"cnr_id":"comfy-core","ver":"0.3.66","ue_properties":{"widget_ue_connectable":{},"input_ue_unconnectable":{},"version":"7.3"}},"widgets_values":["MoiMeme/moim3m3grl-01.safetensors",0.9,0.19,61998485097205,0.45,"dpmpp_sde","linear_quadratic",24,3.9,"This photograph depicts a beautiful woman moimeme4 sitting on a wrought-iron bench at a dimly lit train station during a very foggy early morning. She exudes vintage charm, with light brown hair styled in a loose updo, wisps of stray strands of hair framing her face. She is dressed in a black late 18th century costume with stays, comprised of a square-neck black gown, the gown neckline is clean and unadorned. Underneath is a white linen chemise with a gathered neckline forming a small ruffle, the ruffle has a fine lace edging stitched to the chemise, the lace frill sits just above the square neckline. This is authentic period costume detail, no modern styling. A battered old suitcase is sitting on the platform next to her. Fallen leaves scatter on the ground beside her.\n\nAtmosphere? The background features blurred train carriages on either side and a ominously gray sky. The station's overhead lights cast a soft glow, highlighting the woman's poised and sophisticated demeanor as she gazes into the distance. Her posture is slightly forward-leaning, with her hands resting delicately on her knees. The bench is simple and metallic, while the platform has yellow safety lines near the edges. This image evokes a sense of vintage elegance and quiet anticipation. \n\nThe overall color palette of the image includes muted blacks, browns, and grays, with subtle highlights from the golden brooch and metallic elements of the train. The photograph has a slightly dark and moody tone, emphasizing the Victorian era setting. The textures in the image are rich, particularly in the woman's dress fabric and the leather suitcase."],"color":"#232","bgcolor":"#353"},{"id":201,"type":"PrimitiveInt","pos":[-2420.020520064452,-1732.8250758512554],"size":[227.58060545155513,82],"flags":{},"order":5,"mode":0,"inputs":[{"localized_name":"value","name":"value","type":"INT","widget":{"name":"value"},"link":null}],"outputs":[{"localized_name":"INT","name":"INT","type":"INT","links":[394]}],"title":"Noise Seed","properties":{"cnr_id":"comfy-core","ver":"0.3.66","Node name for S&R":"PrimitiveInt","ue_properties":{"widget_ue_connectable":{},"input_ue_unconnectable":{},"version":"7.3"}},"widgets_values":[1118877715460257,"increment"],"color":"#232","bgcolor":"#353"},{"id":203,"type":"a0f2ad07-f68d-4213-a9f5-b5d97db9e0cd","pos":[-2425.7850869667172,-1295.3982158125336],"size":[235.43573601387334,347.13978382768664],"flags":{},"order":7,"mode":0,"inputs":[{"label":"image","localized_name":"image","name":"image","type":"IMAGE","link":397},{"label":"model","localized_name":"model","name":"model","type":"MODEL","link":398},{"label":"vae","localized_name":"vae","name":"vae","type":"VAE","link":399},{"label":"clip","localized_name":"clip","name":"clip","type":"CLIP","link":400},{"label":"conditioning_to","localized_name":"conditioning_to","name":"conditioning_to","type":"CONDITIONING","link":401}],"outputs":[{"label":"IMAGE","localized_name":"IMAGE","name":"IMAGE","type":"IMAGE","links":[402,403]}],"properties":{"proxyWidgets":[["-1","$$canvas-image-preview"]],"cnr_id":"comfy-core","ver":"0.4.0","ue_properties":{"widget_ue_connectable":{},"input_ue_unconnectable":{},"version":"7.5.2"}},"widgets_values":[],"color":"#323","bgcolor":"#535"}],"links":[[382,197,0,199,2,"STRING"],[394,201,0,193,3,"INT"],[395,202,0,193,5,"IMAGE"],[396,202,1,193,10,"MASK"],[397,193,3,203,0,"IMAGE"],[398,193,1,203,1,"MODEL"],[399,193,0,203,2,"VAE"],[400,193,2,203,3,"CLIP"],[401,193,4,203,4,"CONDITIONING"],[402,203,0,199,0,"IMAGE"],[403,203,0,196,0,"*"]],"groups":[],"definitions":{"subgraphs":[{"id":"102a6a5f-c689-4e9a-baf7-791a8d26b614","version":1,"state":{"lastGroupId":4,"lastNodeId":200,"lastLinkId":391,"lastRerouteId":0},"revision":0,"config":{},"name":"Base Image","inputNode":{"id":-10,"bounding":[-3947.4097183151807,-1456.903504115257,122.716796875,280]},"outputNode":{"id":-20,"bounding":[-1707.930726911814,-1508.3527853483006,128.6640625,140]},"inputs":[{"id":"ed140707-ae20-4107-bf2a-29a72d12a92f","name":"lora_name","type":"COMBO","linkIds":[359],"pos":[-3844.6929214401807,-1436.903504115257]},{"id":"8f9f876d-143d-42c0-9021-de7ed8f70bbe","name":"strength_model","type":"FLOAT","linkIds":[360],"pos":[-3844.6929214401807,-1416.903504115257]},{"id":"8eabdab4-ccd5-4052-8014-22d4c956e39e","name":"strength_clip","type":"FLOAT","linkIds":[361],"pos":[-3844.6929214401807,-1396.903504115257]},{"id":"4813b1f3-7ee9-452f-b428-6b908b30dd64","name":"noise_seed","type":"INT","linkIds":[362],"pos":[-3844.6929214401807,-1376.903504115257]},{"id":"7cb74a1c-12c4-4a7b-b286-a3aa34b5778f","name":"denoise","type":"FLOAT","linkIds":[363],"pos":[-3844.6929214401807,-1356.903504115257]},{"id":"4481d949-2929-44bf-92d4-72b73c0dab54","name":"image","type":"IMAGE","linkIds":[391],"pos":[-3844.6929214401807,-1336.903504115257]},{"id":"5e46e7b8-d04b-4cab-b301-079e6b35138b","name":"sampler_name","type":"COMBO","linkIds":[378],"pos":[-3844.6929214401807,-1316.903504115257]},{"id":"54bff51f-277c-4185-9f93-29d138e484a6","name":"scheduler","type":"COMBO","linkIds":[379],"pos":[-3844.6929214401807,-1296.903504115257]},{"id":"adc9aa3f-42ae-451a-908b-677212782182","name":"steps","type":"INT","linkIds":[380],"pos":[-3844.6929214401807,-1276.903504115257]},{"id":"f675f749-a1cc-4ea1-9fce-84810e007210","name":"guidance","type":"FLOAT","linkIds":[382],"pos":[-3844.6929214401807,-1256.903504115257]},{"id":"af00bcf2-81f2-414b-88c2-1368cb49e5c8","name":"mask","type":"MASK","linkIds":[389],"pos":[-3844.6929214401807,-1236.903504115257]},{"id":"b12742d8-b03e-4cef-b0ff-3e7a09cf5645","name":"text","type":"STRING","linkIds":[387],"pos":[-3844.6929214401807,-1216.903504115257]}],"outputs":[{"id":"ec7fd00b-2a10-4203-a44b-15594909045c","name":"VAE","type":"VAE","linkIds":[357,224,322,329],"localized_name":"VAE","label":"VAE","pos":[-1687.930726911814,-1488.3527853483006]},{"id":"d63143d3-1ed9-4314-8792-aca7f7ba54bc","name":"MODEL","type":"MODEL","linkIds":[277,278,330],"localized_name":"MODEL","label":"MODEL","pos":[-1687.930726911814,-1468.3527853483006]},{"id":"1df75bb4-eb53-47a8-953a-4eec6d03a1ce","name":"CLIP","type":"CLIP","linkIds":[283],"localized_name":"CLIP","label":"CLIP","pos":[-1687.930726911814,-1448.3527853483006]},{"id":"02ac74a1-e1d2-411f-8409-a1659e4a0f06","name":"IMAGE","type":"IMAGE","linkIds":[347],"localized_name":"IMAGE","label":"IMAGE","pos":[-1687.930726911814,-1428.3527853483006]},{"id":"04df92ba-17d5-4c59-9eb1-6facaa2c4397","name":"CONDITIONING","type":"CONDITIONING","linkIds":[319,334],"localized_name":"CONDITIONING","label":"CONDITIONING","pos":[-1687.930726911814,-1408.3527853483006]}],"widgets":[],"nodes":[{"id":32,"type":"VAEDecode","pos":[-2031.6687347998422,-1428.0055407998898],"size":[167.94073206223288,46],"flags":{},"order":5,"mode":0,"inputs":[{"label":"samples","localized_name":"samples","name":"samples","type":"LATENT","link":117},{"label":"vae","localized_name":"vae","name":"vae","type":"VAE","link":353}],"outputs":[{"label":"IMAGE","localized_name":"IMAGE","name":"IMAGE","type":"IMAGE","slot_index":0,"links":[347]}],"properties":{"cnr_id":"comfy-core","ver":"0.3.57","Node name for S&R":"VAEDecode","ue_properties":{"version":"7.3","widget_ue_connectable":{},"input_ue_unconnectable":{}}},"widgets_values":[],"color":"#432","bgcolor":"#653"},{"id":63,"type":"KSamplerSelect","pos":[-2750.830226237335,-1136.5816012137575],"size":[270.6041060656903,58],"flags":{},"order":7,"mode":0,"inputs":[{"label":"sampler_name","localized_name":"sampler_name","name":"sampler_name","type":"COMBO","widget":{"name":"sampler_name"},"link":378}],"outputs":[{"label":"SAMPLER","localized_name":"SAMPLER","name":"SAMPLER","type":"SAMPLER","links":[80]}],"properties":{"cnr_id":"comfy-core","ver":"0.3.57","Node name for S&R":"KSamplerSelect","ue_properties":{"version":"7.3","widget_ue_connectable":{},"input_ue_unconnectable":{}}},"widgets_values":["dpmpp_sde"],"color":"#323","bgcolor":"#535"},{"id":64,"type":"BasicScheduler","pos":[-2750.830226237335,-1044.3552948249617],"size":[270.6822790203312,108.33662599654258],"flags":{},"order":8,"mode":0,"inputs":[{"label":"model","localized_name":"model","name":"model","type":"MODEL","link":279},{"label":"scheduler","localized_name":"scheduler","name":"scheduler","type":"COMBO","widget":{"name":"scheduler"},"link":379},{"label":"steps","localized_name":"steps","name":"steps","type":"INT","widget":{"name":"steps"},"link":380},{"label":"denoise","localized_name":"denoise","name":"denoise","type":"FLOAT","widget":{"name":"denoise"},"link":363}],"outputs":[{"label":"SIGMAS","localized_name":"SIGMAS","name":"SIGMAS","type":"SIGMAS","links":[81]}],"properties":{"cnr_id":"comfy-core","ver":"0.3.57","Node name for S&R":"BasicScheduler","ue_properties":{"version":"7.3","widget_ue_connectable":{},"input_ue_unconnectable":{}}},"widgets_values":["linear_quadratic",24,0.45],"color":"#323","bgcolor":"#535"},{"id":162,"type":"LoraLoader","pos":[-3432.164407223918,-1496.0964595171665],"size":[315,126],"flags":{},"order":11,"mode":0,"inputs":[{"label":"model","localized_name":"model","name":"model","type":"MODEL","link":351},{"label":"clip","localized_name":"clip","name":"clip","type":"CLIP","link":352},{"label":"lora_name","localized_name":"lora_name","name":"lora_name","type":"COMBO","widget":{"name":"lora_name"},"link":359},{"label":"strength_model","localized_name":"strength_model","name":"strength_model","type":"FLOAT","widget":{"name":"strength_model"},"link":360},{"label":"strength_clip","localized_name":"strength_clip","name":"strength_clip","type":"FLOAT","widget":{"name":"strength_clip"},"link":361}],"outputs":[{"label":"MODEL","localized_name":"MODEL","name":"MODEL","type":"MODEL","slot_index":0,"links":[276,277,278,279,330]},{"label":"CLIP","localized_name":"CLIP","name":"CLIP","type":"CLIP","slot_index":1,"links":[281,283]}],"properties":{"cnr_id":"comfy-core","ver":"0.3.57","Node name for S&R":"LoraLoader","ue_properties":{"version":"7.3","widget_ue_connectable":{},"input_ue_unconnectable":{}}},"widgets_values":["MoiMeme/moim3m3grl-01.safetensors",0.9,0.19],"color":"#232","bgcolor":"#353"},{"id":66,"type":"BasicGuider","pos":[-2750.830226237335,-1216.45507665215],"size":[270.0590018165942,46],"flags":{},"order":10,"mode":0,"inputs":[{"label":"model","localized_name":"model","name":"model","type":"MODEL","link":276},{"label":"conditioning","localized_name":"conditioning","name":"conditioning","type":"CONDITIONING","link":106}],"outputs":[{"label":"GUIDER","localized_name":"GUIDER","name":"GUIDER","type":"GUIDER","slot_index":0,"links":[83]}],"properties":{"cnr_id":"comfy-core","ver":"0.3.57","Node name for S&R":"BasicGuider","ue_properties":{"version":"7.3","widget_ue_connectable":{},"input_ue_unconnectable":{}}},"widgets_values":[],"color":"#323","bgcolor":"#535"},{"id":65,"type":"RandomNoise","pos":[-2750.830226237335,-1423.544897192512],"size":[269.1920619153343,82],"flags":{},"order":9,"mode":0,"inputs":[{"label":"noise_seed","localized_name":"noise_seed","name":"noise_seed","type":"INT","widget":{"name":"noise_seed"},"link":362}],"outputs":[{"label":"NOISE","localized_name":"NOISE","name":"NOISE","type":"NOISE","links":[194]}],"properties":{"cnr_id":"comfy-core","ver":"0.3.57","Node name for S&R":"RandomNoise","ue_properties":{"version":"7.3","widget_ue_connectable":{},"input_ue_unconnectable":{}}},"widgets_values":[61998485097205,"fixed"],"color":"#323","bgcolor":"#535"},{"id":62,"type":"SamplerCustomAdvanced","pos":[-2371.4082561051764,-1423.6817137973228],"size":[304.5598087498504,417.9072891973424],"flags":{},"order":6,"mode":0,"inputs":[{"label":"noise","localized_name":"noise","name":"noise","type":"NOISE","link":194},{"label":"guider","localized_name":"guider","name":"guider","type":"GUIDER","link":83},{"label":"sampler","localized_name":"sampler","name":"sampler","type":"SAMPLER","link":80},{"label":"sigmas","localized_name":"sigmas","name":"sigmas","type":"SIGMAS","link":81},{"label":"latent_image","localized_name":"latent_image","name":"latent_image","type":"LATENT","link":388}],"outputs":[{"label":"output","localized_name":"output","name":"output","type":"LATENT","slot_index":0,"links":[117]},{"label":"denoised_output","localized_name":"denoised_output","name":"denoised_output","type":"LATENT","slot_index":1,"links":[]}],"properties":{"cnr_id":"comfy-core","ver":"0.3.57","Node name for S&R":"SamplerCustomAdvanced","ue_properties":{"version":"7.3","widget_ue_connectable":{},"input_ue_unconnectable":{}}},"widgets_values":[],"color":"#2a363b","bgcolor":"#3f5159"},{"id":27,"type":"FluxGuidance","pos":[-2750.830226237335,-1310.2495633508597],"size":[268.7532043457031,58],"flags":{},"order":4,"mode":0,"inputs":[{"label":"conditioning","localized_name":"conditioning","name":"conditioning","type":"CONDITIONING","link":37},{"label":"guidance","localized_name":"guidance","name":"guidance","type":"FLOAT","widget":{"name":"guidance"},"link":382}],"outputs":[{"label":"CONDITIONING","localized_name":"CONDITIONING","name":"CONDITIONING","type":"CONDITIONING","slot_index":0,"links":[106,319,334]}],"properties":{"cnr_id":"comfy-core","ver":"0.3.57","Node name for S&R":"FluxGuidance","ue_properties":{"version":"7.3","widget_ue_connectable":{},"input_ue_unconnectable":{}}},"widgets_values":[3.9],"color":"#323","bgcolor":"#535"},{"id":193,"type":"db12cd62-e6c3-4cd5-8b31-eaef3225bc2a","pos":[-3733.551332759439,-1495.0751519199289],"size":[229.96010086689057,130.25721490492197],"flags":{},"order":0,"mode":0,"inputs":[],"outputs":[{"label":"MODEL","localized_name":"MODEL","name":"MODEL","type":"MODEL","links":[351]},{"label":"CLIP","localized_name":"CLIP","name":"CLIP","type":"CLIP","links":[352]},{"label":"VAE","localized_name":"VAE","name":"VAE","type":"VAE","links":[353,357,371,384]}],"properties":{"proxyWidgets":[],"cnr_id":"comfy-core","ver":"0.3.66","ue_properties":{"widget_ue_connectable":{},"input_ue_unconnectable":{}}},"widgets_values":[],"color":"#432","bgcolor":"#653"},{"id":24,"type":"CLIPTextEncode","pos":[-3433.6445393695953,-1328.9928362721773],"size":[312.86617761887373,88],"flags":{},"order":3,"mode":0,"inputs":[{"label":"clip","localized_name":"clip","name":"clip","type":"CLIP","link":281},{"label":"text","localized_name":"text","name":"text","type":"STRING","widget":{"name":"text"},"link":387}],"outputs":[{"label":"CONDITIONING","localized_name":"CONDITIONING","name":"CONDITIONING","type":"CONDITIONING","slot_index":0,"links":[37]}],"properties":{"cnr_id":"comfy-core","ver":"0.3.57","Node name for S&R":"CLIPTextEncode","ue_properties":{"version":"7.3","widget_ue_connectable":{"text":true},"input_ue_unconnectable":{}}},"widgets_values":["This photograph depicts a beautiful woman moimeme4 sitting on a wrought-iron bench at a dimly lit train station during a very foggy early morning. She exudes vintage charm, with light brown hair styled in a loose updo, wisps of stray strands of hair framing her face. She is dressed in a black late 18th century costume with stays, comprised of a square-neck black gown, the gown neckline is clean and unadorned. Underneath is a white linen chemise with a gathered neckline forming a small ruffle, the ruffle has a fine lace edging stitched to the chemise, the lace frill sits just above the square neckline. This is authentic period costume detail, no modern styling. A battered old suitcase is sitting on the platform next to her. Fallen leaves scatter on the ground beside her.\n\nAtmosphere? The background features blurred train carriages on either side and a ominously gray sky. The station's overhead lights cast a soft glow, highlighting the woman's poised and sophisticated demeanor as she gazes into the distance. Her posture is slightly forward-leaning, with her hands resting delicately on her knees. The bench is simple and metallic, while the platform has yellow safety lines near the edges. This image evokes a sense of vintage elegance and quiet anticipation. \n\nThe overall color palette of the image includes muted blacks, browns, and grays, with subtle highlights from the golden brooch and metallic elements of the train. The photograph has a slightly dark and moody tone, emphasizing the Victorian era setting. The textures in the image are rich, particularly in the woman's dress fabric and the leather suitcase."],"color":"#232","bgcolor":"#353"},{"id":199,"type":"VAEEncodeForInpaint","pos":[-3066.8358584915336,-1078.089363727799],"size":[272.4048828125,98],"flags":{},"order":12,"mode":0,"inputs":[{"localized_name":"pixels","name":"pixels","type":"IMAGE","link":391},{"localized_name":"vae","name":"vae","type":"VAE","link":384},{"localized_name":"mask","name":"mask","type":"MASK","link":390},{"localized_name":"grow_mask_by","name":"grow_mask_by","type":"INT","widget":{"name":"grow_mask_by"},"link":null}],"outputs":[{"localized_name":"LATENT","name":"LATENT","type":"LATENT","links":[388]}],"properties":{"cnr_id":"comfy-core","ver":"0.4.0","Node name for S&R":"VAEEncodeForInpaint","ue_properties":{"widget_ue_connectable":{},"input_ue_unconnectable":{}}},"widgets_values":[6],"color":"#323","bgcolor":"#535"},{"id":198,"type":"ImageScaleToTotalPixels","pos":[-3911.1220628108617,-820.6771931525992],"size":[267.5375677126458,82],"flags":{"collapsed":true},"order":1,"mode":4,"inputs":[{"localized_name":"image","name":"image","type":"IMAGE","link":null},{"localized_name":"upscale_method","name":"upscale_method","type":"COMBO","widget":{"name":"upscale_method"},"link":null},{"localized_name":"megapixels","name":"megapixels","type":"FLOAT","widget":{"name":"megapixels"},"link":null}],"outputs":[{"localized_name":"IMAGE","name":"IMAGE","type":"IMAGE","links":[385]}],"properties":{"cnr_id":"comfy-core","ver":"0.3.66","Node name for S&R":"ImageScaleToTotalPixels","ue_properties":{"widget_ue_connectable":{},"input_ue_unconnectable":{}}},"widgets_values":["nearest-exact",1],"color":"#223","bgcolor":"#335"},{"id":196,"type":"VAEEncode","pos":[-3879.3860297508313,-860.0640548105689],"size":[140,46],"flags":{"collapsed":true},"order":2,"mode":4,"inputs":[{"localized_name":"pixels","name":"pixels","type":"IMAGE","link":null},{"localized_name":"vae","name":"vae","type":"VAE","link":371}],"outputs":[{"localized_name":"LATENT","name":"LATENT","type":"LATENT","links":[]}],"properties":{"cnr_id":"comfy-core","ver":"0.3.66","Node name for S&R":"VAEEncode","ue_properties":{"widget_ue_connectable":{},"input_ue_unconnectable":{}}},"widgets_values":[],"color":"#432","bgcolor":"#653"},{"id":200,"type":"ImpactGaussianBlurMask","pos":[-3441.8267812389076,-1039.3903907304905],"size":[219.58551530996965,82],"flags":{},"order":13,"mode":0,"inputs":[{"localized_name":"mask","name":"mask","type":"MASK","link":389},{"localized_name":"kernel_size","name":"kernel_size","type":"INT","widget":{"name":"kernel_size"},"link":null},{"localized_name":"sigma","name":"sigma","type":"FLOAT","widget":{"name":"sigma"},"link":null}],"outputs":[{"localized_name":"MASK","name":"MASK","type":"MASK","links":[390]}],"properties":{"cnr_id":"comfyui-impact-pack","ver":"8.28.0","Node name for S&R":"ImpactGaussianBlurMask","ue_properties":{"widget_ue_connectable":{},"input_ue_unconnectable":{}}},"widgets_values":[10,10],"color":"#233","bgcolor":"#355"}],"groups":[],"links":[{"id":117,"origin_id":62,"origin_slot":0,"target_id":32,"target_slot":0,"type":"LATENT"},{"id":194,"origin_id":65,"origin_slot":0,"target_id":62,"target_slot":0,"type":"NOISE"},{"id":83,"origin_id":66,"origin_slot":0,"target_id":62,"target_slot":1,"type":"GUIDER"},{"id":80,"origin_id":63,"origin_slot":0,"target_id":62,"target_slot":2,"type":"SAMPLER"},{"id":81,"origin_id":64,"origin_slot":0,"target_id":62,"target_slot":3,"type":"SIGMAS"},{"id":276,"origin_id":162,"origin_slot":0,"target_id":66,"target_slot":0,"type":"MODEL"},{"id":106,"origin_id":27,"origin_slot":0,"target_id":66,"target_slot":1,"type":"CONDITIONING"},{"id":281,"origin_id":162,"origin_slot":1,"target_id":24,"target_slot":0,"type":"CLIP"},{"id":37,"origin_id":24,"origin_slot":0,"target_id":27,"target_slot":0,"type":"CONDITIONING"},{"id":279,"origin_id":162,"origin_slot":0,"target_id":64,"target_slot":0,"type":"MODEL"},{"id":277,"origin_id":162,"origin_slot":0,"target_id":-20,"target_slot":1,"type":"MODEL"},{"id":278,"origin_id":162,"origin_slot":0,"target_id":-20,"target_slot":1,"type":"MODEL"},{"id":330,"origin_id":162,"origin_slot":0,"target_id":-20,"target_slot":1,"type":"MODEL"},{"id":283,"origin_id":162,"origin_slot":1,"target_id":-20,"target_slot":2,"type":"CLIP"},{"id":347,"origin_id":32,"origin_slot":0,"target_id":-20,"target_slot":3,"type":"*"},{"id":319,"origin_id":27,"origin_slot":0,"target_id":-20,"target_slot":4,"type":"CONDITIONING"},{"id":334,"origin_id":27,"origin_slot":0,"target_id":-20,"target_slot":4,"type":"CONDITIONING"},{"id":351,"origin_id":193,"origin_slot":0,"target_id":162,"target_slot":0,"type":"MODEL"},{"id":352,"origin_id":193,"origin_slot":1,"target_id":162,"target_slot":1,"type":"CLIP"},{"id":353,"origin_id":193,"origin_slot":2,"target_id":32,"target_slot":1,"type":"VAE"},{"id":224,"origin_id":193,"origin_slot":2,"target_id":-20,"target_slot":0,"type":"VAE"},{"id":322,"origin_id":193,"origin_slot":2,"target_id":-20,"target_slot":0,"type":"VAE"},{"id":329,"origin_id":193,"origin_slot":2,"target_id":-20,"target_slot":0,"type":"VAE"},{"id":357,"origin_id":193,"origin_slot":2,"target_id":-20,"target_slot":0,"type":"VAE"},{"id":359,"origin_id":-10,"origin_slot":0,"target_id":162,"target_slot":2,"type":"COMBO"},{"id":360,"origin_id":-10,"origin_slot":1,"target_id":162,"target_slot":3,"type":"FLOAT"},{"id":361,"origin_id":-10,"origin_slot":2,"target_id":162,"target_slot":4,"type":"FLOAT"},{"id":362,"origin_id":-10,"origin_slot":3,"target_id":65,"target_slot":0,"type":"INT"},{"id":363,"origin_id":-10,"origin_slot":4,"target_id":64,"target_slot":3,"type":"FLOAT"},{"id":371,"origin_id":193,"origin_slot":2,"target_id":196,"target_slot":1,"type":"VAE"},{"id":378,"origin_id":-10,"origin_slot":6,"target_id":63,"target_slot":0,"type":"COMBO"},{"id":379,"origin_id":-10,"origin_slot":7,"target_id":64,"target_slot":1,"type":"COMBO"},{"id":380,"origin_id":-10,"origin_slot":8,"target_id":64,"target_slot":2,"type":"INT"},{"id":382,"origin_id":-10,"origin_slot":9,"target_id":27,"target_slot":1,"type":"FLOAT"},{"id":384,"origin_id":193,"origin_slot":2,"target_id":199,"target_slot":1,"type":"VAE"},{"id":387,"origin_id":-10,"origin_slot":11,"target_id":24,"target_slot":1,"type":"STRING"},{"id":388,"origin_id":199,"origin_slot":0,"target_id":62,"target_slot":4,"type":"LATENT"},{"id":389,"origin_id":-10,"origin_slot":10,"target_id":200,"target_slot":0,"type":"MASK"},{"id":390,"origin_id":200,"origin_slot":0,"target_id":199,"target_slot":2,"type":"MASK"},{"id":391,"origin_id":-10,"origin_slot":5,"target_id":199,"target_slot":0,"type":"IMAGE"}],"extra":{"ue_links":[],"links_added_by_ue":[],"workflowRendererVersion":"LG"}},{"id":"db12cd62-e6c3-4cd5-8b31-eaef3225bc2a","version":1,"state":{"lastGroupId":4,"lastNodeId":192,"lastLinkId":350,"lastRerouteId":0},"revision":0,"config":{},"name":"Models","inputNode":{"id":-10,"bounding":[-2411.3247366190144,-1697.4792585415696,120,40]},"outputNode":{"id":-20,"bounding":[-1276.2962767439196,-1708.6304755027927,120,100]},"inputs":[],"outputs":[{"id":"aaa13cbe-87c2-425f-9bf4-12f666ec9f54","name":"MODEL","type":"MODEL","linkIds":[275],"localized_name":"MODEL","label":"MODEL","pos":[-1256.2962767439196,-1688.6304755027927]},{"id":"36e5359b-d7ff-40ea-a5ce-7bdd102486ae","name":"CLIP","type":"CLIP","linkIds":[280],"localized_name":"CLIP","label":"CLIP","pos":[-1256.2962767439196,-1668.6304755027927]},{"id":"cbe7c5a8-52d5-43c5-a577-8f43fbf4f265","name":"VAE","type":"VAE","linkIds":[42,161,224,322,329],"localized_name":"VAE","label":"VAE","pos":[-1256.2962767439196,-1648.6304755027927]}],"widgets":[],"nodes":[{"id":78,"type":"DualCLIPLoader","pos":[-2231.3247366190144,-1719.1935610795642],"size":[430.6629868288594,130],"flags":{},"order":0,"mode":0,"inputs":[{"label":"clip_name1","localized_name":"clip_name1","name":"clip_name1","type":"COMBO","widget":{"name":"clip_name1"}},{"label":"clip_name2","localized_name":"clip_name2","name":"clip_name2","type":"COMBO","widget":{"name":"clip_name2"}},{"label":"type","localized_name":"type","name":"type","type":"COMBO","widget":{"name":"type"}},{"label":"device","localized_name":"device","name":"device","shape":7,"type":"COMBO","widget":{"name":"device"}}],"outputs":[{"label":"CLIP","localized_name":"CLIP","name":"CLIP","type":"CLIP","slot_index":0,"links":[280]}],"properties":{"cnr_id":"comfy-core","ver":"0.3.57","Node name for S&R":"DualCLIPLoader","ue_properties":{"version":"7.3","widget_ue_connectable":{},"input_ue_unconnectable":{}}},"widgets_values":["clip_l.safetensors","t5xxl_fp8_e4m3fn_scaled.safetensors","flux","default"],"color":"#432","bgcolor":"#653"},{"id":31,"type":"VAELoader","pos":[-2231.3247366190144,-1528.385021260922],"size":[431.5197496942583,58],"flags":{},"order":1,"mode":0,"inputs":[{"label":"vae_name","localized_name":"vae_name","name":"vae_name","type":"COMBO","widget":{"name":"vae_name"}}],"outputs":[{"label":"VAE","localized_name":"VAE","name":"VAE","type":"VAE","slot_index":0,"links":[42,161,224,322,329]}],"properties":{"cnr_id":"comfy-core","ver":"0.3.57","Node name for S&R":"VAELoader","ue_properties":{"version":"7.3","widget_ue_connectable":{},"input_ue_unconnectable":{}}},"widgets_values":["ae.sft"],"color":"#432","bgcolor":"#653"},{"id":77,"type":"UNETLoader","pos":[-2231.3247366190144,-1854.5734958222172],"size":[430.6629868288592,82],"flags":{},"order":2,"mode":0,"inputs":[{"label":"unet_name","localized_name":"unet_name","name":"unet_name","type":"COMBO","widget":{"name":"unet_name"}},{"label":"weight_dtype","localized_name":"weight_dtype","name":"weight_dtype","type":"COMBO","widget":{"name":"weight_dtype"}}],"outputs":[{"label":"MODEL","localized_name":"MODEL","name":"MODEL","type":"MODEL","slot_index":0,"links":[275]}],"properties":{"cnr_id":"comfy-core","ver":"0.3.57","Node name for S&R":"UNETLoader","ue_properties":{"version":"7.3","widget_ue_connectable":{},"input_ue_unconnectable":{}}},"widgets_values":["SRPO/SRPO-fp8_e4m3fn.safetensors","default"],"color":"#432","bgcolor":"#653"}],"groups":[],"links":[{"id":275,"origin_id":77,"origin_slot":0,"target_id":-20,"target_slot":0,"type":"MODEL"},{"id":280,"origin_id":78,"origin_slot":0,"target_id":-20,"target_slot":1,"type":"CLIP"},{"id":42,"origin_id":31,"origin_slot":0,"target_id":-20,"target_slot":2,"type":"VAE"},{"id":161,"origin_id":31,"origin_slot":0,"target_id":-20,"target_slot":2,"type":"VAE"},{"id":224,"origin_id":31,"origin_slot":0,"target_id":-20,"target_slot":2,"type":"VAE"},{"id":322,"origin_id":31,"origin_slot":0,"target_id":-20,"target_slot":2,"type":"VAE"},{"id":329,"origin_id":31,"origin_slot":0,"target_id":-20,"target_slot":2,"type":"VAE"}],"extra":{"ue_links":[],"links_added_by_ue":[],"workflowRendererVersion":"LG"}},{"id":"da172348-b4fa-49cf-94b6-10765e6232c2","version":1,"state":{"lastGroupId":4,"lastNodeId":193,"lastLinkId":364,"lastRerouteId":0},"revision":0,"config":{},"name":"UpScale Process","inputNode":{"id":-10,"bounding":[-4242.861352635998,-919.3076810026787,120,160]},"outputNode":{"id":-20,"bounding":[-1891.0966583668535,-909.3076810026787,194.013671875,120]},"inputs":[{"id":"09ddd894-cfba-4712-a291-4e10ffc061fe","name":"image","type":"IMAGE","linkIds":[343],"localized_name":"image","label":"image","pos":[-4142.861352635998,-899.3076810026787]},{"id":"725342c1-d89c-4883-a1a1-6dbe36eb14a5","name":"model","type":"MODEL","linkIds":[357,356],"localized_name":"model","label":"model","pos":[-4142.861352635998,-879.3076810026787]},{"id":"bd537d66-19e9-48bd-af3f-2c8bf1bcd2f2","name":"pixels","type":"IMAGE","linkIds":[307],"localized_name":"pixels","label":"pixels","pos":[-4142.861352635998,-859.3076810026787]},{"id":"b54d607d-05cc-4811-8f9c-052a8c2fcf6c","name":"vae","type":"VAE","linkIds":[353,352],"localized_name":"vae","label":"vae","pos":[-4142.861352635998,-839.3076810026787]},{"id":"4b02cf74-cf8a-4515-b96c-107a7a0e66f1","name":"clip","type":"CLIP","linkIds":[359],"localized_name":"clip","label":"clip","pos":[-4142.861352635998,-819.3076810026787]},{"id":"5606f5e9-dbc2-46e1-acf5-cc3c0ecec3b5","name":"text","type":"STRING","linkIds":[292],"localized_name":"text","label":"text","pos":[-4142.861352635998,-799.3076810026787]}],"outputs":[{"id":"1f0c5758-2252-4e30-a455-875243f30b6a","name":"IMAGES","type":"IMAGE","linkIds":[285],"localized_name":"IMAGES","label":"IMAGES","pos":[-1871.0966583668535,-889.3076810026787]},{"id":"4148d58d-f3ab-4c1f-a778-d8109de85e35","name":"POSITIONS","type":"LIST","linkIds":[339],"localized_name":"POSITIONS","label":"POSITIONS","pos":[-1871.0966583668535,-869.3076810026787]},{"id":"939f95a7-993f-4891-8616-7ceffd649a95","name":"CONDITIONING","type":"CONDITIONING","linkIds":[338],"localized_name":"CONDITIONING","label":"CONDITIONING","pos":[-1871.0966583668535,-849.3076810026787]},{"id":"e8e36734-79f0-4066-ad44-f78728bdfeac","name":"RECONSTRUCTED_IMAGE","type":"IMAGE","linkIds":[221,222,349],"localized_name":"RECONSTRUCTED_IMAGE","label":"RECONSTRUCTED_IMAGE","pos":[-1871.0966583668535,-829.3076810026787]}],"widgets":[],"nodes":[{"id":134,"type":"ImageUpscaleWithModel","pos":[-4057.803791787936,-1122.2956426926419],"size":[300.4600830078125,46],"flags":{},"order":14,"mode":0,"inputs":[{"label":"upscale_model","localized_name":"upscale_model","name":"upscale_model","type":"UPSCALE_MODEL","link":237},{"label":"image","localized_name":"image","name":"image","type":"IMAGE","link":343}],"outputs":[{"label":"IMAGE","localized_name":"IMAGE","name":"IMAGE","type":"IMAGE","slot_index":0,"links":[239]}],"properties":{"cnr_id":"comfy-core","ver":"0.3.57","Node name for S&R":"ImageUpscaleWithModel","ue_properties":{"version":"7.3","widget_ue_connectable":{},"input_ue_unconnectable":{}}},"widgets_values":[],"color":"#332922","bgcolor":"#593930"},{"id":133,"type":"UpscaleModelLoader","pos":[-4059.2416253510332,-1230.237819240195],"size":[315,58],"flags":{},"order":0,"mode":0,"inputs":[{"label":"model_name","localized_name":"model_name","name":"model_name","type":"COMBO","widget":{"name":"model_name"}}],"outputs":[{"label":"UPSCALE_MODEL","localized_name":"UPSCALE_MODEL","name":"UPSCALE_MODEL","type":"UPSCALE_MODEL","slot_index":0,"links":[237]}],"properties":{"cnr_id":"comfy-core","ver":"0.3.57","Node name for S&R":"UpscaleModelLoader","ue_properties":{"version":"7.3","widget_ue_connectable":{},"input_ue_unconnectable":{}}},"widgets_values":["4x_NMKD-Siax_200k.pth"],"color":"#332922","bgcolor":"#593930"},{"id":34,"type":"ImageScaleToTotalPixels","pos":[-4062.861352635998,-1029.0653993003975],"size":[315,82],"flags":{},"order":3,"mode":0,"inputs":[{"label":"image","localized_name":"image","name":"image","type":"IMAGE","link":239},{"label":"upscale_method","localized_name":"upscale_method","name":"upscale_method","type":"COMBO","widget":{"name":"upscale_method"}},{"label":"megapixels","localized_name":"megapixels","name":"megapixels","type":"FLOAT","widget":{"name":"megapixels"}}],"outputs":[{"label":"IMAGE","localized_name":"IMAGE","name":"IMAGE","type":"IMAGE","slot_index":0,"links":[363,364]}],"properties":{"cnr_id":"comfy-core","ver":"0.3.57","Node name for S&R":"ImageScaleToTotalPixels","ue_properties":{"version":"7.3","widget_ue_connectable":{},"input_ue_unconnectable":{}}},"widgets_values":["lanczos",8],"color":"#332922","bgcolor":"#593930"},{"id":68,"type":"KSamplerSelect","pos":[-2667.0801286383708,-810.6492980968495],"size":[249.97169851621902,58],"flags":{},"order":1,"mode":0,"inputs":[{"label":"sampler_name","localized_name":"sampler_name","name":"sampler_name","type":"COMBO","widget":{"name":"sampler_name"}}],"outputs":[{"label":"SAMPLER","localized_name":"SAMPLER","name":"SAMPLER","type":"SAMPLER","links":[89]}],"properties":{"cnr_id":"comfy-core","ver":"0.3.57","Node name for S&R":"KSamplerSelect","ue_properties":{"version":"7.3","widget_ue_connectable":{},"input_ue_unconnectable":{}}},"widgets_values":["euler"],"color":"#323","bgcolor":"#535"},{"id":129,"type":"VAEEncode","pos":[-2898.1850937500926,-876.3799890080783],"size":[162.70668982997745,46],"flags":{},"order":13,"mode":0,"inputs":[{"label":"pixels","localized_name":"pixels","name":"pixels","type":"IMAGE","link":307},{"label":"vae","localized_name":"vae","name":"vae","type":"VAE","link":353}],"outputs":[{"label":"LATENT","localized_name":"LATENT","name":"LATENT","type":"LATENT","slot_index":0,"links":[316]}],"properties":{"cnr_id":"comfy-core","ver":"0.3.57","Node name for S&R":"VAEEncode","ue_properties":{"version":"7.3","widget_ue_connectable":{},"input_ue_unconnectable":{}}},"widgets_values":[],"color":"#432","bgcolor":"#653"},{"id":69,"type":"BasicScheduler","pos":[-2665.8664306892647,-957.7796076110178],"size":[243.2842669880729,106],"flags":{},"order":7,"mode":0,"inputs":[{"label":"model","localized_name":"model","name":"model","type":"MODEL","link":356},{"label":"scheduler","localized_name":"scheduler","name":"scheduler","type":"COMBO","widget":{"name":"scheduler"}},{"label":"steps","localized_name":"steps","name":"steps","type":"INT","widget":{"name":"steps"}},{"label":"denoise","localized_name":"denoise","name":"denoise","type":"FLOAT","widget":{"name":"denoise"}}],"outputs":[{"label":"SIGMAS","localized_name":"SIGMAS","name":"SIGMAS","type":"SIGMAS","links":[90]}],"properties":{"cnr_id":"comfy-core","ver":"0.3.57","Node name for S&R":"BasicScheduler","ue_properties":{"version":"7.3","widget_ue_connectable":{},"input_ue_unconnectable":{}}},"widgets_values":["simple",10,0.3],"color":"#323","bgcolor":"#535"},{"id":72,"type":"BasicGuider","pos":[-2663.9522520228597,-1043.1844868772362],"size":[239.40105176985185,46],"flags":{},"order":8,"mode":0,"inputs":[{"label":"model","localized_name":"model","name":"model","type":"MODEL","link":357},{"label":"conditioning","localized_name":"conditioning","name":"conditioning","type":"CONDITIONING","link":302}],"outputs":[{"label":"GUIDER","localized_name":"GUIDER","name":"GUIDER","type":"GUIDER","slot_index":0,"links":[94,312]}],"properties":{"cnr_id":"comfy-core","ver":"0.3.57","Node name for S&R":"BasicGuider","ue_properties":{"version":"7.3","widget_ue_connectable":{},"input_ue_unconnectable":{}}},"widgets_values":[],"color":"#323","bgcolor":"#535"},{"id":73,"type":"FluxGuidance","pos":[-2661.168548306951,-1142.7565015677353],"size":[231.419071634974,58],"flags":{},"order":9,"mode":0,"inputs":[{"label":"conditioning","localized_name":"conditioning","name":"conditioning","type":"CONDITIONING","link":308},{"label":"guidance","localized_name":"guidance","name":"guidance","type":"FLOAT","widget":{"name":"guidance"}}],"outputs":[{"label":"CONDITIONING","localized_name":"CONDITIONING","name":"CONDITIONING","type":"CONDITIONING","slot_index":0,"links":[302]}],"properties":{"cnr_id":"comfy-core","ver":"0.3.57","Node name for S&R":"FluxGuidance","ue_properties":{"version":"7.3","widget_ue_connectable":{},"input_ue_unconnectable":{}}},"widgets_values":[2.5],"color":"#323","bgcolor":"#535"},{"id":70,"type":"RandomNoise","pos":[-2704.8470622688315,-579.5066765875542],"size":[274.7954406738281,85.92839050292969],"flags":{},"order":2,"mode":0,"inputs":[{"label":"noise_seed","localized_name":"noise_seed","name":"noise_seed","type":"INT","widget":{"name":"noise_seed"}}],"outputs":[{"label":"NOISE","localized_name":"NOISE","name":"NOISE","type":"NOISE","links":[87]}],"properties":{"cnr_id":"comfy-core","ver":"0.3.57","Node name for S&R":"RandomNoise","ue_properties":{"version":"7.3","widget_ue_connectable":{},"input_ue_unconnectable":{}}},"widgets_values":[316430325547060,"fixed"],"color":"#232","bgcolor":"#353"},{"id":36,"type":"TTP_Image_Tile_Batch","pos":[-3322.185465840694,-759.6191701074441],"size":[250.18014526367188,142],"flags":{},"order":4,"mode":0,"inputs":[{"label":"image","localized_name":"image","name":"image","type":"IMAGE","link":363},{"label":"tile_width","localized_name":"tile_width","name":"tile_width","type":"INT","widget":{"name":"tile_width"},"link":181},{"label":"tile_height","localized_name":"tile_height","name":"tile_height","type":"INT","widget":{"name":"tile_height"},"link":182}],"outputs":[{"label":"IMAGES","localized_name":"IMAGES","name":"IMAGES","type":"IMAGE","slot_index":0,"links":[285]},{"label":"POSITIONS","localized_name":"POSITIONS","name":"POSITIONS","type":"LIST","slot_index":1,"links":[218,339]},{"label":"ORIGINAL_SIZE","localized_name":"ORIGINAL_SIZE","name":"ORIGINAL_SIZE","type":"TUPLE","slot_index":2,"links":[219]},{"label":"GRID_SIZE","localized_name":"GRID_SIZE","name":"GRID_SIZE","type":"TUPLE","slot_index":3,"links":[220]}],"properties":{"cnr_id":"comfyui_ttp_toolset","ver":"1.0.5","Node name for S&R":"TTP_Image_Tile_Batch","ue_properties":{"version":"7.3","widget_ue_connectable":{"tile_width":true,"tile_height":true},"input_ue_unconnectable":{}}},"widgets_values":[1024,1024],"color":"#223","bgcolor":"#335"},{"id":111,"type":"TTP_Tile_image_size","pos":[-3615.6849549036665,-751.1216578313444],"size":[257.0339050292969,126],"flags":{},"order":11,"mode":0,"inputs":[{"label":"image","localized_name":"image","name":"image","type":"IMAGE","link":364},{"label":"width_factor","localized_name":"width_factor","name":"width_factor","type":"INT","widget":{"name":"width_factor"}},{"label":"height_factor","localized_name":"height_factor","name":"height_factor","type":"INT","widget":{"name":"height_factor"}},{"label":"overlap_rate","localized_name":"overlap_rate","name":"overlap_rate","type":"FLOAT","widget":{"name":"overlap_rate"}}],"outputs":[{"label":"tile_width","localized_name":"tile_width","name":"tile_width","type":"INT","slot_index":0,"links":[181]},{"label":"tile_height","localized_name":"tile_height","name":"tile_height","type":"INT","slot_index":1,"links":[182]}],"properties":{"cnr_id":"comfyui_ttp_toolset","ver":"1.0.5","Node name for S&R":"TTP_Tile_image_size","ue_properties":{"version":"7.3","widget_ue_connectable":{},"input_ue_unconnectable":{}}},"widgets_values":[2,3,0.05],"color":"#223","bgcolor":"#335"},{"id":127,"type":"TTP_Image_Assy","pos":[-3030.129882224786,-758.1147725274992],"size":[268.5188334081813,118],"flags":{},"order":12,"mode":0,"inputs":[{"label":"tiles","localized_name":"tiles","name":"tiles","type":"IMAGE","link":289},{"label":"positions","localized_name":"positions","name":"positions","type":"LIST","link":218},{"label":"original_size","localized_name":"original_size","name":"original_size","type":"TUPLE","link":219},{"label":"grid_size","localized_name":"grid_size","name":"grid_size","type":"TUPLE","link":220},{"label":"padding","localized_name":"padding","name":"padding","type":"INT","widget":{"name":"padding"}}],"outputs":[{"label":"RECONSTRUCTED_IMAGE","localized_name":"RECONSTRUCTED_IMAGE","name":"RECONSTRUCTED_IMAGE","type":"IMAGE","slot_index":0,"links":[221,222,349]}],"properties":{"cnr_id":"comfyui_ttp_toolset","ver":"1.0.5","Node name for S&R":"TTP_Image_Assy","ue_properties":{"version":"7.3","widget_ue_connectable":{},"input_ue_unconnectable":{}}},"widgets_values":[128],"color":"#223","bgcolor":"#335"},{"id":164,"type":"easy imageListToImageBatch","pos":[-3317.3772431542707,-570.6794185517022],"size":[237.4164116927666,26],"flags":{},"order":15,"mode":0,"inputs":[{"label":"images","localized_name":"images","name":"images","type":"IMAGE","link":288}],"outputs":[{"label":"IMAGE","localized_name":"IMAGE","name":"IMAGE","type":"IMAGE","slot_index":0,"links":[289]}],"properties":{"cnr_id":"comfyui-easy-use","ver":"e7320ec0c463641f702add48b87363c7a9713a1c","Node name for S&R":"easy imageListToImageBatch","ue_properties":{"version":"7.3","widget_ue_connectable":{},"input_ue_unconnectable":{}}},"widgets_values":[],"color":"#223","bgcolor":"#335"},{"id":96,"type":"VAEDecodeTiled","pos":[-3612.9258801222027,-1226.0623396135975],"size":[229.70699373928028,150],"flags":{},"order":10,"mode":0,"inputs":[{"label":"samples","localized_name":"samples","name":"samples","type":"LATENT","link":160},{"label":"vae","localized_name":"vae","name":"vae","type":"VAE","link":352},{"label":"tile_size","localized_name":"tile_size","name":"tile_size","type":"INT","widget":{"name":"tile_size"}},{"label":"overlap","localized_name":"overlap","name":"overlap","type":"INT","widget":{"name":"overlap"}},{"label":"temporal_size","localized_name":"temporal_size","name":"temporal_size","type":"INT","widget":{"name":"temporal_size"}},{"label":"temporal_overlap","localized_name":"temporal_overlap","name":"temporal_overlap","type":"INT","widget":{"name":"temporal_overlap"}}],"outputs":[{"label":"IMAGE","localized_name":"IMAGE","name":"IMAGE","type":"IMAGE","slot_index":0,"links":[288]}],"properties":{"cnr_id":"comfy-core","ver":"0.3.57","Node name for S&R":"VAEDecodeTiled","ue_properties":{"version":"7.3","widget_ue_connectable":{},"input_ue_unconnectable":{}}},"widgets_values":[1024,64,64,8],"color":"#432","bgcolor":"#653"},{"id":42,"type":"CLIPTextEncode","pos":[-4064.922101718154,-775.4010825004351],"size":[308.0869090353041,88],"flags":{},"order":5,"mode":0,"inputs":[{"label":"clip","localized_name":"clip","name":"clip","type":"CLIP","link":359},{"label":"text","localized_name":"text","name":"text","type":"STRING","widget":{"name":"text"},"link":292}],"outputs":[{"label":"CONDITIONING","localized_name":"CONDITIONING","name":"CONDITIONING","type":"CONDITIONING","slot_index":0,"links":[308,338]}],"properties":{"cnr_id":"comfy-core","ver":"0.3.57","Node name for S&R":"CLIPTextEncode","ue_properties":{"version":"7.3","widget_ue_connectable":{"text":true},"input_ue_unconnectable":{}}},"widgets_values":[""],"color":"#232","bgcolor":"#353"},{"id":67,"type":"SamplerCustomAdvanced","pos":[-2300.9675642789875,-849.8598672232155],"size":[349.8709059121338,424.0136718016407],"flags":{},"order":6,"mode":0,"inputs":[{"label":"noise","localized_name":"noise","name":"noise","type":"NOISE","link":87},{"label":"guider","localized_name":"guider","name":"guider","type":"GUIDER","link":94},{"label":"sampler","localized_name":"sampler","name":"sampler","type":"SAMPLER","link":89},{"label":"sigmas","localized_name":"sigmas","name":"sigmas","type":"SIGMAS","link":90},{"label":"latent_image","localized_name":"latent_image","name":"latent_image","type":"LATENT","link":316}],"outputs":[{"label":"output","localized_name":"output","name":"output","type":"LATENT","slot_index":0,"links":[160]},{"label":"denoised_output","localized_name":"denoised_output","name":"denoised_output","type":"LATENT"}],"properties":{"cnr_id":"comfy-core","ver":"0.3.57","Node name for S&R":"SamplerCustomAdvanced","ue_properties":{"version":"7.3","widget_ue_connectable":{},"input_ue_unconnectable":{}}},"widgets_values":[],"color":"#2a363b","bgcolor":"#3f5159"}],"groups":[],"links":[{"id":288,"origin_id":96,"origin_slot":0,"target_id":164,"target_slot":0,"type":"IMAGE"},{"id":237,"origin_id":133,"origin_slot":0,"target_id":134,"target_slot":0,"type":"UPSCALE_MODEL"},{"id":302,"origin_id":73,"origin_slot":0,"target_id":72,"target_slot":1,"type":"CONDITIONING"},{"id":308,"origin_id":42,"origin_slot":0,"target_id":73,"target_slot":0,"type":"CONDITIONING"},{"id":363,"origin_id":34,"origin_slot":0,"target_id":36,"target_slot":0,"type":"IMAGE"},{"id":181,"origin_id":111,"origin_slot":0,"target_id":36,"target_slot":1,"type":"INT"},{"id":182,"origin_id":111,"origin_slot":1,"target_id":36,"target_slot":2,"type":"INT"},{"id":239,"origin_id":134,"origin_slot":0,"target_id":34,"target_slot":0,"type":"IMAGE"},{"id":364,"origin_id":34,"origin_slot":0,"target_id":111,"target_slot":0,"type":"IMAGE"},{"id":160,"origin_id":67,"origin_slot":0,"target_id":96,"target_slot":0,"type":"LATENT"},{"id":289,"origin_id":164,"origin_slot":0,"target_id":127,"target_slot":0,"type":"IMAGE"},{"id":218,"origin_id":36,"origin_slot":1,"target_id":127,"target_slot":1,"type":"LIST"},{"id":219,"origin_id":36,"origin_slot":2,"target_id":127,"target_slot":2,"type":"TUPLE"},{"id":220,"origin_id":36,"origin_slot":3,"target_id":127,"target_slot":3,"type":"TUPLE"},{"id":87,"origin_id":70,"origin_slot":0,"target_id":67,"target_slot":0,"type":"NOISE"},{"id":94,"origin_id":72,"origin_slot":0,"target_id":67,"target_slot":1,"type":"GUIDER"},{"id":89,"origin_id":68,"origin_slot":0,"target_id":67,"target_slot":2,"type":"SAMPLER"},{"id":90,"origin_id":69,"origin_slot":0,"target_id":67,"target_slot":3,"type":"SIGMAS"},{"id":316,"origin_id":129,"origin_slot":0,"target_id":67,"target_slot":4,"type":"LATENT"},{"id":343,"origin_id":-10,"origin_slot":0,"target_id":134,"target_slot":1,"type":"IMAGE"},{"id":357,"origin_id":-10,"origin_slot":1,"target_id":72,"target_slot":0,"type":"MODEL"},{"id":356,"origin_id":-10,"origin_slot":1,"target_id":69,"target_slot":0,"type":"MODEL"},{"id":307,"origin_id":-10,"origin_slot":2,"target_id":129,"target_slot":0,"type":"IMAGE"},{"id":353,"origin_id":-10,"origin_slot":3,"target_id":129,"target_slot":1,"type":"VAE"},{"id":352,"origin_id":-10,"origin_slot":3,"target_id":96,"target_slot":1,"type":"VAE"},{"id":359,"origin_id":-10,"origin_slot":4,"target_id":42,"target_slot":0,"type":"CLIP"},{"id":292,"origin_id":-10,"origin_slot":5,"target_id":42,"target_slot":1,"type":"STRING"},{"id":285,"origin_id":36,"origin_slot":0,"target_id":-20,"target_slot":0,"type":"IMAGE"},{"id":339,"origin_id":36,"origin_slot":1,"target_id":-20,"target_slot":1,"type":"LIST"},{"id":338,"origin_id":42,"origin_slot":0,"target_id":-20,"target_slot":2,"type":"CONDITIONING"},{"id":221,"origin_id":127,"origin_slot":0,"target_id":-20,"target_slot":3,"type":"IMAGE"},{"id":222,"origin_id":127,"origin_slot":0,"target_id":-20,"target_slot":3,"type":"IMAGE"},{"id":349,"origin_id":127,"origin_slot":0,"target_id":-20,"target_slot":3,"type":"IMAGE"}],"extra":{"ue_links":[],"links_added_by_ue":[],"workflowRendererVersion":"LG"}},{"id":"8cc071ed-8bd7-424d-a9eb-bce3fd785a3d","version":1,"state":{"lastGroupId":4,"lastNodeId":194,"lastLinkId":377,"lastRerouteId":0},"revision":0,"config":{},"name":"Vision Model","inputNode":{"id":-10,"bounding":[-2626.517275632431,-214.5635900217178,120,60]},"outputNode":{"id":-20,"bounding":[-1378.1795544122847,-224.5635900217178,120,80]},"inputs":[{"id":"373d2ad8-07d7-42d6-9d78-aa7caffa016f","name":"image","type":"IMAGE","linkIds":[372],"localized_name":"image","label":"image","pos":[-2526.517275632431,-194.5635900217178]}],"outputs":[{"id":"da79f01c-db0a-4ad7-9efe-3080ef842b06","name":"STRING","type":"STRING","linkIds":[370],"localized_name":"STRING","label":"text","shape":6,"pos":[-1358.1795544122847,-204.5635900217178]},{"id":"741d7e1e-64ef-491d-adf0-af37ea1bc33a","name":"IMAGE","type":"IMAGE","linkIds":[310,367],"localized_name":"IMAGE","label":"IMAGE","shape":6,"pos":[-1358.1795544122847,-184.5635900217178]}],"widgets":[],"nodes":[{"id":58,"type":"ShowText|pysssss","pos":[-2015.7126704650484,-614.5389897229508],"size":[577.5331160527637,889.9507994024659],"flags":{},"order":2,"mode":0,"inputs":[{"label":"text","localized_name":"text","name":"text","type":"STRING","link":271}],"outputs":[{"label":"text","localized_name":"STRING","name":"STRING","shape":6,"type":"STRING","slot_index":0,"links":[370]}],"properties":{"cnr_id":"comfyui-custom-scripts","ver":"aac13aa7ce35b07d43633c3bbe654a38c00d74f5","Node name for S&R":"ShowText|pysssss","ue_properties":{"version":"7.3","widget_ue_connectable":{},"input_ue_unconnectable":{}}},"widgets_values":["The image shows a woman in a black dress standing on a train platform, surrounded by pillars and a roof. The background is slightly blurred, giving the image a dreamy feel.","The image shows a woman standing in front of a train station, illuminated by lights hanging from the ceiling. She has a human face and is looking directly at the camera. The background is slightly blurred, giving the image a dreamy feel.","The image shows a woman in a black dress sitting on a bench in front of a train, with two poles in the background.","The image shows a woman in a black dress sitting on a bench at a train station, with a railway track in the background, a building, and a light pole.","The image shows a woman sitting on a bench with her legs crossed, wearing a black dress. The background is slightly blurred, giving the image a dreamy feel.","The image shows a woman sitting on a bench with her legs crossed, wearing a long black skirt. She is surrounded by a road in the background."],"color":"#222","bgcolor":"#000"},{"id":45,"type":"Florence2Run","pos":[-2446.517275632431,-102.32789427352432],"size":[400,364],"flags":{},"order":1,"mode":0,"inputs":[{"label":"image","localized_name":"image","name":"image","type":"IMAGE","link":286},{"label":"florence2_model","localized_name":"florence2_model","name":"florence2_model","type":"FL2MODEL","link":50},{"label":"text_input","localized_name":"text_input","name":"text_input","type":"STRING","widget":{"name":"text_input"}},{"label":"task","localized_name":"task","name":"task","type":"COMBO","widget":{"name":"task"}},{"label":"fill_mask","localized_name":"fill_mask","name":"fill_mask","type":"BOOLEAN","widget":{"name":"fill_mask"}},{"label":"keep_model_loaded","localized_name":"keep_model_loaded","name":"keep_model_loaded","shape":7,"type":"BOOLEAN","widget":{"name":"keep_model_loaded"}},{"label":"max_new_tokens","localized_name":"max_new_tokens","name":"max_new_tokens","shape":7,"type":"INT","widget":{"name":"max_new_tokens"}},{"label":"num_beams","localized_name":"num_beams","name":"num_beams","shape":7,"type":"INT","widget":{"name":"num_beams"}},{"label":"do_sample","localized_name":"do_sample","name":"do_sample","shape":7,"type":"BOOLEAN","widget":{"name":"do_sample"}},{"label":"output_mask_select","localized_name":"output_mask_select","name":"output_mask_select","shape":7,"type":"STRING","widget":{"name":"output_mask_select"}},{"label":"seed","localized_name":"seed","name":"seed","shape":7,"type":"INT","widget":{"name":"seed"}}],"outputs":[{"label":"image","localized_name":"image","name":"image","type":"IMAGE"},{"label":"mask","localized_name":"mask","name":"mask","type":"MASK"},{"label":"caption","localized_name":"caption","name":"caption","type":"STRING","slot_index":2,"links":[271]},{"label":"data","localized_name":"data","name":"data","type":"JSON"}],"properties":{"cnr_id":"comfyui-florence2","ver":"1.0.5","Node name for S&R":"Florence2Run","ue_properties":{"version":"7.3","widget_ue_connectable":{},"input_ue_unconnectable":{}}},"widgets_values":["","detailed_caption",true,false,1024,3,true,"",1118877715456454,"fixed"],"color":"#222","bgcolor":"#000"},{"id":46,"type":"DownloadAndLoadFlorence2Model","pos":[-2444.6984553725574,-296.9838878132651],"size":[381.9399719238281,130],"flags":{},"order":0,"mode":0,"inputs":[{"label":"lora","localized_name":"lora","name":"lora","shape":7,"type":"PEFTLORA"},{"label":"model","localized_name":"model","name":"model","type":"COMBO","widget":{"name":"model"}},{"label":"precision","localized_name":"precision","name":"precision","type":"COMBO","widget":{"name":"precision"}},{"label":"attention","localized_name":"attention","name":"attention","type":"COMBO","widget":{"name":"attention"}},{"label":"convert_to_safetensors","localized_name":"convert_to_safetensors","name":"convert_to_safetensors","shape":7,"type":"BOOLEAN","widget":{"name":"convert_to_safetensors"}}],"outputs":[{"label":"florence2_model","localized_name":"florence2_model","name":"florence2_model","type":"FL2MODEL","slot_index":0,"links":[50]}],"properties":{"cnr_id":"comfyui-florence2","ver":"1.0.5","Node name for S&R":"DownloadAndLoadFlorence2Model","ue_properties":{"version":"7.3","widget_ue_connectable":{},"input_ue_unconnectable":{}}},"widgets_values":["microsoft/Florence-2-large","bf16","sdpa",false],"color":"#222","bgcolor":"#000"},{"id":163,"type":"easy imageBatchToImageList","pos":[-2387.908106179011,-612.8623556950982],"size":[210,26],"flags":{},"order":3,"mode":0,"inputs":[{"label":"image","localized_name":"image","name":"image","type":"IMAGE","link":372}],"outputs":[{"label":"IMAGE","localized_name":"IMAGE","name":"IMAGE","shape":6,"type":"IMAGE","slot_index":0,"links":[286,310,367]}],"properties":{"cnr_id":"comfyui-easy-use","ver":"e7320ec0c463641f702add48b87363c7a9713a1c","Node name for S&R":"easy imageBatchToImageList","ue_properties":{"version":"7.3","widget_ue_connectable":{},"input_ue_unconnectable":{}}},"widgets_values":[],"color":"#222","bgcolor":"#000"}],"groups":[],"links":[{"id":271,"origin_id":45,"origin_slot":2,"target_id":58,"target_slot":0,"type":"STRING"},{"id":286,"origin_id":163,"origin_slot":0,"target_id":45,"target_slot":0,"type":"IMAGE"},{"id":50,"origin_id":46,"origin_slot":0,"target_id":45,"target_slot":1,"type":"FL2MODEL"},{"id":372,"origin_id":-10,"origin_slot":0,"target_id":163,"target_slot":0,"type":"IMAGE"},{"id":370,"origin_id":58,"origin_slot":0,"target_id":-20,"target_slot":0,"type":"STRING"},{"id":310,"origin_id":163,"origin_slot":0,"target_id":-20,"target_slot":1,"type":"IMAGE"},{"id":367,"origin_id":163,"origin_slot":0,"target_id":-20,"target_slot":1,"type":"IMAGE"}],"extra":{"ue_links":[],"links_added_by_ue":[],"workflowRendererVersion":"LG"}},{"id":"b793d2c3-2854-43fa-8879-9fca46a28d8d","version":1,"state":{"lastGroupId":4,"lastNodeId":199,"lastLinkId":383,"lastRerouteId":0},"revision":0,"config":{},"name":"Texture Detailer","inputNode":{"id":-10,"bounding":[-4132.330402562125,-2373.94112746019,120.73046875,180]},"outputNode":{"id":-20,"bounding":[-2856.0234574278084,-2265.2371997228943,120,60]},"inputs":[{"id":"2bc8053d-144f-4d48-bf61-97db561898c9","name":"model","type":"MODEL","linkIds":[358],"localized_name":"model","label":"model","pos":[-4031.5999338121246,-2353.94112746019]},{"id":"e2fee7ff-89ad-4666-b5a4-995d9c15836b","name":"Positions","type":"LIST","linkIds":[373],"localized_name":"Positions","label":"Positions","pos":[-4031.5999338121246,-2333.94112746019]},{"id":"d948ebe3-696c-43ec-bb8d-15c6841aaa65","name":"image","type":"IMAGE","linkIds":[377],"localized_name":"image","label":"image","pos":[-4031.5999338121246,-2313.94112746019]},{"id":"fb4157c9-a76a-479e-8aa1-4cb1ad070592","name":"conditionings","type":"CONDITIONING","linkIds":[374],"localized_name":"conditionings","label":"conditionings","pos":[-4031.5999338121246,-2293.94112746019]},{"id":"c47c75fb-ab77-4f65-bbcc-34165f405c7f","name":"conditioning_to","type":"CONDITIONING","linkIds":[362,361],"localized_name":"conditioning_to","label":"conditioning_to","pos":[-4031.5999338121246,-2273.94112746019]},{"id":"7cf15641-e2ff-46ed-9519-d2c6165bc5be","name":"vae","type":"VAE","linkIds":[355,354],"localized_name":"vae","label":"vae","pos":[-4031.5999338121246,-2253.94112746019]},{"id":"fdd05479-4486-4980-b82f-30c8fce180ed","name":"noise_strength","type":"FLOAT","linkIds":[383],"pos":[-4031.5999338121246,-2233.94112746019]}],"outputs":[{"id":"6aeff311-4c0d-4e09-b036-df2808d29340","name":"IMAGE","type":"IMAGE","linkIds":[324],"localized_name":"IMAGE","label":"IMAGE","pos":[-2836.0234574278084,-2245.2371997228943]}],"widgets":[],"nodes":[{"id":184,"type":"TTP_CoordinateSplitter","pos":[-3774.372531400457,-2723.9418907727672],"size":[267,26],"flags":{},"order":6,"mode":0,"inputs":[{"label":"Positions","localized_name":"Positions","name":"Positions","type":"LIST","link":373}],"outputs":[{"label":"Coordinates","localized_name":"Coordinates","name":"Coordinates","type":"LIST","slot_index":0,"links":[337]}],"properties":{"cnr_id":"comfyui_ttp_toolset","ver":"1.0.5","Node name for S&R":"TTP_CoordinateSplitter","ue_properties":{"version":"7.3","widget_ue_connectable":{},"input_ue_unconnectable":{}}},"widgets_values":[],"color":"#233","bgcolor":"#355"},{"id":186,"type":"TTP_condsetarea_merge","pos":[-3457.346583464594,-2778.2657902781734],"size":[262,78],"flags":{},"order":8,"mode":0,"inputs":[{"label":"conditioning_batch","localized_name":"conditioning_batch","name":"conditioning_batch","type":"CONDITIONING","link":336},{"label":"coordinates","localized_name":"coordinates","name":"coordinates","type":"LIST","link":337},{"label":"strength","localized_name":"strength","name":"strength","type":"FLOAT","widget":{"name":"strength"}}],"outputs":[{"label":"CONDITIONING","localized_name":"CONDITIONING","name":"CONDITIONING","type":"CONDITIONING","slot_index":0,"links":[340]}],"properties":{"cnr_id":"comfyui_ttp_toolset","ver":"1.0.5","Node name for S&R":"TTP_condsetarea_merge","ue_properties":{"version":"7.3","widget_ue_connectable":{},"input_ue_unconnectable":{}}},"widgets_values":[1],"color":"#233","bgcolor":"#355"},{"id":177,"type":"VAEDecodeTiled","pos":[-3135.916129513006,-2302.284292419524],"size":[219.89267208519777,150],"flags":{},"order":1,"mode":0,"inputs":[{"label":"samples","localized_name":"samples","name":"samples","type":"LATENT","link":323},{"label":"vae","localized_name":"vae","name":"vae","type":"VAE","link":354},{"label":"tile_size","localized_name":"tile_size","name":"tile_size","type":"INT","widget":{"name":"tile_size"}},{"label":"overlap","localized_name":"overlap","name":"overlap","type":"INT","widget":{"name":"overlap"}},{"label":"temporal_size","localized_name":"temporal_size","name":"temporal_size","type":"INT","widget":{"name":"temporal_size"}},{"label":"temporal_overlap","localized_name":"temporal_overlap","name":"temporal_overlap","type":"INT","widget":{"name":"temporal_overlap"}}],"outputs":[{"label":"IMAGE","localized_name":"IMAGE","name":"IMAGE","type":"IMAGE","slot_index":0,"links":[324]}],"properties":{"cnr_id":"comfy-core","ver":"0.3.57","Node name for S&R":"VAEDecodeTiled","ue_properties":{"version":"7.3","widget_ue_connectable":{},"input_ue_unconnectable":{}}},"widgets_values":[1024,64,64,8],"color":"#432","bgcolor":"#653"},{"id":176,"type":"KSamplerAdvanced //Inspire","pos":[-3483.804838926235,-2300.3464589410587],"size":[300.87152099609375,730.0000000000001],"flags":{},"order":0,"mode":0,"inputs":[{"label":"model","localized_name":"model","name":"model","type":"MODEL","link":331},{"label":"positive","localized_name":"positive","name":"positive","type":"CONDITIONING","link":335},{"label":"negative","localized_name":"negative","name":"negative","type":"CONDITIONING","link":361},{"label":"latent_image","localized_name":"latent_image","name":"latent_image","type":"LATENT","link":326},{"label":"noise_opt","localized_name":"noise_opt","name":"noise_opt","shape":7,"type":"NOISE_IMAGE"},{"label":"scheduler_func_opt","localized_name":"scheduler_func_opt","name":"scheduler_func_opt","shape":7,"type":"SCHEDULER_FUNC"},{"label":"add_noise","localized_name":"add_noise","name":"add_noise","type":"BOOLEAN","widget":{"name":"add_noise"}},{"label":"noise_seed","localized_name":"noise_seed","name":"noise_seed","type":"INT","widget":{"name":"noise_seed"}},{"label":"steps","localized_name":"steps","name":"steps","type":"INT","widget":{"name":"steps"}},{"label":"cfg","localized_name":"cfg","name":"cfg","type":"FLOAT","widget":{"name":"cfg"}},{"label":"sampler_name","localized_name":"sampler_name","name":"sampler_name","type":"COMBO","widget":{"name":"sampler_name"}},{"label":"scheduler","localized_name":"scheduler","name":"scheduler","type":"COMBO","widget":{"name":"scheduler"}},{"label":"start_at_step","localized_name":"start_at_step","name":"start_at_step","type":"INT","widget":{"name":"start_at_step"}},{"label":"end_at_step","localized_name":"end_at_step","name":"end_at_step","type":"INT","widget":{"name":"end_at_step"}},{"label":"noise_mode","localized_name":"noise_mode","name":"noise_mode","type":"COMBO","widget":{"name":"noise_mode"}},{"label":"return_with_leftover_noise","localized_name":"return_with_leftover_noise","name":"return_with_leftover_noise","type":"BOOLEAN","widget":{"name":"return_with_leftover_noise"}},{"label":"batch_seed_mode","localized_name":"batch_seed_mode","name":"batch_seed_mode","type":"COMBO","widget":{"name":"batch_seed_mode"}},{"label":"variation_seed","localized_name":"variation_seed","name":"variation_seed","type":"INT","widget":{"name":"variation_seed"}},{"label":"variation_strength","localized_name":"variation_strength","name":"variation_strength","type":"FLOAT","widget":{"name":"variation_strength"}},{"label":"variation_method","localized_name":"variation_method","name":"variation_method","shape":7,"type":"COMBO","widget":{"name":"variation_method"}},{"localized_name":"internal_seed","name":"internal_seed","shape":7,"type":"INT","widget":{"name":"internal_seed"},"link":null}],"outputs":[{"label":"LATENT","localized_name":"LATENT","name":"LATENT","type":"LATENT","slot_index":0,"links":[323]}],"properties":{"cnr_id":"comfyui-inspire-pack","ver":"1.20.0","Node name for S&R":"KSamplerAdvanced //Inspire","ue_properties":{"version":"7.3","widget_ue_connectable":{},"input_ue_unconnectable":{}}},"widgets_values":[true,833195543994351,"fixed",20,1,"euler","simple",18,10000,"GPU(=A1111)",false,"incremental",0,0,"linear",0],"color":"#2a363b","bgcolor":"#3f5159"},{"id":179,"type":"InjectLatentNoise+","pos":[-3151.500041523608,-2690.4565089417138],"size":[265.5225524902344,150],"flags":{},"order":2,"mode":0,"inputs":[{"label":"latent","localized_name":"latent","name":"latent","type":"LATENT","link":328},{"label":"noise_seed","localized_name":"noise_seed","name":"noise_seed","type":"INT","widget":{"name":"noise_seed"}},{"label":"noise_strength","localized_name":"noise_strength","name":"noise_strength","type":"FLOAT","widget":{"name":"noise_strength"},"link":383},{"label":"mask","localized_name":"mask","name":"mask","shape":7,"type":"MASK"}],"outputs":[{"label":"LATENT","localized_name":"LATENT","name":"LATENT","type":"LATENT","slot_index":0,"links":[326]}],"properties":{"cnr_id":"comfyui_essentials","ver":"1.1.0","Node name for S&R":"InjectLatentNoise+","ue_properties":{"version":"7.3","widget_ue_connectable":{},"input_ue_unconnectable":{}}},"widgets_values":[0,"fixed",0.05],"color":"#233","bgcolor":"#355"},{"id":192,"type":"ImageSmartSharpen+","pos":[-3774.372531400457,-2644.1752885295446],"size":[265.993164099179,130],"flags":{"collapsed":false},"order":9,"mode":0,"inputs":[{"label":"image","localized_name":"image","name":"image","type":"IMAGE","link":377},{"localized_name":"noise_radius","name":"noise_radius","type":"INT","widget":{"name":"noise_radius"},"link":null},{"localized_name":"preserve_edges","name":"preserve_edges","type":"FLOAT","widget":{"name":"preserve_edges"},"link":null},{"localized_name":"sharpen","name":"sharpen","type":"FLOAT","widget":{"name":"sharpen"},"link":null},{"localized_name":"ratio","name":"ratio","type":"FLOAT","widget":{"name":"ratio"},"link":null}],"outputs":[{"label":"IMAGE","localized_name":"IMAGE","name":"IMAGE","type":"IMAGE","slot_index":0,"links":[350]}],"properties":{"cnr_id":"comfyui_essentials","ver":"1.1.0","Node name for S&R":"ImageSmartSharpen+","ue_properties":{"version":"7.3","widget_ue_connectable":{},"input_ue_unconnectable":{}}},"widgets_values":[4,0.75,3,0.5],"color":"#233","bgcolor":"#355"},{"id":185,"type":"TTP_condtobatch","pos":[-3774.372531400457,-2798.900520126817],"size":[267.92712261080715,28.03377342224121],"flags":{},"order":7,"mode":0,"inputs":[{"label":"conditionings","localized_name":"conditionings","name":"conditionings","type":"CONDITIONING","link":374}],"outputs":[{"label":"CONDITIONING","localized_name":"CONDITIONING","name":"CONDITIONING","type":"CONDITIONING","slot_index":0,"links":[336]}],"properties":{"cnr_id":"comfyui_ttp_toolset","ver":"1.0.5","Node name for S&R":"TTP_condtobatch","ue_properties":{"version":"7.3","widget_ue_connectable":{},"input_ue_unconnectable":{}}},"widgets_values":[],"color":"#233","bgcolor":"#355"},{"id":183,"type":"ConditioningConcat","pos":[-3150.736353741304,-2777.05650543514],"size":[260.00852331039505,48.261848212461246],"flags":{},"order":5,"mode":0,"inputs":[{"label":"conditioning_to","localized_name":"conditioning_to","name":"conditioning_to","type":"CONDITIONING","link":362},{"label":"conditioning_from","localized_name":"conditioning_from","name":"conditioning_from","type":"CONDITIONING","link":340}],"outputs":[{"label":"CONDITIONING","localized_name":"CONDITIONING","name":"CONDITIONING","type":"CONDITIONING","slot_index":0,"links":[335]}],"properties":{"cnr_id":"comfy-core","ver":"0.3.57","Node name for S&R":"ConditioningConcat","ue_properties":{"version":"7.3","widget_ue_connectable":{},"input_ue_unconnectable":{}}},"widgets_values":[],"color":"#233","bgcolor":"#355"},{"id":180,"type":"VAEEncode","pos":[-3367.0220699033894,-2655.308153977776],"size":[169.4751164666293,46],"flags":{},"order":3,"mode":0,"inputs":[{"label":"pixels","localized_name":"pixels","name":"pixels","type":"IMAGE","link":350},{"label":"vae","localized_name":"vae","name":"vae","type":"VAE","link":355}],"outputs":[{"label":"LATENT","localized_name":"LATENT","name":"LATENT","type":"LATENT","slot_index":0,"links":[328]}],"properties":{"cnr_id":"comfy-core","ver":"0.3.57","Node name for S&R":"VAEEncode","ue_properties":{"version":"7.3","widget_ue_connectable":{},"input_ue_unconnectable":{}}},"widgets_values":[],"color":"#432","bgcolor":"#653"},{"id":181,"type":"TiledDiffusion","pos":[-3787.925339218075,-2299.48350467604],"size":[268.2269287109375,154],"flags":{},"order":4,"mode":0,"inputs":[{"label":"model","localized_name":"model","name":"model","type":"MODEL","link":358},{"label":"method","localized_name":"method","name":"method","type":"COMBO","widget":{"name":"method"}},{"label":"tile_width","localized_name":"tile_width","name":"tile_width","type":"INT","widget":{"name":"tile_width"}},{"label":"tile_height","localized_name":"tile_height","name":"tile_height","type":"INT","widget":{"name":"tile_height"}},{"label":"tile_overlap","localized_name":"tile_overlap","name":"tile_overlap","type":"INT","widget":{"name":"tile_overlap"}},{"label":"tile_batch_size","localized_name":"tile_batch_size","name":"tile_batch_size","type":"INT","widget":{"name":"tile_batch_size"}}],"outputs":[{"label":"MODEL","localized_name":"MODEL","name":"MODEL","type":"MODEL","slot_index":0,"links":[331]}],"properties":{"cnr_id":"ComfyUI-TiledDiffusion","ver":"a155b1bac39147381aeaa52b9be42e545626a44f","Node name for S&R":"TiledDiffusion","ue_properties":{"version":"7.3","widget_ue_connectable":{},"input_ue_unconnectable":{}}},"widgets_values":["Mixture of Diffusers",1024,1024,96,4],"color":"#223","bgcolor":"#335"}],"groups":[],"links":[{"id":340,"origin_id":186,"origin_slot":0,"target_id":183,"target_slot":1,"type":"CONDITIONING"},{"id":336,"origin_id":185,"origin_slot":0,"target_id":186,"target_slot":0,"type":"CONDITIONING"},{"id":337,"origin_id":184,"origin_slot":0,"target_id":186,"target_slot":1,"type":"LIST"},{"id":328,"origin_id":180,"origin_slot":0,"target_id":179,"target_slot":0,"type":"LATENT"},{"id":350,"origin_id":192,"origin_slot":0,"target_id":180,"target_slot":0,"type":"IMAGE"},{"id":323,"origin_id":176,"origin_slot":0,"target_id":177,"target_slot":0,"type":"LATENT"},{"id":331,"origin_id":181,"origin_slot":0,"target_id":176,"target_slot":0,"type":"MODEL"},{"id":335,"origin_id":183,"origin_slot":0,"target_id":176,"target_slot":1,"type":"CONDITIONING"},{"id":326,"origin_id":179,"origin_slot":0,"target_id":176,"target_slot":3,"type":"LATENT"},{"id":358,"origin_id":-10,"origin_slot":0,"target_id":181,"target_slot":0,"type":"MODEL"},{"id":373,"origin_id":-10,"origin_slot":1,"target_id":184,"target_slot":0,"type":"LIST"},{"id":377,"origin_id":-10,"origin_slot":2,"target_id":192,"target_slot":0,"type":"IMAGE"},{"id":374,"origin_id":-10,"origin_slot":3,"target_id":185,"target_slot":0,"type":"CONDITIONING"},{"id":362,"origin_id":-10,"origin_slot":4,"target_id":183,"target_slot":0,"type":"CONDITIONING"},{"id":361,"origin_id":-10,"origin_slot":4,"target_id":176,"target_slot":2,"type":"CONDITIONING"},{"id":355,"origin_id":-10,"origin_slot":5,"target_id":180,"target_slot":1,"type":"VAE"},{"id":354,"origin_id":-10,"origin_slot":5,"target_id":177,"target_slot":1,"type":"VAE"},{"id":324,"origin_id":177,"origin_slot":0,"target_id":-20,"target_slot":0,"type":"IMAGE"},{"id":383,"origin_id":-10,"origin_slot":6,"target_id":179,"target_slot":2,"type":"FLOAT"}],"extra":{"ue_links":[],"links_added_by_ue":[],"workflowRendererVersion":"LG"}},{"id":"a0f2ad07-f68d-4213-a9f5-b5d97db9e0cd","version":1,"state":{"lastGroupId":0,"lastNodeId":202,"lastLinkId":396,"lastRerouteId":0},"revision":0,"config":{},"name":"UpScale-TTP","inputNode":{"id":-10,"bounding":[-2596.6241179642593,-1278.1598066534561,120.73046875,140]},"outputNode":{"id":-20,"bounding":[-2032.8981169178878,-1238.1598066534561,120,60]},"inputs":[{"id":"68a1e483-cb7a-44d5-a85c-2551782f414d","name":"image","type":"IMAGE","linkIds":[392],"localized_name":"image","label":"image","pos":[-2495.8936492142593,-1258.1598066534561]},{"id":"f1aa6e03-ac90-4456-bcc5-32ac64444dc7","name":"model","type":"MODEL","linkIds":[366,383],"localized_name":"model","label":"model","pos":[-2495.8936492142593,-1238.1598066534561]},{"id":"a11f0c15-f117-41e9-8346-512c2800477d","name":"vae","type":"VAE","linkIds":[368,388],"localized_name":"vae","label":"vae","pos":[-2495.8936492142593,-1218.1598066534561]},{"id":"b2031f81-4e0f-4866-af3a-172dc46458c6","name":"clip","type":"CLIP","linkIds":[369],"localized_name":"clip","label":"clip","pos":[-2495.8936492142593,-1198.1598066534561]},{"id":"77b9e8e7-0678-4864-927e-88b17e2bd782","name":"conditioning_to","type":"CONDITIONING","linkIds":[387],"localized_name":"conditioning_to","label":"conditioning_to","pos":[-2495.8936492142593,-1178.1598066534561]}],"outputs":[{"id":"c4b83c1e-9b0d-4a8f-9854-1f0a3079c5eb","name":"IMAGE","type":"IMAGE","linkIds":[390,391],"localized_name":"IMAGE","label":"IMAGE","pos":[-2012.8981169178878,-1218.1598066534561]}],"widgets":[],"nodes":[{"id":195,"type":"8cc071ed-8bd7-424d-a9eb-bce3fd785a3d","pos":[-2408.825152586731,-1756.8564518530802],"size":[315.927035668843,78],"flags":{},"order":1,"mode":0,"inputs":[{"label":"image","localized_name":"image","name":"image","type":"IMAGE","link":378}],"outputs":[{"label":"text","localized_name":"STRING","name":"STRING","type":"STRING","links":[379]},{"label":"IMAGE","localized_name":"IMAGE","name":"IMAGE","type":"IMAGE","links":[381]}],"properties":{"proxyWidgets":[["45","seed"]],"cnr_id":"comfy-core","ver":"0.3.66","ue_properties":{"widget_ue_connectable":{},"input_ue_unconnectable":{},"version":"7.3"}},"widgets_values":[],"color":"#222","bgcolor":"#000"},{"id":194,"type":"da172348-b4fa-49cf-94b6-10765e6232c2","pos":[-2413.3898347689105,-1629.513706114289],"size":[318.0924255732325,460.60791774835116],"flags":{},"order":0,"mode":0,"inputs":[{"label":"image","localized_name":"image","name":"image","type":"IMAGE","link":392},{"label":"model","localized_name":"model","name":"model","type":"MODEL","link":366},{"label":"pixels","localized_name":"pixels","name":"pixels","type":"IMAGE","link":381},{"label":"vae","localized_name":"vae","name":"vae","type":"VAE","link":368},{"label":"clip","localized_name":"clip","name":"clip","type":"CLIP","link":369},{"label":"text","localized_name":"text","name":"text","type":"STRING","widget":{"name":"text"},"link":379}],"outputs":[{"label":"IMAGES","localized_name":"IMAGES","name":"IMAGES","type":"IMAGE","links":[378]},{"label":"POSITIONS","localized_name":"POSITIONS","name":"POSITIONS","type":"LIST","links":[384]},{"label":"CONDITIONING","localized_name":"CONDITIONING","name":"CONDITIONING","type":"CONDITIONING","links":[386]},{"label":"RECONSTRUCTED_IMAGE","localized_name":"RECONSTRUCTED_IMAGE","name":"RECONSTRUCTED_IMAGE","type":"IMAGE","links":[385]}],"properties":{"proxyWidgets":[["-1","text"],["-1","$$canvas-image-preview"]],"cnr_id":"comfy-core","ver":"0.3.66","ue_properties":{"widget_ue_connectable":{"text":true},"input_ue_unconnectable":{},"version":"7.3"}},"widgets_values":[""],"color":"#223","bgcolor":"#335"},{"id":200,"type":"b793d2c3-2854-43fa-8879-9fca46a28d8d","pos":[-2415.8936492142593,-1115.644531972724],"size":[321.68717262231803,486.1813705188922],"flags":{},"order":2,"mode":0,"inputs":[{"label":"model","localized_name":"model","name":"model","type":"MODEL","link":383},{"label":"Positions","localized_name":"Positions","name":"Positions","type":"LIST","link":384},{"label":"image","localized_name":"image","name":"image","type":"IMAGE","link":385},{"label":"conditionings","localized_name":"conditionings","name":"conditionings","type":"CONDITIONING","link":386},{"label":"conditioning_to","localized_name":"conditioning_to","name":"conditioning_to","type":"CONDITIONING","link":387},{"label":"vae","localized_name":"vae","name":"vae","type":"VAE","link":388},{"name":"noise_strength","type":"FLOAT","widget":{"name":"noise_strength"},"link":null}],"outputs":[{"label":"IMAGE","localized_name":"IMAGE","name":"IMAGE","type":"IMAGE","links":[390,391]}],"title":"Texture Detailer","properties":{"proxyWidgets":[["-1","noise_strength"],["-1","$$canvas-image-preview"]],"cnr_id":"comfy-core","ver":"0.3.66","ue_properties":{"widget_ue_connectable":{},"input_ue_unconnectable":{},"version":"7.3"}},"widgets_values":[0.05],"color":"#233","bgcolor":"#355"}],"groups":[],"links":[{"id":378,"origin_id":194,"origin_slot":0,"target_id":195,"target_slot":0,"type":"IMAGE"},{"id":381,"origin_id":195,"origin_slot":1,"target_id":194,"target_slot":2,"type":"IMAGE"},{"id":379,"origin_id":195,"origin_slot":0,"target_id":194,"target_slot":5,"type":"STRING"},{"id":384,"origin_id":194,"origin_slot":1,"target_id":200,"target_slot":1,"type":"LIST"},{"id":385,"origin_id":194,"origin_slot":3,"target_id":200,"target_slot":2,"type":"IMAGE"},{"id":386,"origin_id":194,"origin_slot":2,"target_id":200,"target_slot":3,"type":"CONDITIONING"},{"id":392,"origin_id":-10,"origin_slot":0,"target_id":194,"target_slot":0,"type":"IMAGE"},{"id":366,"origin_id":-10,"origin_slot":1,"target_id":194,"target_slot":1,"type":"MODEL"},{"id":383,"origin_id":-10,"origin_slot":1,"target_id":200,"target_slot":0,"type":"MODEL"},{"id":368,"origin_id":-10,"origin_slot":2,"target_id":194,"target_slot":3,"type":"VAE"},{"id":388,"origin_id":-10,"origin_slot":2,"target_id":200,"target_slot":5,"type":"VAE"},{"id":369,"origin_id":-10,"origin_slot":3,"target_id":194,"target_slot":4,"type":"CLIP"},{"id":387,"origin_id":-10,"origin_slot":4,"target_id":200,"target_slot":4,"type":"CONDITIONING"},{"id":390,"origin_id":200,"origin_slot":0,"target_id":-20,"target_slot":0,"type":"IMAGE"},{"id":391,"origin_id":200,"origin_slot":0,"target_id":-20,"target_slot":0,"type":"*"}],"extra":{"workflowRendererVersion":"LG","ue_links":[],"links_added_by_ue":[]}}]},"config":{},"extra":{"links_added_by_ue":[],"ue_links":[],"0246.VERSION":[0,0,4],"frontendVersion":"1.33.13","groupNodes":{},"VHS_latentpreview":false,"VHS_latentpreviewrate":0,"VHS_MetadataImage":true,"VHS_KeepIntermediate":true,"workflowRendererVersion":"LG"},"version":0.4}

**tool:**

All the files uploaded by the user have been fully loaded. Searching won't provide additional information.

**Emily:**

Got it — and thank you for the workflow JSON. I can see **two separate “detail amplifiers”** in your graph, and *either one* can make textures feel too crunchy. Here’s what your workflow is doing, and exactly which nodes are the best knobs to turn. fileciteturn0file0

---

## What your “SRPO → TTP → Texture Detailer” workflow actually is
In your file, the pipeline is basically:

1) **Base Image** (your SRPO + LoRA + mask inpaint stage)  
   - This is the part where you’re generating/repairing the *base* image.

2) **UpScale Process** (pre-upscale + tile refinement + reassemble)  
   - You upscale using a model (**4x_NMKD-Siax_200k.pth**) then resize to **8 megapixels**, then run a **tile-level diffusion refine** (TTP tiling + Flux sampler per tile), then reassemble.

3) **Texture Detailer** (a *second* pass of tile diffusion + sharpening + latent noise)  
   - This is where a lot of “too aggressive texture” commonly comes from, because it explicitly adds sharpening and a tiny bit of latent noise, and then does tiled diffusion again.

So: even if you “dial back TTP,” you might still get crunchy texture from the **Texture Detailer** section unless you tame that too.

---

## Where the over-texturing is most likely coming from (in *your* nodes)
### A) The *Texture Detailer* block is a prime suspect
In **Texture Detailer**:

- **ImageSmartSharpen+ (node 192)** has **sharpen = 3** (pretty strong), preserve_edges = 0.75, ratio = 0.5.  
  This alone can push fabric/skin into “crisped micro-contrast” territory.

- **InjectLatentNoise+ (node 179)** has **noise_strength = 0.05**.  
  That’s small, but with tiling it can turn into “grain that the model interprets as texture,” especially on cloth and skin.

- **TiledDiffusion (node 181)** is enabled with tile_overlap = 96, tile size 1024×1024.  
  This makes the model operate locally, which is great for faces… and also great at inventing microtexture if the input is already sharpened/noisy.

### B) The *UpScale Process* tile refine also contributes
Inside **UpScale Process** (your TTP refine loop):

- **BasicScheduler (node 69)** uses **steps = 10** and **denoise = 0.3** for the tile refine.  
  Denoise 0.3 is a “real rewrite,” not a gentle polish.

- **FluxGuidance (node 73)** is **2.5** (not crazy), but combined with denoise 0.3 it can still over-commit to “detail.”

- You’re also resizing to **8 megapixels** before tiling (ImageScaleToTotalPixels node 34).  
  Higher MP = the model has more “room” to hallucinate surface detail.

---

## The fastest way to dial it back (do these in order)
### Step 1 — neuter the Texture Detailer *without breaking the workflow*
This is the quickest A/B test because it doesn’t touch your core upscale.

1) **Drop Sharpen**  
   - Node 192 (ImageSmartSharpen+)  
   - Change **sharpen: 3 → 1** (or even **0.6**).  
   - If you want to keep edge clarity, keep preserve_edges as-is.

2) **Reduce or zero the latent noise**  
   - Node 179 (InjectLatentNoise+)  
   - Change **noise_strength: 0.05 → 0.02**  
   - If you want the cleanest result: **0.00** (pure test).

Run that once. If the “aggressive texturing” largely disappears: you’ve found your culprit.

### Step 2 — soften the TTP refine pass (UpScale Process)
If you still want it calmer:

- Node 69 (BasicScheduler in UpScale Process tile refine):  
  - **denoise: 0.30 → 0.22** (first try)  
  - If still crunchy: **0.18**

Keep steps at 10 for now. Denoise is the real lever.

### Step 3 — reduce the megapixels before tiling (optional but powerful)
- Node 34 (ImageScaleToTotalPixels): **8 MP → 6 MP**  
This often reduces “microtexture enthusiasm” while still giving you a nice upscale.

---

## My “sane presets” for your exact graph
### Preset A — “cleaner cloth + cleaner skin”
- Node 192 sharpen: **1.0**
- Node 179 noise_strength: **0.02**
- Node 69 denoise: **0.22**
- Keep guidance 2.5

### Preset B — “polite and painterly”
- Node 192 sharpen: **0.6**
- Node 179 noise_strength: **0.00**
- Node 69 denoise: **0.18**
- (Optional) Node 34 megapixels: **6**

### Preset C — “faces still improve, textures don’t crunch”
- Node 192 sharpen: **1.0**
- Node 179 noise_strength: **0.01**
- Node 69 denoise: **0.20**
- Node 73 guidance: **2.2** (slight reduction)

---

## One sneaky thing I’d check
In **Texture Detailer**, your **KSamplerAdvanced //Inspire (node 176)** is set with **steps=20**, and **start_at_step=18**. That means you’re only running the *very tail end* of the schedule — which can act like a “final detail punch.” If you keep this block, try:

- **start_at_step: 18 → 16** (slightly broader, often less “ringy”)
- or just lower the impact by reducing the sharpening/noise first (my preference).

---

## If you tell me what “too aggressive” looks like, I can target it precisely
Different “aggressive” signatures point to different knobs:

- **gritty skin / pores everywhere** → reduce denoise + latent noise first  
- **crispy fabric weave / crunchy edges** → reduce sharpening first  
- **overall “too sharp” look** → megapixels down + sharpen down  

If you can paste a **before/after crop** (even just a face + bodice area), I’ll tell you which two changes will get the biggest win with the least disruption.

And if you’d like, I can also mark up your JSON with a tiny “TTP mellow” variant (just the key values changed) so you can load it as a second workflow.

**Robyn:**


<img src="/assets/images/chat/6c33c60.jpg" style="float: right; width: 420px; margin-left: 20px" />

I might send you the image with just that one change in ImageSmartSharpen (Node#192) = I changed sharpen from 4 to 1. What do you think? I feel I have the detail, even if it is a bit softer focus. However, the hair also seems softer, less crisp. Ideas?

**Emily:**

Yep — **that change looks like a win**.

With **Sharpen 4 → 1** you’ve basically removed the “crispy micro-contrast” that reads as synthetic skin/fabric texture. The face now looks *clean and photographic*, and the dress keeps its folds without turning into crunchy weave. The trade-off you’re noticing (hair slightly less “etched”) is the *normal* side effect of backing off global sharpening.

Here are a few ways to get **hair/eyes crisp again** without reintroducing the over-textured look.

## Option 1: Keep Sharpen=1 globally, then sharpen *selectively* (best solution)
Do a second, very gentle sharpen pass, **masked to hair + eyes** (or “head/hair region”), leaving cheeks/forehead alone.

**How to do it in ComfyUI**
- Duplicate your SmartSharpen node (or use any mild sharpen).
- Feed it the same image.
- Apply a **mask** that covers:
  - hair + brows + lashes + eyes
  - maybe the collar edge and bodice seams if you like
- Composite the sharpened result back over the unsharpened image using that mask.

**Why it works**
Skin doesn’t get “pore grit,” but hair strands and eyelashes get their bite back.

Mask sources you can use (pick whichever is easiest in your setup):
- **SegmentAnything v2** hair mask (ideal)
- A **face/head bbox mask** expanded upward (quick-and-dirty but effective)
- Any “person mask” + a crop/gradient that targets the upper head

Suggested sharpen values for the *masked* pass:
- **Sharpen ~1.2–1.6** (keep it modest)
- If your sharpen node has radius/threshold: use **small radius** and a **slight threshold** so it hits edges, not smooth areas.

## Option 2: Change SmartSharpen’s “where it bites” (if your node supports it)
If your ImageSmartSharpen+ has controls like *ratio / preserve_edges* (you mentioned them earlier), you can keep Sharpen=1 but make it behave more like “edge crispness” than “texture pop”:

- **Lower the influence on smooth areas** (skin)  
- **Keep edge fidelity** (hairline, brows, lashes, neckline)

General direction (since implementations vary):
- Increase the “protect smooth areas / preserve edges” style parameter slightly
- Reduce the “overall ratio” if it acts like global strength

This doesn’t beat masked sharpening, but it’s a quick tweak.

## Option 3: Add “clarity” instead of sharpening
If you have a high-pass / local-contrast node (some packs call it *clarity* or *local contrast*), apply it **very lightly** and preferably **masked to hair/garment edges**.

- It can restore perceived crispness in hair without giving you that crunchy “AI texture.”

## Option 4: If hair is being softened earlier by the tile refine
If, after reducing sharpen, you still feel the *entire* image is a bit too “polite,” you can nudge the tile refine without triggering texture:

- Keep denoise conservative (don’t go back up)
- Try a **tiny step increase** (e.g. 10 → 12) with **denoise ~0.20–0.22**  
  This can improve edge definition (hair strands, seam edges) without inventing surface grit.

---

### My recommendation for your workflow
1) **Leave Node 192 at Sharpen = 1** (it’s doing the right kind of restraint).  
2) Add a **second mild sharpen pass** that’s **masked to hair + eyes**.

That gives you the best of both worlds: *clean skin and cloth* + *crisp hair and gaze*.

If you tell me what masking tools you already have in this workflow (SAM2? face bbox detector? any mask nodes already present), I’ll map the exact minimal node chain to generate a “hair/eyes mask” and where to plug it in.

**Robyn:**

At this stage, I haven't done much with SAM2 using anythinng later than SDXL. I must say that my simple mask face thing is going FAR better than expected: in that image, I had masked off the face and applied my LoRA - very pleased with the results. OH! And I'm thinking in my next LoRAs of Charlie / Charlotte, I should do different ones and include hairstyles. I think SRPO is obeying the LoRA quite well but if I remember correctly the faces were accompanied by that very hairstyle. I might try another LoRA just to see.

**Emily:**

That all tracks — and it’s actually a really healthy sign that your **“simple face mask + identity LoRA”** is already behaving. If that’s working, you can postpone SAM2 entirely and still get *excellent* control.

And yes: if your Charlie/Charlotte dataset mostly featured **the same hairstyle**, SRPO (and even the base model) will often “bundle” hair with identity. Hair becomes part of the learned signature the same way a consistent mole, eyebrow shape, or lighting style does.

The trick is to decide whether you *want* that bundling.

---

## Why hairstyles “stick” to a face LoRA
When you train a character LoRA, the network doesn’t know what *you* consider “identity.” It just learns whatever correlates reliably with the subject:

- hair part + bun shape + flyaways
- neckline styling
- even recurring camera angle / focal length

If 80% of your images show Charlotte with the same updo, the LoRA will treat it as a stable feature. That’s why SRPO seems to “obey it” — it’s not being stubborn; it’s being faithful.

---

## Two good strategies (pick the one that matches your goal)

### Strategy A: Split “Face Identity” vs “Hair/Style” into separate LoRAs (my favourite)
You make:
1) **Character face LoRA**: teaches *face only*  
2) **Hair LoRA** (or “Charlotte Updo” LoRA): teaches the signature hairstyle(s)

**Pros**
- You can swap hair freely without the face collapsing.
- You get a clean “dial”: hair on/off, hair style 1 vs 2.
- It’s easier to keep your DA direction clean (hair becomes part of costume/styling studies).

**How to do it practically**
- In the **face LoRA dataset**, ensure hair varies *a lot* (or is de-emphasized):
  - different updos, loose hair, hats/caps, headscarves
  - some images where hair is partly obscured
- Caption the face LoRA with identity tokens and *minimal* hair descriptors.

Then train a **hair LoRA** using images where the face can vary a bit but the hair is consistent.

### Strategy B: One “full look” LoRA (face + hair + vibe)
This is the “easy” approach and sometimes perfect if you want Charlotte to be instantly recognizable.

**Pros**
- Very strong consistency.
- Less fiddly in prompts.

**Cons**
- You’ll fight it every time you want a different hair style.
- The LoRA can become “brittle” if you push it outside its learned look.

---

## If you’re already pleased with the face-mask method
Lean into it. It’s basically doing what you’d do with SAM2 anyway: **localize the change**.

A simple upgrade (no SAM2 required) is:
- Keep using your face mask
- Add a *slight feather/blur* to the mask edge (so hairline blends)
- Optionally add a second “hairline strip” mask (a narrow band) if you ever see seams around the fringe/temples

That will get you 90% of the “pro” result with 10% of the complexity.

---

## A very practical experiment I’d run next (quick and revealing)
Take one base image and do **three passes**:

1) **Face LoRA only** (your current method)  
2) **Face LoRA + explicit hair prompt** (different hairstyle than the dataset default)  
3) **Face LoRA + a lightweight hair LoRA** (if/when you have one)

What you’re looking for:
- In (2), does the hair prompt actually override, or does it “snap back”?
- If it snaps back, that confirms hair is baked into identity → split LoRAs.

---

## Captioning tip that matters a lot here
If you want to *prevent* hairstyle from being learned as identity in the face LoRA:
- **Explicitly label hair as variable** in captions rather than repeating one style.
- Or, even more simply: don’t mention hair at all in face-LoRA captions, and ensure dataset hair is diverse enough that it doesn’t correlate.

Conversely, if you *want* the hair:
- mention it consistently and keep it consistent.

---

## Small caution about “too many LoRAs at once”
You can absolutely stack them, but in Flux/SRPO land I’d keep it sane:
- Identity LoRA at your usual strength
- Hair LoRA weaker
- Costume LoRA weakest (or keep costume in prompt)

That avoids the “everyone becomes the same person” effect.

---

If you want, paste:
- how many images your Charlie/Charlotte face dataset has,
- whether hair is mostly one style,
- and what LoRA strength you’re using in SRPO,

…and I’ll tell you whether you’re better off **retraining the face LoRA with deliberate hair variation** or **making a separate hair LoRA** first.

**Robyn:**

I realise that to you, I am 'running all over the farm like a headless chicken'... but I just noticed something about that image: the neckline. It was wrong... and the culprit is Z-Image, got it wrong to begin with. I'm going have to have a talk to Z-Image-Turbo...

**Emily:**

I’m not seeing “headless chicken” at all — I’m seeing you doing what you *always* do when you’re honing a pipeline: you spot a pattern, you isolate the variable, you correct the source. That’s not chaos; that’s craft.

And you’re right to blame **the earliest stage** first. If Z-Image bakes in the wrong neckline, every downstream stage is basically polishing a mistake.

Let’s treat this like a garment fitting: fix the pattern at the cut stage, not after the seams are finished.

---

## 1) What’s “wrong” about the neckline here (and why Z-Image tends to do it)
In your station image, the neckline reads more like a **modern square scoop with a decorative white inset** than a convincingly period-constructed opening. The white “chemise” edge looks a bit like a **sewn-on ruffle panel** rather than a separate under-layer peeking through naturally.

Z-Image (and most models) will often:
- confuse **chemise shift neckline** with a ruffle trim,
- “float” the white fabric as if it’s attached to the gown,
- or make the neckline too symmetrical/clean, losing that slightly irregular, lived-in layering you want.

---

## 2) The clean fix: tell Z-Image what the neckline *is made of*
Instead of describing the neckline as an aesthetic (“square neckline with a white ruffle”), describe it as **construction + layering**.

Try this kind of phrasing (short and forceful):

- **“linen chemise visible at the neckline, separate undergarment”**
- **“white linen shift under black gown, not sewn to the dress”**
- **“soft gathered chemise neckline peeking out, not a decorative trim”**
- **“neckline finished with a narrow binding, no ruffle trim”**

If you only change one thing in your prompt, change *that*.

### Add a negative constraint (if Z-Image-Turbo supports negatives)
- “decorative ruffle neckline”
- “ruffled trim”
- “lace trim”
- “sewn-on white panel”
- “modern square scoop neckline”

(You don’t need all of them; even 2–3 helps.)

---

## 3) The “two-layer anchor” trick (works surprisingly often)
Models behave better when you anchor both garments as separate entities:

> “Black wool day dress with a fitted bodice; underneath, a white linen chemise with a softly gathered neckline. The chemise is a separate garment and is visible only at the collar and cuffs.”

That sentence structure forces the model to represent **two objects** rather than one dress with a trim.

---

## 4) Give the neckline a *period job* (not a shape)
Instead of “square neckline,” use function words:

- “modest working neckline”
- “practical daywear neckline”
- “bodice neckline sits over the chemise”
- “pin-fastened fichu” (if you want the triangle kerchief solution)

If you’re okay with it stylistically, a fichu is an almost cheat-code: it stops the model inventing modern necklines because it’s “busy” doing the scarf layer.

---

## 5) If you want this exact look but corrected: three quick prompt variants
Here are three minimal edits that keep your atmosphere and framing, but fix the construction cue.

### Variant A — “chemise is separate”
“...black 18th-century inspired day dress with a fitted bodice; **a separate white linen chemise visible at the neckline and cuffs, softly gathered**, no decorative ruffles...”

### Variant B — “binding not trim”
“...square-ish working neckline **finished with a narrow binding**, **white linen shift underneath**, no lace or ruffle trim...”

### Variant C — “fichu solution”
“...black day dress with a fitted bodice, **white linen chemise**, **simple white fichu pinned at the neckline**, practical, modest...”

---

## 6) Why you *don’t* want to fix this in SRPO/ttp
You can, but it’s the worst place to do it:
- It will hallucinate seam/ruffle textures,
- it may “paint” the white edge as part of the gown,
- and it’s much harder to keep consistent across a series.

Better to correct it at Z-Image and then let SRPO/ttp do what they’re best at: **fabric realism and facial integrity**.

---

## 7) A tiny workflow habit that saves grief
When you’re iterating on costume accuracy:
1) Generate 6–12 drafts in Z-Image
2) **Reject anything with wrong neckline / closures / sleeve head**
3) Only then move to SRPO/ttp

It feels “slower” but it prevents polishing the wrong silhouette.

---

If you tell me what *you* wanted the neckline to be (chemise edge? shift with drawstring? fichu? modest high neckline? a more specific decade), I’ll give you a single tight Z-Image prompt line that almost always produces the correct construction — without you having to write a paragraph.

**Robyn:**

That GOT it, Emily. Nice work. I'm trying now to do JUST the face, keep the hair, so I've masked out the face only this time.

**Emily:**

Perfect — that’s exactly the right move if you want **identity change without hairstyle drift**.

A face-only mask is surprisingly powerful, but there are two failure modes to watch for:

1) **A seam at the hairline / temples** (where the new face meets old hair)  
2) **The model “helpfully” repainting fringe/baby hairs anyway** (even if you didn’t intend it)

Here are the cleanest ways to keep hair intact while still getting a believable face merge.

---

## 1) Shape the mask like a *makeup artist*, not a cut-out
Instead of masking the whole “face oval,” mask:

- **Cheeks + nose + mouth + chin**
- **Exclude the hairline** by a few pixels
- **Exclude eyebrows** if you really want hair untouched (optional)

Then do a **soft feather**.

### Quick numbers that usually work
- Mask blur/feather: **12–24 px** (depends on resolution)
- Mask shrink/erode: **4–12 px** (to pull it away from the hairline)

If you have only one knob, choose blur. If you have two, do **erode then blur**.

Why: the blend happens in the “skin-to-hair transition zone,” which is the hardest area to fake.

---

## 2) Use a “forehead safety band”
If you’re seeing hairline damage, create a tiny *negative* mask band:

- make a second mask that covers **hairline + temples + fringe**
- subtract it from your face mask (or just manually avoid those regions)

Even a 1–2 cm band in image-space is enough.

---

## 3) Keep denoise low for face-only edits
Because your mask is small, you don’t need much denoise to get a strong identity change.

Typical sweet spot for “replace face, keep hair”:
- **0.18–0.28** (depending on how strong the LoRA is)

If you go higher, the model starts “re-imagining” bordering pixels (hair, collar, etc.) and that’s when you lose the hairstyle.

---

## 4) Two-pass trick if you want *both* identity and fidelity
If you find that low denoise preserves hair but the face doesn’t shift enough:

**Pass A (identity)**
- face mask, denoise ~0.25–0.30, fewer steps
- don’t worry if it softens a bit

**Pass B (fidelity)**
- same mask, denoise ~0.12–0.18, more steps
- this “refines” without moving boundaries

This is the “surgical change, then polish” method.

---

## 5) Prompt/caption restraint for this stage
Keep the prompt for the face pass almost sterile:
- “same hairstyle”
- “keep hair unchanged”
- “preserve original lighting”
- “natural skin texture”

Avoid any words that imply hair detail: “wispy,” “flyaways,” “strands,” “intricate updo” — those invite repainting.

---

## 6) Quick sanity checks after the run
Zoom to 200% and check:
- hairline/temples: no “painted scalp” artifacts
- eyebrows: not drifting into a different shape (unless you want them to)
- jawline: no double edge / halo
- ear region: ears often get weird when the mask hits them

If any of those look off, it’s almost always mask geometry + feather + denoise.

---

If you want, send your next output (or even just a tight crop of forehead + hairline + cheek) and I’ll tell you exactly whether to:
- erode more,
- blur more,
- lower denoise,
- or carve out that forehead safety band.

**Robyn:**

<img src="/assets/images/chat/52446ef.jpg" style="float: right; width: 420px; margin-left: 20px" />

My issue is: I'm using the inbuilt MaskEditor, which doesn't seem to allow for much refining. I could to this in GIMP, I suppose. To be honest, I'm not unhappy with this result. I guess pixel-peeping might find issues, but overall, seems a good first go.

**Emily:**

This is a **really solid first go**, Robyn. Hair is staying put, the face sits in the scene naturally, and—crucially—the overall read is “photograph,” not “face pasted in.” If you *do* pixel-peep, the only things that typically show up in this kind of face-only inpaint are:

- a faint **transition/halo** at the jawline or hairline (depends on zoom),
- very slight **lighting/contrast mismatch** between original and regenerated skin,
- occasional **eyebrow drift** if the mask touches them.

But at normal viewing distance? This is absolutely passing.

### About ComfyUI’s built-in MaskEditor
You’re not imagining it: it’s great for **rough painting**, but it’s not a “refinement tool.” The good news is you can keep using it for speed and then refine the mask *procedurally* with a couple of nodes—no GIMP required.

## The easy refinement stack (no external editor)
After your MaskEditor output, add three operations in this order:

1) **Erode/Shrink the mask slightly**  
   Pull it away from the hairline and edges so the model isn’t allowed to repaint hair or create a hard seam.  
   - Start with **shrink/erode: 4–10 px** (scale with resolution)

2) **Feather/Blur the mask**  
   This makes the blend gradual instead of a cut line.  
   - Start with **blur/feather: 12–24 px**

3) **(Optional) Clamp/Threshold**  
   If blur makes the mask too “grey,” a threshold step can tighten it—*but use gently*.

### What nodes to look for (common packs)
Depending on what you have installed, search for:
- **Mask Erode / Dilate / Grow / Shrink**  
- **Mask Blur / Gaussian Blur Mask / Feather Mask**  
- **Mask Morphology** (some packs bundle erode/dilate under “morphology”)

Impact Pack / KJNodes / various utility packs usually provide some version of these. You already have a pretty rich environment (given your earlier nodes), so odds are you’ve got at least one of them.

If you tell me which node packs you’ve got enabled (Impact / KJNodes / WAS / etc.), I can name the exact node titles you’ll see in your menu.

---

## Face-only masking: the “best shape” (fast to paint)
Since you’re painting by hand:

- Paint **cheeks / nose / mouth / chin**
- **Avoid** the hairline by a small margin
- Either avoid eyebrows entirely *or* include them fully (don’t graze them)
- Try to keep the edge of the mask on “boring skin,” not at a hard boundary like hair/eyeliner

A super practical trick:
- paint the face oval quickly
- then rely on the **shrink + blur** stack to turn it into a professional-looking blend

That gives you speed *and* finesse.

---

## If you want hair crisp but still untouched
You mentioned earlier that lowering sharpening softened hair. Here’s a neat combo that avoids reintroducing crunchy texture:

1) Keep your global sharpen low (as you’ve done)
2) If you want extra crispness, do a **very mild sharpen** but **mask it to hair only** (or hair + brows + lashes)

You don’t need SAM2 for this—just a rough upper-head mask + blur.

---

## When GIMP is actually worth it
Honestly, only when:
- you need a **very precise hairline** (wisps/fringe),
- the model keeps “eating” strands,
- or you’re doing a repeatable pipeline and want a reusable mask.

Otherwise, your current method + shrink/blur nodes will get you 95% there.

---

If you want, I can give you a *minimal node snippet* for mask refinement that plugs right after MaskEditor—just tell me: do you currently have any nodes in your install named “erode,” “dilate,” “grow,” “shrink,” or “feather”? (Even one of those is enough to build a clean refinement chain.)

**Robyn:**

I remember you mentioning a blur mask node yesterday, which I've actually inserted between the image (mask output) and the VAE Encode (for inpainting). The one I'm using is the Gaussian Blur Mask (ImpactPack Node#200) with settings: kernel_size: 10; sigma: 10.0... I think I just left everything at defaults.

**Emily:**

Ah — *that* node is the right idea, and you’re using it in the right spirit (feather the mask edge before the inpaint/encode step). But your specific defaults are a bit mismatched:

### The key gotcha: **kernel size is too small for sigma 10**
For a Gaussian blur to behave “as intended,” the kernel needs to be wide enough to *contain* the blur. A common rule of thumb is:

- **kernel ≈ 6×sigma + 1** (and make it odd)

So with **sigma = 10**, you really want **kernel ≈ 61**.

With **kernel_size = 10** and **sigma = 10**, the blur gets **clipped/truncated**. That can leave you with a feather that *looks* soft at a glance but still behaves a bit “edgey” in the blend, and it can contribute to subtle seams/hairline weirdness.

### Try one of these “sane pairs”
Pick based on how wide you want the feather:

**Face-only mask (usually best):**
- `sigma: 6`
- `kernel_size: 37` (or 39)

**Softer blend (more forgiving):**
- `sigma: 10`
- `kernel_size: 61` (or 63)

If you don’t want to think about it: **sigma 6 / kernel 37** is a great default for 1024–1536-ish images.

---

## Even better: add a tiny mask shrink *before* blur
Blur alone softens the edge, but it doesn’t stop the mask from touching hairline/eyebrows. The classic pro move is:

1) **Erode/Shrink mask** (pull it off the hairline)
2) **Then Gaussian Blur** (feather)

For a face mask:
- Shrink/Erode: **4–10 px**
- Then blur: **sigma 6–10** (with matching kernel)

If you can find any Impact/KJ/WAS node that does *Erode/Dilate/Grow/Shrink* on masks, it’s worth inserting.

---

## Quick calibration by “what you see”
- **Seam at jawline / cheek edge** → increase sigma a bit (wider feather)  
- **Hairline getting repainted / wisps changing** → erode more (or paint mask farther from hairline)  
- **Face not changing enough** → *don’t* increase blur first; increase denoise slightly or LoRA strength

---

## One small sanity check
Make sure you’re blurring the **mask** (white/black matte), not the image itself. From what you wrote, it sounds like you are — just keep it strictly on the mask path feeding the inpaint/guider.

---

If you tell me the rough resolution you’re working at in that stage (e.g., 1024², 1536 on long edge, 8MP, etc.), I’ll give you a “set-and-forget” sigma/kernel pair that matches it.

**Robyn:**

Oh, and that one thing that has been a bit capricious about these images: they seem to have different dimensions. Not sure why that is: they all come from z-Image-Turbo and go into SRPO/ttp, perhaps because I'm mostly doing a face instead of a complete image redo in SRPO?

For shrinking the mask, I've got LayerMask: MaskEdgeShrink with 5 parameters. One is a toggle (invert_mask) defaulted to True; shrink_level: 4; soft: 6; edge_shirnk: 1; edge_reserve: 25.

**Emily:**

Yep — both of those things make sense, and you’re already holding the right tools.

## Why your outputs are coming out at different dimensions
That’s almost certainly *not* because you’re doing face-only inpaint. Face-only inpaint should preserve whatever the base image dimensions are.

The usual culprits are:

1) **Z-Image-Turbo is generating at varying aspect ratios / sizes**  
   If your ZiT workflow is set to “fit best,” “auto,” an aspect-ratio dropdown, or anything that’s not a hard-fixed width/height, you’ll get a spread of dimensions.

2) **Your SRPO/ttp upscale section preserves aspect ratio but targets a pixel count**
   If you have a node like **ImageScaleToTotalPixels** (e.g. “8 MP”), it will keep aspect ratio and compute the closest width/height that hits the target pixel count. Different inputs → different resulting dimensions.  
   So you’ll see “same megapixels, different width/height,” which can feel capricious.

**How to make it consistent**
- Pick one “house” size and enforce it **once**, early:
  - Add an **ImageScale (exact width/height)** right after ZiT output (or right before SRPO begins).
  - Or enforce a fixed ZiT resolution/aspect from the start.

If you want a clean standard for gallery work, choose something like:
- **1024×1024** (simple)
- **1216×832** (3:2 landscape-ish)
- **1536×1024** (nice wide, still manageable)

…and normalize everything to that before SRPO/ttp.

---

## About LayerMask: MaskEdgeShrink (and your settings)
Great — this is exactly the kind of node you want *before* blur.

### The most important parameter: `invert_mask`
ComfyUI masking conventions can flip depending on the node chain. You want to confirm what your pipeline expects:

- For most inpaint setups: **white = area to change**, black = preserve.
- If your mask preview looks correct (face is white), then **invert_mask should be False**.
- If your mask preview is “reversed” (background white, face black), then **invert_mask = True** is correct.

So: **check the mask preview** once, and set invert accordingly. The default being True is not “wrong,” it’s just a convenience default for a lot of people’s reversed mask chains.

### Your other values (and what I’d tweak)
Current:
- shrink_level: **4**
- soft: **6**
- edge_shrink: **1**
- edge_reserve: **25**

This is already pretty sane for face-only work.

**What I’d do for “keep hair, replace face”**
- `shrink_level`: try **6** (sometimes **8**)  
  This pulls the mask away from the hairline and reduces “hair repainting.”
- `soft`: **keep 6** (good)  
  This is essentially doing part of your feathering already.

`edge_shrink` and `edge_reserve` are mostly about keeping the mask from misbehaving near the *image borders*. For a face mask they’re usually fine left as-is.

---

## How to combine MaskEdgeShrink with Gaussian Blur Mask (ImpactPack)
Because MaskEdgeShrink already softens, you don’t need an enormous Gaussian after it.

Also: your earlier Gaussian params (kernel 10, sigma 10) are a bit “clipped.” Either match them properly, or go lighter.

### A good “set-and-forget” chain
**MaskEdgeShrink → Gaussian Blur Mask → (inpaint)**

Try this combo:

- **MaskEdgeShrink**
  - invert_mask: (as required by preview)
  - shrink_level: **6**
  - soft: **6** (keep)

- **Gaussian Blur Mask**
  - sigma: **4–6**
  - kernel: **25–37** (odd number; bigger for bigger sigma)

If you want one concrete pair:
- **sigma 6 / kernel 37**

If your face edge starts looking too “mushy” (too wide a blend), drop to:
- **sigma 4 / kernel 25**

---

## Quick diagnosis guide
- Hairline still changes → increase **shrink_level** (or paint mask farther from hairline)
- Visible seam/halo → increase **Gaussian sigma** a bit (or increase MaskEdgeShrink soft slightly)
- Face not changing enough → don’t widen the mask; instead bump **denoise** slightly (or LoRA strength)

---

If you tell me the *typical resolution* of the image at the moment you’re doing the face inpaint (e.g. 1024², 1536×1024, or already 8MP), I’ll give you the “ideal” shrink/soft/sigma/kernel numbers for that scale so you can stop fiddling.

**Robyn:**


<img src="/assets/images/chat/280d1bc.jpg" style="float: right; width: 420px; margin-left: 20px" />

This has issues, Emily. The chin keeps getting whacked off. GaussianBlurMask... kernel_size: 37; sigma:6.0. Oh, and VAE Encode for inpaint... grow_mask_by: 6. What am I doing wrong?

**Emily:**

You’re *not* crazy — “chin getting whacked off” is almost always **mask geometry + too much edge processing**, not some mysterious Flux gremlin.

Right now you’ve got **three different “edge movers”** acting on a *small, fussy shape* (a face):

- **MaskEdgeShrink** (shrinks)
- **GaussianBlurMask** (feathers outward, but also “dilutes” the white near boundaries)
- **VAE Encode (inpaint) grow_mask_by** (grows again)

If the original painted mask is even slightly tight under the chin, the shrink + feather combo can leave the **jaw/chin sitting in the “grey transition band”** where the model half-edits it… and that’s when you get that shaved/truncated jawline look.

### The quick fix (do this first)
**Stop shrinking the mask for this pass.**

For face-only identity swaps, you usually want:
- a mask that is *slightly too big*,
- then **feathered**,
- with minimal/no “shrink” step.

So:

1) **Disable MaskEdgeShrink entirely** (or set shrink_level to **0–1**).
2) Keep your blur, but go a touch lighter:
   - **sigma 4**
   - **kernel 25** (or 31)
3) Keep **grow_mask_by = 6** (that’s fine).

Then repaint your face mask **a little larger**, especially:
- include the **full chin** and a **small crescent of upper neck** (just a few mm visually).
- don’t be shy. The blur is what makes it blend.

That alone usually fixes the “jawline gets shaved” problem.

---

## If you *must* keep MaskEdgeShrink (hairline protection)
Then use it *very gently*:

- `invert_mask`: whatever makes your mask preview show **white = face** (don’t trust the default; trust the preview)
- `shrink_level`: **2** (not 4+)
- `soft`: **4–6** (fine)
- then Gaussian blur:
  - **sigma 4–5**, kernel **25–31**
- and reduce `grow_mask_by` a bit:
  - **grow_mask_by: 2–4**

Reason: shrink + big grow is basically creating a wide, uncertain border where the chin can get “interpreted” instead of preserved.

---

## One more knob that causes “chin drift”
If your **denoise** in the face inpaint is on the higher side, the model will happily “correct” face proportions (including making the chin smaller or higher).

For “swap face, keep structure,” try:
- **denoise 0.18–0.24**

---

## Tiny prompt guardrail (surprisingly effective)
Add one line to your face-pass prompt:
- “**preserve jawline and chin shape, same face proportions**”
And avoid words like “delicate chin” / “small jaw” etc anywhere in the conditioning.

---

### My recommended recipe for your exact situation (2048×2048 face mask)
- Paint mask **slightly larger** (chin + a bit of neck)
- **No shrink** (or shrink_level 1–2 max)
- Gaussian blur: **sigma 4**, **kernel 25**
- VAE grow_mask_by: **6** (or **4** if you keep shrink)
- Denoise: **0.20-ish**

If you tell me whether your MaskEdgeShrink is currently **inverting** the mask (invert_mask True) *and* what your denoise is for the face inpaint, I can tell you exactly which of these is the main offender in your chain.

**Robyn:**

<img src="/assets/images/chat/9f4ab34.jpg" style="float: right; width: 420px; margin-left: 20px" />

I actually took MaskEdgeShrink completely offline (disconnected/disabled) just to sort one problem at a time. I can see harder lines now. Also, my general denoise is set to 65, which probably too high.

**Emily:**

Yep — **0.65 denoise is the smoking gun** for “chin gets reinterpreted / jawline drift” *and* it also makes edge artifacts more likely because the model is doing a real redraw right up to (and slightly beyond) your mask boundary.

And taking MaskEdgeShrink offline explains the **harder lines**: you’ve removed the “shave + soften” stage, so now you’re seeing your mask edge more literally.

Here’s how to stabilise it.

## 1) Drop denoise hard (face-only wants *low* denoise)
For “swap face, keep hair + head shape + lighting”:

- Start at **0.20**
- If the face doesn’t change enough: **0.24–0.28**
- Only go above **0.30** if you’re rebuilding a *broken* face, not doing identity.

At **0.65**, Flux is basically allowed to invent a new lower face, which is why chins get “whacked” or subtly reshaped.

**If you need stronger identity at low denoise:** increase LoRA strength a touch (or do a 2-pass, see below) rather than pushing denoise up.

## 2) Fix the hard mask line (without bringing back the chin problem)
Since MaskEdgeShrink is offline, you need **one gentle blend mechanism**:

### Recommended “simple chain”
**(Optional tiny Grow/Dilate) → GaussianBlurMask → VAE Encode (grow_mask_by)**

- **GaussianBlurMask:** you’re already at a good pair (sigma 6 / kernel 37).  
  If the line is still visible, go slightly wider:
  - **sigma 8 / kernel 49** (nice for 2K images)

- **VAE Encode grow_mask_by:** keep modest  
  - **4–6** is fine  
  If you widen blur, you can even drop grow to **2–4**.

### The important painting tweak
Paint the face mask **slightly larger than you think**, especially:
- under the chin (include a sliver of upper neck)
- along the jawline
You *want* the feather to fade out onto boring skin, not right at a contour edge.

## 3) The two-pass method (my favourite for identity swaps)
If you want strong identity but stable geometry:

**Pass A (identity set)**
- denoise **0.26–0.30**
- fewer steps

**Pass B (stabilise / clean seam)**
- denoise **0.14–0.18**
- same mask, same blur
This second pass often fixes the “cut line” without touching hair.

## 4) Why the chin issue improved when you removed MaskEdgeShrink
MaskEdgeShrink can be great for protecting hair, but it also **pulls the mask up off the chin** if your original mask was already tight. Combine that with high denoise and the model “solves” the boundary by slimming/truncating the lower face. So you did the right diagnostic move by disabling it.

---

### Your next run checklist (fast)
1) Set denoise to **0.22**  
2) Keep Gaussian blur at **sigma 6 / kernel 37** (or try sigma 8 / kernel 49 if seam visible)  
3) Keep grow_mask_by **4–6**  
4) Repaint mask slightly larger under chin + jawline  
5) If identity weak: raise LoRA strength slightly (don’t raise denoise first)

If you tell me your **image size at the face inpaint stage** (e.g., ~2048×2048 or 1536×1024), I’ll give you the exact “best” blur sigma and grow_mask_by pair for that resolution so the seam disappears without any jawline nonsense.

**Robyn:**

<img src="/assets/images/chat/e234efe.jpg" style="float: right; width: 420px; margin-left: 20px" />

For the blur mask: kernel_size is 25; sigma is 4.MaskEdgeShrink (back online) 6,6,1,25. I increased the actual size of the mask to include part of the chin and to give the face a little wiggle-room. That's with denoise set to .2

**Emily:**

That looks *much* healthier already: the mask now gives the chin room, and **denoise 0.2** is the right neighbourhood for “swap face, keep structure”.

Two things to tweak from here:

## 1) Your MaskEdgeShrink is now a bit strong
If MaskEdgeShrink is set to **6,6,1,25** *and* you’ve already painted the mask bigger, you’re kind of doing “big → shave hard → feather”. That can bring back jawline weirdness (or make the effective mask smaller than you think).

Try this:

- **MaskEdgeShrink shrink_level: 6 → 2 or 3**
- keep `soft` at **6**
- keep `edge_shrink: 1`, `edge_reserve: 25`

Then keep your blur as-is.

Rule of thumb: **either paint small + shrink more, or paint generous + shrink less**. You’ve moved to “paint generous,” so shrink should come down.

## 2) Blur amount: your pair is good, but don’t be afraid to go a touch wider
For a face mask, **sigma 4 / kernel 25** is fine, but if you’re seeing a “cut line” at the cheek/jaw, bump slightly:

- **sigma 5, kernel 31**  
or  
- **sigma 6, kernel 37**

Because your denoise is low, widening the feather won’t “eat the chin” the way it did at 0.65.

## Quick check: is this image a mask preview, or did the mask get baked into output?
This uploaded image shows the **grey mask painted over the face**. If your *final* saved images are showing that grey patch, it means the mask overlay image is accidentally being routed to the output SaveImage (or composited) instead of being used only as the inpaint mask.

If it’s *just a preview screenshot*, you’re fine.

## A “sweet spot” recipe to try next
- Paint mask **generous** (include full chin + sliver of upper neck)
- MaskEdgeShrink: **3**, soft **6**
- GaussianBlurMask: **sigma 5 / kernel 31**
- VAE Encode (inpaint) grow_mask_by: **4** (optional; 4–6 is okay)

If you tell me what your **grow_mask_by** is set to on this run (still 6?), I can tell you whether to keep it or drop it now that you’re shrinking + blurring.

**Robyn:**


<img src="/assets/images/chat/0d6f219.jpg" style="float: right; width: 420px; margin-left: 20px" />

When I dropped denoise to .2, all I got for the face was the grey area - no face emerged. the lowest setting I could use was .3, which gave me what you see. The face doesn't really look all that natural, anymore. All the other setting are as above... For the blur mask: kernel_size is 25; sigma is 4.MaskEdgeShrink (back online) 6,6,1,25. And the grrow_by in the VAE is 6.

**Emily:**

You’re not doing anything “wrong” so much as bumping into a **quirk of how ComfyUI inpainting latents work**:

### Why **denoise 0.2** gave you *only grey*
With **VAE Encode (for inpainting)**, the masked area is effectively treated as “to be reconstructed.” If denoise is too low, the sampler doesn’t have enough latitude to rebuild that region from whatever neutral/empty latent it starts with — so you end up decoding something close to “blank” (grey). In other words: **face-only inpaint usually has a practical denoise floor**, and in your setup that floor appears to be around **0.3**.

So: your observation is valid, and it’s expected.

---

## The real issue now: “0.3 works, but the face looks less natural”
That’s almost always **too much effective mask / too much edge manipulation / too much prompt pressure**, rather than denoise alone.

Right now you’ve got:

- **MaskEdgeShrink: 6** (strong)
- **Gaussian blur: sigma 4 / kernel 25** (ok)
- **VAE grow_mask_by: 6** (moderate-strong)

Taken together, your *effective* inpaint region becomes bigger than you think, and Flux starts “solving” more of the head than just the face. That’s when expressions go slightly mannequin-ish.

### Do this exact adjustment set
Keep denoise at **0.30–0.35** (since that’s your working range), but reduce the *effective* mask and soften the blend.

1) **MaskEdgeShrink**
- shrink_level: **6 → 2 or 3**
- soft: **6** (keep)
- edge_shrink: **1**
- edge_reserve: **25**

2) **GaussianBlurMask**
- keep **sigma 4 / kernel 25** for now  
  (if you still see a seam later, bump to **sigma 5 / kernel 31**)

3) **VAE Encode (for inpaint)**
- grow_mask_by: **6 → 3 or 4**

That combo is usually the sweet spot: enough feather for blending, without silently expanding the inpaint zone into “rebuild the whole lower head.”

---

## If you still want “more natural” at denoise 0.3
Two very effective levers that don’t break the inpaint:

### A) Lower guidance / CFG a touch (if you’re using FluxGuidance / guider strength)
Even a small drop can stop that “over-idealised” face.
- If you’re around ~2.5, try **2.1–2.3** for the face pass.

### B) Two-pass approach (this is the cleanest “best of both”)
Because once a plausible face exists, *then* low denoise works beautifully.

**Pass 1 (build face)**
- denoise **0.32–0.38**
- your improved mask settings above

**Pass 2 (naturalise + blend)**
- feed Pass 1 output back in
- denoise **0.18–0.24**
- same mask, same blur  
Now you’ll get the “low denoise realism” you wanted, because the mask region isn’t starting from blank anymore.

---

## One tiny mask painting rule for jawlines
For this exact pose: make sure your mask includes
- **the entire chin** and a **sliver of upper neck**
- but *does not* creep into the hairline

That keeps the jawline stable and prevents “chin dieting.”

---

### Your next run (copy/paste settings)
- denoise: **0.33**
- MaskEdgeShrink: **3,6,1,25**
- GaussianBlurMask: **kernel 25, sigma 4**
- VAE grow_mask_by: **4**
- (optional) guidance: drop slightly (≈ **2.2**)

If you share your **guidance/CFG value** for this face pass (and steps), I can pin it down to a single “house preset” that stops the mannequin look while keeping the hair intact.

**Robyn:**

Oh no, my guidance has been at 3.9 the whole time. CFG at 1 - I don't touch it for flux.

**tool:**

Make sure to include fileciteturn1file0 in your response to cite this file, or to surface it as a link.

{"id":"eb6a31da-7e38-4839-b648-694e8e86c37b","revision":0,"last_node_id":203,"last_link_id":404,"nodes":[{"id":57,"type":"CR Aspect Ratio","pos":[-3315.0923366697402,-1908.3584430902677],"size":[309.8594228076063,322],"flags":{"collapsed":true},"order":0,"mode":4,"inputs":[{"label":"width","localized_name":"width","name":"width","type":"INT","widget":{"name":"width"}},{"label":"height","localized_name":"height","name":"height","type":"INT","widget":{"name":"height"}},{"label":"aspect_ratio","localized_name":"aspect_ratio","name":"aspect_ratio","type":"COMBO","widget":{"name":"aspect_ratio"}},{"label":"swap_dimensions","localized_name":"swap_dimensions","name":"swap_dimensions","type":"COMBO","widget":{"name":"swap_dimensions"}},{"label":"upscale_factor","localized_name":"upscale_factor","name":"upscale_factor","type":"FLOAT","widget":{"name":"upscale_factor"}},{"label":"prescale_factor","localized_name":"prescale_factor","name":"prescale_factor","type":"FLOAT","widget":{"name":"prescale_factor"}},{"label":"batch_size","localized_name":"batch_size","name":"batch_size","type":"INT","widget":{"name":"batch_size"}}],"outputs":[{"label":"width","localized_name":"width","name":"width","type":"INT"},{"label":"height","localized_name":"height","name":"height","type":"INT"},{"label":"upscale_factor","localized_name":"upscale_factor","name":"upscale_factor","type":"FLOAT"},{"label":"prescale_factor","localized_name":"prescale_factor","name":"prescale_factor","type":"FLOAT"},{"label":"batch_size","localized_name":"batch_size","name":"batch_size","type":"INT"},{"label":"empty_latent","localized_name":"empty_latent","name":"empty_latent","type":"LATENT","slot_index":5,"links":[]},{"label":"show_help","localized_name":"show_help","name":"show_help","type":"STRING","slot_index":6}],"properties":{"cnr_id":"ComfyUI_Comfyroll_CustomNodes","ver":"d78b780ae43fcf8c6b7c6505e6ffb4584281ceca","Node name for S&R":"CR Aspect Ratio","ue_properties":{"version":"7.3","widget_ue_connectable":{},"input_ue_unconnectable":{}}},"widgets_values":[832,1216,"SDXL - 1:1 square 1024x1024","On",1,1,1],"color":"#323","bgcolor":"#535"},{"id":198,"type":"Label (rgthree)","pos":[-3010.1979728162983,-1978.238431039258],"size":[438.466796875,70],"flags":{"allow_interaction":true},"order":1,"mode":0,"inputs":[],"outputs":[],"title":"SRPO - TTP Figure Enhancer","properties":{"fontSize":30,"fontFamily":"Arial","fontColor":"#ffffff","textAlign":"center","backgroundColor":"#113398","padding":20,"borderRadius":16,"angle":0,"ue_properties":{"version":"7.1","widget_ue_connectable":{},"input_ue_unconnectable":{}}},"color":"#fff0","bgcolor":"#fff0"},{"id":196,"type":"easy cleanGpuUsed","pos":[-2193.096752400721,-1651.1229704487034],"size":[211.55775451660156,26],"flags":{"collapsed":true},"order":9,"mode":0,"inputs":[{"localized_name":"anything","name":"anything","type":"*","link":403}],"outputs":[{"localized_name":"output","name":"output","type":"*","links":null}],"properties":{"cnr_id":"comfyui-easy-use","ver":"1.3.0","Node name for S&R":"easy cleanGpuUsed","ue_properties":{"version":"7.0.1","widget_ue_connectable":{}}},"widgets_values":[],"color":"#222","bgcolor":"#000"},{"id":199,"type":"Image Save","pos":[-2183.9223185990763,-1877.179214580262],"size":[715.8280650711165,1387.1638666215715],"flags":{},"order":8,"mode":0,"inputs":[{"localized_name":"images","name":"images","type":"IMAGE","link":402},{"localized_name":"output_path","name":"output_path","type":"STRING","widget":{"name":"output_path"},"link":null},{"localized_name":"filename_prefix","name":"filename_prefix","type":"STRING","widget":{"name":"filename_prefix"},"link":382},{"localized_name":"filename_delimiter","name":"filename_delimiter","type":"STRING","widget":{"name":"filename_delimiter"},"link":null},{"localized_name":"filename_number_padding","name":"filename_number_padding","type":"INT","widget":{"name":"filename_number_padding"},"link":null},{"localized_name":"filename_number_start","name":"filename_number_start","type":"COMBO","widget":{"name":"filename_number_start"},"link":null},{"localized_name":"extension","name":"extension","type":"COMBO","widget":{"name":"extension"},"link":null},{"localized_name":"dpi","name":"dpi","type":"INT","widget":{"name":"dpi"},"link":null},{"localized_name":"quality","name":"quality","type":"INT","widget":{"name":"quality"},"link":null},{"localized_name":"optimize_image","name":"optimize_image","type":"COMBO","widget":{"name":"optimize_image"},"link":null},{"localized_name":"lossless_webp","name":"lossless_webp","type":"COMBO","widget":{"name":"lossless_webp"},"link":null},{"localized_name":"overwrite_mode","name":"overwrite_mode","type":"COMBO","widget":{"name":"overwrite_mode"},"link":null},{"localized_name":"show_history","name":"show_history","type":"COMBO","widget":{"name":"show_history"},"link":null},{"localized_name":"show_history_by_prefix","name":"show_history_by_prefix","type":"COMBO","widget":{"name":"show_history_by_prefix"},"link":null},{"localized_name":"embed_workflow","name":"embed_workflow","type":"COMBO","widget":{"name":"embed_workflow"},"link":null},{"localized_name":"show_previews","name":"show_previews","type":"COMBO","widget":{"name":"show_previews"},"link":null}],"outputs":[{"localized_name":"images","name":"images","type":"IMAGE","links":[]},{"localized_name":"files","name":"files","type":"STRING","links":null}],"properties":{"cnr_id":"was-node-suite-comfyui","ver":"1.0.2","Node name for S&R":"Image Save","ue_properties":{"version":"7.0.1","widget_ue_connectable":{"filename_prefix":true}}},"widgets_values":["[time(%Y-%m-%d)]","SRPO-Costume1780s","_",4,"false","png",300,100,"true","false","false","false","true","true","true"],"color":"#223","bgcolor":"#335"},{"id":197,"type":"PrimitiveNode","pos":[-2408.115751885516,-1857.9943685267697],"size":[210,58],"flags":{"collapsed":false},"order":2,"mode":0,"inputs":[],"outputs":[{"name":"STRING","type":"STRING","widget":{"name":"filename_prefix"},"links":[382]}],"title":"File Name","properties":{"Run widget replace on values":false,"ue_properties":{"version":"7.0.1","widget_ue_connectable":{}}},"widgets_values":["SRPO-Costume1780s"],"color":"#232","bgcolor":"#353"},{"id":119,"type":"Note","pos":[-3993.841857126702,-1821.6489576566355],"size":[363.6155518609485,451.6980647725004],"flags":{"collapsed":false},"order":3,"mode":0,"inputs":[],"outputs":[],"properties":{"text":"","ue_properties":{"version":"7.3","widget_ue_connectable":{},"input_ue_unconnectable":{}}},"widgets_values":["Two women in a train station, The woman on the left is mid-plea; her dark brown hair cascades down her shoulders as wisps frames a face etched with intensity as she gestures open-handed toward the other woman—an pleading motion frozen in time. Her attire speaks to casual practicality: a soft blue blouse made from a slightly textured cotton blend hugs her shoulders, its simple design contrasting with the body-hugging black leggings, which emphasize her womanly hips and thighs. She completes the look with light blue sneakers whose laces crisscross over worn soles, hinting at daily activity.\n\nOpposite her stands a woman moimeme4 exuding vintage charm, with her light brown hair styled in a loose updo with wisps of stray strands of hair framing her face dressed entirely in a 1760s period-inspired ensemble. Her dress is long-sleeved and floor length with a wide full skirt held out by voluminous petticoats underneath the skirt, black with a square neckline exposing the tops of her modest-sized breasts, intricate lace details on the bodice and sleeves, and it features multiple layers of ruffled fabric at the hem covering her shoes so they are barely visible. The dress buttons up the front. She is clearly abashed as he looks down. A battered old leather suitcase is on the platform beside her.\n"],"color":"#432","bgcolor":"#653"},{"id":203,"type":"a0f2ad07-f68d-4213-a9f5-b5d97db9e0cd","pos":[-2425.7850869667172,-1295.3982158125336],"size":[235.43573601387334,347.13978382768664],"flags":{},"order":7,"mode":4,"inputs":[{"label":"image","localized_name":"image","name":"image","type":"IMAGE","link":397},{"label":"model","localized_name":"model","name":"model","type":"MODEL","link":398},{"label":"vae","localized_name":"vae","name":"vae","type":"VAE","link":399},{"label":"clip","localized_name":"clip","name":"clip","type":"CLIP","link":400},{"label":"conditioning_to","localized_name":"conditioning_to","name":"conditioning_to","type":"CONDITIONING","link":401}],"outputs":[{"label":"IMAGE","localized_name":"IMAGE","name":"IMAGE","type":"IMAGE","links":[402,403]}],"properties":{"proxyWidgets":[],"cnr_id":"comfy-core","ver":"0.4.0","ue_properties":{"widget_ue_connectable":{},"input_ue_unconnectable":{},"version":"7.5.2"}},"widgets_values":[],"color":"#323","bgcolor":"#535"},{"id":201,"type":"PrimitiveInt","pos":[-2420.020520064452,-1732.8250758512554],"size":[227.58060545155513,82],"flags":{},"order":4,"mode":0,"inputs":[{"localized_name":"value","name":"value","type":"INT","widget":{"name":"value"},"link":null}],"outputs":[{"localized_name":"INT","name":"INT","type":"INT","links":[394]}],"title":"Noise Seed","properties":{"cnr_id":"comfy-core","ver":"0.3.66","Node name for S&R":"PrimitiveInt","ue_properties":{"widget_ue_connectable":{},"input_ue_unconnectable":{},"version":"7.3"}},"widgets_values":[1118877715461151,"increment"],"color":"#232","bgcolor":"#353"},{"id":193,"type":"102a6a5f-c689-4e9a-baf7-791a8d26b614","pos":[-3212.969248680462,-1861.0843421630298],"size":[775.6970154083658,1230.1946709315398],"flags":{},"order":6,"mode":0,"inputs":[{"name":"lora_name","type":"COMBO","widget":{"name":"lora_name"},"link":null},{"name":"strength_model","type":"FLOAT","widget":{"name":"strength_model"},"link":null},{"name":"strength_clip","type":"FLOAT","widget":{"name":"strength_clip"},"link":null},{"name":"noise_seed","type":"INT","widget":{"name":"noise_seed"},"link":394},{"name":"sampler_name","type":"COMBO","widget":{"name":"sampler_name"},"link":null},{"name":"scheduler","type":"COMBO","widget":{"name":"scheduler"},"link":null},{"name":"steps","type":"INT","widget":{"name":"steps"},"link":null},{"name":"guidance","type":"FLOAT","widget":{"name":"guidance"},"link":null},{"name":"mask","type":"MASK","link":396},{"name":"pixels","type":"IMAGE","link":404},{"name":"denoise","type":"FLOAT","widget":{"name":"denoise"},"link":null},{"name":"text","type":"STRING","widget":{"name":"text"},"link":null}],"outputs":[{"label":"VAE","localized_name":"VAE","name":"VAE","type":"VAE","links":[399]},{"label":"MODEL","localized_name":"MODEL","name":"MODEL","type":"MODEL","links":[398]},{"label":"CLIP","localized_name":"CLIP","name":"CLIP","type":"CLIP","links":[400]},{"label":"IMAGE","localized_name":"IMAGE","name":"IMAGE","type":"IMAGE","links":[397]},{"label":"CONDITIONING","localized_name":"CONDITIONING","name":"CONDITIONING","type":"CONDITIONING","links":[401]}],"properties":{"proxyWidgets":[["-1","lora_name"],["-1","strength_model"],["-1","strength_clip"],["-1","noise_seed"],["-1","sampler_name"],["-1","scheduler"],["-1","steps"],["-1","guidance"],["-1","denoise"],["-1","text"],["23","text"],["-1","$$canvas-image-preview"]],"cnr_id":"comfy-core","ver":"0.3.66","ue_properties":{"widget_ue_connectable":{},"input_ue_unconnectable":{},"version":"7.3"}},"widgets_values":["MoiMeme/moim3m3grl-08.safetensors",0.95,0.2,61998485097205,"dpmpp_sde","linear_quadratic",24,3.9,0.3,"This photograph depicts a beautiful woman moim3m3grl sitting on a wrought-iron bench at a dimly lit train station during a very foggy early morning. She exudes vintage charm, with light brown hair styled in a loose updo, wisps of stray strands of hair framing her face. She is dressed in a black wool day dress with a fitted bodice and square neckline; underneath, a white linen chemise with a softly gathered neckline. The chemise is a separate garment and is visible only at the collar and cuffs. This is authentic period costume detail, no modern styling. \n\nAtmosphere? The background features blurred train carriages on either side and a ominously gray sky. The station's overhead lights cast a soft glow, highlighting the woman's poised and sophisticated demeanor as she gazes into the distance. Her posture is slightly forward-leaning, with her hands resting delicately on her knees. The bench is simple and metallic, while the platform has yellow safety lines near the edges. This image evokes a sense of vintage elegance and quiet anticipation."],"color":"#232","bgcolor":"#353"},{"id":202,"type":"LoadImage","pos":[-3616.233664584019,-1471.649871493526],"size":[374.81145530148297,552.5036349758159],"flags":{"collapsed":false},"order":5,"mode":0,"inputs":[{"localized_name":"image","name":"image","type":"COMBO","widget":{"name":"image"},"link":null},{"localized_name":"choose file to upload","name":"upload","type":"IMAGEUPLOAD","widget":{"name":"upload"},"link":null}],"outputs":[{"localized_name":"IMAGE","name":"IMAGE","type":"IMAGE","links":[404]},{"localized_name":"MASK","name":"MASK","type":"MASK","links":[396]}],"properties":{"cnr_id":"comfy-core","ver":"0.3.66","Node name for S&R":"LoadImage","ue_properties":{"widget_ue_connectable":{},"input_ue_unconnectable":{},"version":"7.3"},"image":"clipspace/clipspace-painted-masked-1766553866777.png [input]"},"widgets_values":["clipspace/clipspace-painted-masked-1766553866777.png [input]","image"],"color":"#223","bgcolor":"#335"}],"links":[[382,197,0,199,2,"STRING"],[394,201,0,193,3,"INT"],[396,202,1,193,8,"MASK"],[397,193,3,203,0,"IMAGE"],[398,193,1,203,1,"MODEL"],[399,193,0,203,2,"VAE"],[400,193,2,203,3,"CLIP"],[401,193,4,203,4,"CONDITIONING"],[402,203,0,199,0,"IMAGE"],[403,203,0,196,0,"*"],[404,202,0,193,9,"IMAGE"]],"groups":[],"definitions":{"subgraphs":[{"id":"102a6a5f-c689-4e9a-baf7-791a8d26b614","version":1,"state":{"lastGroupId":4,"lastNodeId":202,"lastLinkId":400,"lastRerouteId":0},"revision":0,"config":{},"name":"Base Image","inputNode":{"id":-10,"bounding":[-3947.4097183151807,-1456.903504115257,122.716796875,280]},"outputNode":{"id":-20,"bounding":[-1707.930726911814,-1508.3527853483006,128.6640625,140]},"inputs":[{"id":"ed140707-ae20-4107-bf2a-29a72d12a92f","name":"lora_name","type":"COMBO","linkIds":[359],"pos":[-3844.6929214401807,-1436.903504115257]},{"id":"8f9f876d-143d-42c0-9021-de7ed8f70bbe","name":"strength_model","type":"FLOAT","linkIds":[360],"pos":[-3844.6929214401807,-1416.903504115257]},{"id":"8eabdab4-ccd5-4052-8014-22d4c956e39e","name":"strength_clip","type":"FLOAT","linkIds":[361],"pos":[-3844.6929214401807,-1396.903504115257]},{"id":"4813b1f3-7ee9-452f-b428-6b908b30dd64","name":"noise_seed","type":"INT","linkIds":[362],"pos":[-3844.6929214401807,-1376.903504115257]},{"id":"5e46e7b8-d04b-4cab-b301-079e6b35138b","name":"sampler_name","type":"COMBO","linkIds":[378],"pos":[-3844.6929214401807,-1356.903504115257]},{"id":"54bff51f-277c-4185-9f93-29d138e484a6","name":"scheduler","type":"COMBO","linkIds":[379],"pos":[-3844.6929214401807,-1336.903504115257]},{"id":"adc9aa3f-42ae-451a-908b-677212782182","name":"steps","type":"INT","linkIds":[380],"pos":[-3844.6929214401807,-1316.903504115257]},{"id":"f675f749-a1cc-4ea1-9fce-84810e007210","name":"guidance","type":"FLOAT","linkIds":[382],"pos":[-3844.6929214401807,-1296.903504115257]},{"id":"af00bcf2-81f2-414b-88c2-1368cb49e5c8","name":"mask","type":"MASK","linkIds":[396],"pos":[-3844.6929214401807,-1276.903504115257]},{"id":"f963f0cf-65f4-4927-970f-8d48057baae7","name":"pixels","type":"IMAGE","linkIds":[398],"pos":[-3844.6929214401807,-1256.903504115257]},{"id":"562049c4-b8c7-4ed1-bbb3-38f995bc15cc","name":"denoise","type":"FLOAT","linkIds":[399],"pos":[-3844.6929214401807,-1236.903504115257]},{"id":"690db69f-6657-4cc9-9a90-a80bb6147d31","name":"text","type":"STRING","linkIds":[400],"pos":[-3844.6929214401807,-1216.903504115257]}],"outputs":[{"id":"ec7fd00b-2a10-4203-a44b-15594909045c","name":"VAE","type":"VAE","linkIds":[357,224,322,329],"localized_name":"VAE","label":"VAE","pos":[-1687.930726911814,-1488.3527853483006]},{"id":"d63143d3-1ed9-4314-8792-aca7f7ba54bc","name":"MODEL","type":"MODEL","linkIds":[277,278,330],"localized_name":"MODEL","label":"MODEL","pos":[-1687.930726911814,-1468.3527853483006]},{"id":"1df75bb4-eb53-47a8-953a-4eec6d03a1ce","name":"CLIP","type":"CLIP","linkIds":[283],"localized_name":"CLIP","label":"CLIP","pos":[-1687.930726911814,-1448.3527853483006]},{"id":"02ac74a1-e1d2-411f-8409-a1659e4a0f06","name":"IMAGE","type":"IMAGE","linkIds":[347],"localized_name":"IMAGE","label":"IMAGE","pos":[-1687.930726911814,-1428.3527853483006]},{"id":"04df92ba-17d5-4c59-9eb1-6facaa2c4397","name":"CONDITIONING","type":"CONDITIONING","linkIds":[319,334],"localized_name":"CONDITIONING","label":"CONDITIONING","pos":[-1687.930726911814,-1408.3527853483006]}],"widgets":[],"nodes":[{"id":32,"type":"VAEDecode","pos":[-2031.6687347998422,-1428.0055407998898],"size":[167.94073206223288,46],"flags":{},"order":5,"mode":0,"inputs":[{"label":"samples","localized_name":"samples","name":"samples","type":"LATENT","link":117},{"label":"vae","localized_name":"vae","name":"vae","type":"VAE","link":353}],"outputs":[{"label":"IMAGE","localized_name":"IMAGE","name":"IMAGE","type":"IMAGE","slot_index":0,"links":[347]}],"properties":{"cnr_id":"comfy-core","ver":"0.3.57","Node name for S&R":"VAEDecode","ue_properties":{"version":"7.3","widget_ue_connectable":{},"input_ue_unconnectable":{}}},"widgets_values":[],"color":"#432","bgcolor":"#653"},{"id":63,"type":"KSamplerSelect","pos":[-2750.830226237335,-1136.5816012137575],"size":[270.6041060656903,58],"flags":{},"order":7,"mode":0,"inputs":[{"label":"sampler_name","localized_name":"sampler_name","name":"sampler_name","type":"COMBO","widget":{"name":"sampler_name"},"link":378}],"outputs":[{"label":"SAMPLER","localized_name":"SAMPLER","name":"SAMPLER","type":"SAMPLER","links":[80]}],"properties":{"cnr_id":"comfy-core","ver":"0.3.57","Node name for S&R":"KSamplerSelect","ue_properties":{"version":"7.3","widget_ue_connectable":{},"input_ue_unconnectable":{}}},"widgets_values":["dpmpp_sde"],"color":"#323","bgcolor":"#535"},{"id":64,"type":"BasicScheduler","pos":[-2750.830226237335,-1044.3552948249617],"size":[270.6822790203312,108.33662599654258],"flags":{},"order":8,"mode":0,"inputs":[{"label":"model","localized_name":"model","name":"model","type":"MODEL","link":279},{"label":"scheduler","localized_name":"scheduler","name":"scheduler","type":"COMBO","widget":{"name":"scheduler"},"link":379},{"label":"steps","localized_name":"steps","name":"steps","type":"INT","widget":{"name":"steps"},"link":380},{"label":"denoise","localized_name":"denoise","name":"denoise","type":"FLOAT","widget":{"name":"denoise"},"link":399}],"outputs":[{"label":"SIGMAS","localized_name":"SIGMAS","name":"SIGMAS","type":"SIGMAS","links":[81]}],"properties":{"cnr_id":"comfy-core","ver":"0.3.57","Node name for S&R":"BasicScheduler","ue_properties":{"version":"7.3","widget_ue_connectable":{},"input_ue_unconnectable":{}}},"widgets_values":["linear_quadratic",24,0.3],"color":"#323","bgcolor":"#535"},{"id":162,"type":"LoraLoader","pos":[-3432.164407223918,-1496.0964595171665],"size":[315,126],"flags":{},"order":11,"mode":0,"inputs":[{"label":"model","localized_name":"model","name":"model","type":"MODEL","link":351},{"label":"clip","localized_name":"clip","name":"clip","type":"CLIP","link":352},{"label":"lora_name","localized_name":"lora_name","name":"lora_name","type":"COMBO","widget":{"name":"lora_name"},"link":359},{"label":"strength_model","localized_name":"strength_model","name":"strength_model","type":"FLOAT","widget":{"name":"strength_model"},"link":360},{"label":"strength_clip","localized_name":"strength_clip","name":"strength_clip","type":"FLOAT","widget":{"name":"strength_clip"},"link":361}],"outputs":[{"label":"MODEL","localized_name":"MODEL","name":"MODEL","type":"MODEL","slot_index":0,"links":[276,277,278,279,330]},{"label":"CLIP","localized_name":"CLIP","name":"CLIP","type":"CLIP","slot_index":1,"links":[281,283]}],"properties":{"cnr_id":"comfy-core","ver":"0.3.57","Node name for S&R":"LoraLoader","ue_properties":{"version":"7.3","widget_ue_connectable":{},"input_ue_unconnectable":{}}},"widgets_values":["MoiMeme/moim3m3grl-08.safetensors",0.95,0.2],"color":"#232","bgcolor":"#353"},{"id":66,"type":"BasicGuider","pos":[-2750.830226237335,-1216.45507665215],"size":[270.0590018165942,46],"flags":{},"order":10,"mode":0,"inputs":[{"label":"model","localized_name":"model","name":"model","type":"MODEL","link":276},{"label":"conditioning","localized_name":"conditioning","name":"conditioning","type":"CONDITIONING","link":106}],"outputs":[{"label":"GUIDER","localized_name":"GUIDER","name":"GUIDER","type":"GUIDER","slot_index":0,"links":[83]}],"properties":{"cnr_id":"comfy-core","ver":"0.3.57","Node name for S&R":"BasicGuider","ue_properties":{"version":"7.3","widget_ue_connectable":{},"input_ue_unconnectable":{}}},"widgets_values":[],"color":"#323","bgcolor":"#535"},{"id":65,"type":"RandomNoise","pos":[-2750.830226237335,-1423.544897192512],"size":[269.1920619153343,82],"flags":{},"order":9,"mode":0,"inputs":[{"label":"noise_seed","localized_name":"noise_seed","name":"noise_seed","type":"INT","widget":{"name":"noise_seed"},"link":362}],"outputs":[{"label":"NOISE","localized_name":"NOISE","name":"NOISE","type":"NOISE","links":[194]}],"properties":{"cnr_id":"comfy-core","ver":"0.3.57","Node name for S&R":"RandomNoise","ue_properties":{"version":"7.3","widget_ue_connectable":{},"input_ue_unconnectable":{}}},"widgets_values":[61998485097205,"fixed"],"color":"#323","bgcolor":"#535"},{"id":62,"type":"SamplerCustomAdvanced","pos":[-2371.4082561051764,-1423.6817137973228],"size":[304.5598087498504,417.9072891973424],"flags":{},"order":6,"mode":0,"inputs":[{"label":"noise","localized_name":"noise","name":"noise","type":"NOISE","link":194},{"label":"guider","localized_name":"guider","name":"guider","type":"GUIDER","link":83},{"label":"sampler","localized_name":"sampler","name":"sampler","type":"SAMPLER","link":80},{"label":"sigmas","localized_name":"sigmas","name":"sigmas","type":"SIGMAS","link":81},{"label":"latent_image","localized_name":"latent_image","name":"latent_image","type":"LATENT","link":388}],"outputs":[{"label":"output","localized_name":"output","name":"output","type":"LATENT","slot_index":0,"links":[117]},{"label":"denoised_output","localized_name":"denoised_output","name":"denoised_output","type":"LATENT","slot_index":1,"links":[]}],"properties":{"cnr_id":"comfy-core","ver":"0.3.57","Node name for S&R":"SamplerCustomAdvanced","ue_properties":{"version":"7.3","widget_ue_connectable":{},"input_ue_unconnectable":{}}},"widgets_values":[],"color":"#2a363b","bgcolor":"#3f5159"},{"id":27,"type":"FluxGuidance","pos":[-2750.830226237335,-1310.2495633508597],"size":[268.7532043457031,58],"flags":{},"order":4,"mode":0,"inputs":[{"label":"conditioning","localized_name":"conditioning","name":"conditioning","type":"CONDITIONING","link":37},{"label":"guidance","localized_name":"guidance","name":"guidance","type":"FLOAT","widget":{"name":"guidance"},"link":382}],"outputs":[{"label":"CONDITIONING","localized_name":"CONDITIONING","name":"CONDITIONING","type":"CONDITIONING","slot_index":0,"links":[106,319,334]}],"properties":{"cnr_id":"comfy-core","ver":"0.3.57","Node name for S&R":"FluxGuidance","ue_properties":{"version":"7.3","widget_ue_connectable":{},"input_ue_unconnectable":{}}},"widgets_values":[3.9],"color":"#323","bgcolor":"#535"},{"id":193,"type":"db12cd62-e6c3-4cd5-8b31-eaef3225bc2a","pos":[-3733.551332759439,-1495.0751519199289],"size":[229.96010086689057,130.25721490492197],"flags":{},"order":0,"mode":0,"inputs":[],"outputs":[{"label":"MODEL","localized_name":"MODEL","name":"MODEL","type":"MODEL","links":[351]},{"label":"CLIP","localized_name":"CLIP","name":"CLIP","type":"CLIP","links":[352]},{"label":"VAE","localized_name":"VAE","name":"VAE","type":"VAE","links":[353,357,371,384]}],"properties":{"proxyWidgets":[],"cnr_id":"comfy-core","ver":"0.3.66","ue_properties":{"widget_ue_connectable":{},"input_ue_unconnectable":{}}},"widgets_values":[],"color":"#432","bgcolor":"#653"},{"id":24,"type":"CLIPTextEncode","pos":[-3433.6445393695953,-1328.9928362721773],"size":[312.86617761887373,88],"flags":{},"order":3,"mode":0,"inputs":[{"label":"clip","localized_name":"clip","name":"clip","type":"CLIP","link":281},{"label":"text","localized_name":"text","name":"text","type":"STRING","widget":{"name":"text"},"link":400}],"outputs":[{"label":"CONDITIONING","localized_name":"CONDITIONING","name":"CONDITIONING","type":"CONDITIONING","slot_index":0,"links":[37]}],"properties":{"cnr_id":"comfy-core","ver":"0.3.57","Node name for S&R":"CLIPTextEncode","ue_properties":{"version":"7.3","widget_ue_connectable":{"text":true},"input_ue_unconnectable":{}}},"widgets_values":["This photograph depicts a beautiful woman moim3m3grl sitting on a wrought-iron bench at a dimly lit train station during a very foggy early morning. She exudes vintage charm, with light brown hair styled in a loose updo, wisps of stray strands of hair framing her face. She is dressed in a black wool day dress with a fitted bodice and square neckline; underneath, a white linen chemise with a softly gathered neckline. The chemise is a separate garment and is visible only at the collar and cuffs. This is authentic period costume detail, no modern styling. \n\nAtmosphere? The background features blurred train carriages on either side and a ominously gray sky. The station's overhead lights cast a soft glow, highlighting the woman's poised and sophisticated demeanor as she gazes into the distance. Her posture is slightly forward-leaning, with her hands resting delicately on her knees. The bench is simple and metallic, while the platform has yellow safety lines near the edges. This image evokes a sense of vintage elegance and quiet anticipation."],"color":"#232","bgcolor":"#353"},{"id":198,"type":"ImageScaleToTotalPixels","pos":[-3911.1220628108617,-820.6771931525992],"size":[267.5375677126458,82],"flags":{"collapsed":true},"order":1,"mode":4,"inputs":[{"localized_name":"image","name":"image","type":"IMAGE","link":null},{"localized_name":"upscale_method","name":"upscale_method","type":"COMBO","widget":{"name":"upscale_method"},"link":null},{"localized_name":"megapixels","name":"megapixels","type":"FLOAT","widget":{"name":"megapixels"},"link":null}],"outputs":[{"localized_name":"IMAGE","name":"IMAGE","type":"IMAGE","links":[385]}],"properties":{"cnr_id":"comfy-core","ver":"0.3.66","Node name for S&R":"ImageScaleToTotalPixels","ue_properties":{"widget_ue_connectable":{},"input_ue_unconnectable":{}}},"widgets_values":["nearest-exact",1],"color":"#223","bgcolor":"#335"},{"id":196,"type":"VAEEncode","pos":[-3879.3860297508313,-860.0640548105689],"size":[140,46],"flags":{"collapsed":true},"order":2,"mode":4,"inputs":[{"localized_name":"pixels","name":"pixels","type":"IMAGE","link":null},{"localized_name":"vae","name":"vae","type":"VAE","link":371}],"outputs":[{"localized_name":"LATENT","name":"LATENT","type":"LATENT","links":[]}],"properties":{"cnr_id":"comfy-core","ver":"0.3.66","Node name for S&R":"VAEEncode","ue_properties":{"widget_ue_connectable":{},"input_ue_unconnectable":{}}},"widgets_values":[],"color":"#432","bgcolor":"#653"},{"id":199,"type":"VAEEncodeForInpaint","pos":[-2746.3322525498497,-892.1141365669184],"size":[261.9181176757819,98],"flags":{},"order":12,"mode":0,"inputs":[{"localized_name":"pixels","name":"pixels","type":"IMAGE","link":398},{"localized_name":"vae","name":"vae","type":"VAE","link":384},{"localized_name":"mask","name":"mask","type":"MASK","link":390},{"localized_name":"grow_mask_by","name":"grow_mask_by","type":"INT","widget":{"name":"grow_mask_by"},"link":null}],"outputs":[{"localized_name":"LATENT","name":"LATENT","type":"LATENT","links":[388]}],"properties":{"cnr_id":"comfy-core","ver":"0.4.0","Node name for S&R":"VAEEncodeForInpaint","ue_properties":{"widget_ue_connectable":{},"input_ue_unconnectable":{}}},"widgets_values":[6],"color":"#432","bgcolor":"#653"},{"id":200,"type":"ImpactGaussianBlurMask","pos":[-3078.1352716561837,-849.8236255937715],"size":[219.58551530996965,82],"flags":{},"order":13,"mode":0,"inputs":[{"localized_name":"mask","name":"mask","type":"MASK","link":397},{"localized_name":"kernel_size","name":"kernel_size","type":"INT","widget":{"name":"kernel_size"},"link":null},{"localized_name":"sigma","name":"sigma","type":"FLOAT","widget":{"name":"sigma"},"link":null}],"outputs":[{"localized_name":"MASK","name":"MASK","type":"MASK","links":[390]}],"properties":{"cnr_id":"comfyui-impact-pack","ver":"8.28.0","Node name for S&R":"ImpactGaussianBlurMask","ue_properties":{"widget_ue_connectable":{},"input_ue_unconnectable":{}}},"widgets_values":[31,5],"color":"#233","bgcolor":"#355"},{"id":201,"type":"LayerMask: MaskEdgeShrink","pos":[-3477.731248812901,-861.4676303282298],"size":[281.9751953125,154],"flags":{},"order":14,"mode":0,"inputs":[{"localized_name":"mask","name":"mask","type":"MASK","link":396},{"localized_name":"invert_mask","name":"invert_mask","type":"BOOLEAN","widget":{"name":"invert_mask"},"link":null},{"localized_name":"shrink_level","name":"shrink_level","type":"INT","widget":{"name":"shrink_level"},"link":null},{"localized_name":"soft","name":"soft","type":"INT","widget":{"name":"soft"},"link":null},{"localized_name":"edge_shrink","name":"edge_shrink","type":"INT","widget":{"name":"edge_shrink"},"link":null},{"localized_name":"edge_reserve","name":"edge_reserve","type":"INT","widget":{"name":"edge_reserve"},"link":null}],"outputs":[{"localized_name":"mask","name":"mask","type":"MASK","links":[393,397]}],"properties":{"cnr_id":"comfyui_layerstyle","ver":"1.0.90","Node name for S&R":"LayerMask: MaskEdgeShrink","ue_properties":{"widget_ue_connectable":{},"input_ue_unconnectable":{}}},"widgets_values":[false,3,6,1,25],"color":"#233","bgcolor":"#355"}],"groups":[],"links":[{"id":117,"origin_id":62,"origin_slot":0,"target_id":32,"target_slot":0,"type":"LATENT"},{"id":194,"origin_id":65,"origin_slot":0,"target_id":62,"target_slot":0,"type":"NOISE"},{"id":83,"origin_id":66,"origin_slot":0,"target_id":62,"target_slot":1,"type":"GUIDER"},{"id":80,"origin_id":63,"origin_slot":0,"target_id":62,"target_slot":2,"type":"SAMPLER"},{"id":81,"origin_id":64,"origin_slot":0,"target_id":62,"target_slot":3,"type":"SIGMAS"},{"id":276,"origin_id":162,"origin_slot":0,"target_id":66,"target_slot":0,"type":"MODEL"},{"id":106,"origin_id":27,"origin_slot":0,"target_id":66,"target_slot":1,"type":"CONDITIONING"},{"id":281,"origin_id":162,"origin_slot":1,"target_id":24,"target_slot":0,"type":"CLIP"},{"id":37,"origin_id":24,"origin_slot":0,"target_id":27,"target_slot":0,"type":"CONDITIONING"},{"id":279,"origin_id":162,"origin_slot":0,"target_id":64,"target_slot":0,"type":"MODEL"},{"id":277,"origin_id":162,"origin_slot":0,"target_id":-20,"target_slot":1,"type":"MODEL"},{"id":278,"origin_id":162,"origin_slot":0,"target_id":-20,"target_slot":1,"type":"MODEL"},{"id":330,"origin_id":162,"origin_slot":0,"target_id":-20,"target_slot":1,"type":"MODEL"},{"id":283,"origin_id":162,"origin_slot":1,"target_id":-20,"target_slot":2,"type":"CLIP"},{"id":347,"origin_id":32,"origin_slot":0,"target_id":-20,"target_slot":3,"type":"*"},{"id":319,"origin_id":27,"origin_slot":0,"target_id":-20,"target_slot":4,"type":"CONDITIONING"},{"id":334,"origin_id":27,"origin_slot":0,"target_id":-20,"target_slot":4,"type":"CONDITIONING"},{"id":351,"origin_id":193,"origin_slot":0,"target_id":162,"target_slot":0,"type":"MODEL"},{"id":352,"origin_id":193,"origin_slot":1,"target_id":162,"target_slot":1,"type":"CLIP"},{"id":353,"origin_id":193,"origin_slot":2,"target_id":32,"target_slot":1,"type":"VAE"},{"id":224,"origin_id":193,"origin_slot":2,"target_id":-20,"target_slot":0,"type":"VAE"},{"id":322,"origin_id":193,"origin_slot":2,"target_id":-20,"target_slot":0,"type":"VAE"},{"id":329,"origin_id":193,"origin_slot":2,"target_id":-20,"target_slot":0,"type":"VAE"},{"id":357,"origin_id":193,"origin_slot":2,"target_id":-20,"target_slot":0,"type":"VAE"},{"id":359,"origin_id":-10,"origin_slot":0,"target_id":162,"target_slot":2,"type":"COMBO"},{"id":360,"origin_id":-10,"origin_slot":1,"target_id":162,"target_slot":3,"type":"FLOAT"},{"id":361,"origin_id":-10,"origin_slot":2,"target_id":162,"target_slot":4,"type":"FLOAT"},{"id":362,"origin_id":-10,"origin_slot":3,"target_id":65,"target_slot":0,"type":"INT"},{"id":371,"origin_id":193,"origin_slot":2,"target_id":196,"target_slot":1,"type":"VAE"},{"id":378,"origin_id":-10,"origin_slot":4,"target_id":63,"target_slot":0,"type":"COMBO"},{"id":379,"origin_id":-10,"origin_slot":5,"target_id":64,"target_slot":1,"type":"COMBO"},{"id":380,"origin_id":-10,"origin_slot":6,"target_id":64,"target_slot":2,"type":"INT"},{"id":382,"origin_id":-10,"origin_slot":7,"target_id":27,"target_slot":1,"type":"FLOAT"},{"id":384,"origin_id":193,"origin_slot":2,"target_id":199,"target_slot":1,"type":"VAE"},{"id":388,"origin_id":199,"origin_slot":0,"target_id":62,"target_slot":4,"type":"LATENT"},{"id":390,"origin_id":200,"origin_slot":0,"target_id":199,"target_slot":2,"type":"MASK"},{"id":396,"origin_id":-10,"origin_slot":8,"target_id":201,"target_slot":0,"type":"MASK"},{"id":397,"origin_id":201,"origin_slot":0,"target_id":200,"target_slot":0,"type":"MASK"},{"id":398,"origin_id":-10,"origin_slot":9,"target_id":199,"target_slot":0,"type":"IMAGE"},{"id":399,"origin_id":-10,"origin_slot":10,"target_id":64,"target_slot":3,"type":"FLOAT"},{"id":400,"origin_id":-10,"origin_slot":11,"target_id":24,"target_slot":1,"type":"STRING"}],"extra":{"ue_links":[],"links_added_by_ue":[],"workflowRendererVersion":"LG"}},{"id":"db12cd62-e6c3-4cd5-8b31-eaef3225bc2a","version":1,"state":{"lastGroupId":4,"lastNodeId":192,"lastLinkId":350,"lastRerouteId":0},"revision":0,"config":{},"name":"Models","inputNode":{"id":-10,"bounding":[-2411.3247366190144,-1697.4792585415696,120,40]},"outputNode":{"id":-20,"bounding":[-1276.2962767439196,-1708.6304755027927,120,100]},"inputs":[],"outputs":[{"id":"aaa13cbe-87c2-425f-9bf4-12f666ec9f54","name":"MODEL","type":"MODEL","linkIds":[275],"localized_name":"MODEL","label":"MODEL","pos":[-1256.2962767439196,-1688.6304755027927]},{"id":"36e5359b-d7ff-40ea-a5ce-7bdd102486ae","name":"CLIP","type":"CLIP","linkIds":[280],"localized_name":"CLIP","label":"CLIP","pos":[-1256.2962767439196,-1668.6304755027927]},{"id":"cbe7c5a8-52d5-43c5-a577-8f43fbf4f265","name":"VAE","type":"VAE","linkIds":[42,161,224,322,329],"localized_name":"VAE","label":"VAE","pos":[-1256.2962767439196,-1648.6304755027927]}],"widgets":[],"nodes":[{"id":78,"type":"DualCLIPLoader","pos":[-2231.3247366190144,-1719.1935610795642],"size":[430.6629868288594,130],"flags":{},"order":0,"mode":0,"inputs":[{"label":"clip_name1","localized_name":"clip_name1","name":"clip_name1","type":"COMBO","widget":{"name":"clip_name1"}},{"label":"clip_name2","localized_name":"clip_name2","name":"clip_name2","type":"COMBO","widget":{"name":"clip_name2"}},{"label":"type","localized_name":"type","name":"type","type":"COMBO","widget":{"name":"type"}},{"label":"device","localized_name":"device","name":"device","shape":7,"type":"COMBO","widget":{"name":"device"}}],"outputs":[{"label":"CLIP","localized_name":"CLIP","name":"CLIP","type":"CLIP","slot_index":0,"links":[280]}],"properties":{"cnr_id":"comfy-core","ver":"0.3.57","Node name for S&R":"DualCLIPLoader","ue_properties":{"version":"7.3","widget_ue_connectable":{},"input_ue_unconnectable":{}}},"widgets_values":["clip_l.safetensors","t5xxl_fp8_e4m3fn_scaled.safetensors","flux","default"],"color":"#432","bgcolor":"#653"},{"id":31,"type":"VAELoader","pos":[-2231.3247366190144,-1528.385021260922],"size":[431.5197496942583,58],"flags":{},"order":1,"mode":0,"inputs":[{"label":"vae_name","localized_name":"vae_name","name":"vae_name","type":"COMBO","widget":{"name":"vae_name"}}],"outputs":[{"label":"VAE","localized_name":"VAE","name":"VAE","type":"VAE","slot_index":0,"links":[42,161,224,322,329]}],"properties":{"cnr_id":"comfy-core","ver":"0.3.57","Node name for S&R":"VAELoader","ue_properties":{"version":"7.3","widget_ue_connectable":{},"input_ue_unconnectable":{}}},"widgets_values":["ae.sft"],"color":"#432","bgcolor":"#653"},{"id":77,"type":"UNETLoader","pos":[-2231.3247366190144,-1854.5734958222172],"size":[430.6629868288592,82],"flags":{},"order":2,"mode":0,"inputs":[{"label":"unet_name","localized_name":"unet_name","name":"unet_name","type":"COMBO","widget":{"name":"unet_name"}},{"label":"weight_dtype","localized_name":"weight_dtype","name":"weight_dtype","type":"COMBO","widget":{"name":"weight_dtype"}}],"outputs":[{"label":"MODEL","localized_name":"MODEL","name":"MODEL","type":"MODEL","slot_index":0,"links":[275]}],"properties":{"cnr_id":"comfy-core","ver":"0.3.57","Node name for S&R":"UNETLoader","ue_properties":{"version":"7.3","widget_ue_connectable":{},"input_ue_unconnectable":{}}},"widgets_values":["SRPO/SRPO-fp8_e4m3fn.safetensors","default"],"color":"#432","bgcolor":"#653"}],"groups":[],"links":[{"id":275,"origin_id":77,"origin_slot":0,"target_id":-20,"target_slot":0,"type":"MODEL"},{"id":280,"origin_id":78,"origin_slot":0,"target_id":-20,"target_slot":1,"type":"CLIP"},{"id":42,"origin_id":31,"origin_slot":0,"target_id":-20,"target_slot":2,"type":"VAE"},{"id":161,"origin_id":31,"origin_slot":0,"target_id":-20,"target_slot":2,"type":"VAE"},{"id":224,"origin_id":31,"origin_slot":0,"target_id":-20,"target_slot":2,"type":"VAE"},{"id":322,"origin_id":31,"origin_slot":0,"target_id":-20,"target_slot":2,"type":"VAE"},{"id":329,"origin_id":31,"origin_slot":0,"target_id":-20,"target_slot":2,"type":"VAE"}],"extra":{"ue_links":[],"links_added_by_ue":[],"workflowRendererVersion":"LG"}},{"id":"da172348-b4fa-49cf-94b6-10765e6232c2","version":1,"state":{"lastGroupId":4,"lastNodeId":193,"lastLinkId":364,"lastRerouteId":0},"revision":0,"config":{},"name":"UpScale Process","inputNode":{"id":-10,"bounding":[-4242.861352635998,-919.3076810026787,120,160]},"outputNode":{"id":-20,"bounding":[-1891.0966583668535,-909.3076810026787,194.013671875,120]},"inputs":[{"id":"09ddd894-cfba-4712-a291-4e10ffc061fe","name":"image","type":"IMAGE","linkIds":[343],"localized_name":"image","label":"image","pos":[-4142.861352635998,-899.3076810026787]},{"id":"725342c1-d89c-4883-a1a1-6dbe36eb14a5","name":"model","type":"MODEL","linkIds":[357,356],"localized_name":"model","label":"model","pos":[-4142.861352635998,-879.3076810026787]},{"id":"bd537d66-19e9-48bd-af3f-2c8bf1bcd2f2","name":"pixels","type":"IMAGE","linkIds":[307],"localized_name":"pixels","label":"pixels","pos":[-4142.861352635998,-859.3076810026787]},{"id":"b54d607d-05cc-4811-8f9c-052a8c2fcf6c","name":"vae","type":"VAE","linkIds":[353,352],"localized_name":"vae","label":"vae","pos":[-4142.861352635998,-839.3076810026787]},{"id":"4b02cf74-cf8a-4515-b96c-107a7a0e66f1","name":"clip","type":"CLIP","linkIds":[359],"localized_name":"clip","label":"clip","pos":[-4142.861352635998,-819.3076810026787]},{"id":"5606f5e9-dbc2-46e1-acf5-cc3c0ecec3b5","name":"text","type":"STRING","linkIds":[292],"localized_name":"text","label":"text","pos":[-4142.861352635998,-799.3076810026787]}],"outputs":[{"id":"1f0c5758-2252-4e30-a455-875243f30b6a","name":"IMAGES","type":"IMAGE","linkIds":[285],"localized_name":"IMAGES","label":"IMAGES","pos":[-1871.0966583668535,-889.3076810026787]},{"id":"4148d58d-f3ab-4c1f-a778-d8109de85e35","name":"POSITIONS","type":"LIST","linkIds":[339],"localized_name":"POSITIONS","label":"POSITIONS","pos":[-1871.0966583668535,-869.3076810026787]},{"id":"939f95a7-993f-4891-8616-7ceffd649a95","name":"CONDITIONING","type":"CONDITIONING","linkIds":[338],"localized_name":"CONDITIONING","label":"CONDITIONING","pos":[-1871.0966583668535,-849.3076810026787]},{"id":"e8e36734-79f0-4066-ad44-f78728bdfeac","name":"RECONSTRUCTED_IMAGE","type":"IMAGE","linkIds":[221,222,349],"localized_name":"RECONSTRUCTED_IMAGE","label":"RECONSTRUCTED_IMAGE","pos":[-1871.0966583668535,-829.3076810026787]}],"widgets":[],"nodes":[{"id":134,"type":"ImageUpscaleWithModel","pos":[-4057.803791787936,-1122.2956426926419],"size":[300.4600830078125,46],"flags":{},"order":14,"mode":4,"inputs":[{"label":"upscale_model","localized_name":"upscale_model","name":"upscale_model","type":"UPSCALE_MODEL","link":237},{"label":"image","localized_name":"image","name":"image","type":"IMAGE","link":343}],"outputs":[{"label":"IMAGE","localized_name":"IMAGE","name":"IMAGE","type":"IMAGE","slot_index":0,"links":[239]}],"properties":{"cnr_id":"comfy-core","ver":"0.3.57","Node name for S&R":"ImageUpscaleWithModel","ue_properties":{"version":"7.3","widget_ue_connectable":{},"input_ue_unconnectable":{}}},"widgets_values":[],"color":"#332922","bgcolor":"#593930"},{"id":133,"type":"UpscaleModelLoader","pos":[-4059.2416253510332,-1230.237819240195],"size":[315,58],"flags":{},"order":0,"mode":4,"inputs":[{"label":"model_name","localized_name":"model_name","name":"model_name","type":"COMBO","widget":{"name":"model_name"}}],"outputs":[{"label":"UPSCALE_MODEL","localized_name":"UPSCALE_MODEL","name":"UPSCALE_MODEL","type":"UPSCALE_MODEL","slot_index":0,"links":[237]}],"properties":{"cnr_id":"comfy-core","ver":"0.3.57","Node name for S&R":"UpscaleModelLoader","ue_properties":{"version":"7.3","widget_ue_connectable":{},"input_ue_unconnectable":{}}},"widgets_values":["4x_NMKD-Siax_200k.pth"],"color":"#332922","bgcolor":"#593930"},{"id":34,"type":"ImageScaleToTotalPixels","pos":[-4062.861352635998,-1029.0653993003975],"size":[315,82],"flags":{},"order":3,"mode":4,"inputs":[{"label":"image","localized_name":"image","name":"image","type":"IMAGE","link":239},{"label":"upscale_method","localized_name":"upscale_method","name":"upscale_method","type":"COMBO","widget":{"name":"upscale_method"}},{"label":"megapixels","localized_name":"megapixels","name":"megapixels","type":"FLOAT","widget":{"name":"megapixels"}}],"outputs":[{"label":"IMAGE","localized_name":"IMAGE","name":"IMAGE","type":"IMAGE","slot_index":0,"links":[363,364]}],"properties":{"cnr_id":"comfy-core","ver":"0.3.57","Node name for S&R":"ImageScaleToTotalPixels","ue_properties":{"version":"7.3","widget_ue_connectable":{},"input_ue_unconnectable":{}}},"widgets_values":["lanczos",8],"color":"#332922","bgcolor":"#593930"},{"id":68,"type":"KSamplerSelect","pos":[-2667.0801286383708,-810.6492980968495],"size":[249.97169851621902,58],"flags":{},"order":1,"mode":4,"inputs":[{"label":"sampler_name","localized_name":"sampler_name","name":"sampler_name","type":"COMBO","widget":{"name":"sampler_name"}}],"outputs":[{"label":"SAMPLER","localized_name":"SAMPLER","name":"SAMPLER","type":"SAMPLER","links":[89]}],"properties":{"cnr_id":"comfy-core","ver":"0.3.57","Node name for S&R":"KSamplerSelect","ue_properties":{"version":"7.3","widget_ue_connectable":{},"input_ue_unconnectable":{}}},"widgets_values":["euler"],"color":"#323","bgcolor":"#535"},{"id":129,"type":"VAEEncode","pos":[-2898.1850937500926,-876.3799890080783],"size":[162.70668982997745,46],"flags":{},"order":13,"mode":4,"inputs":[{"label":"pixels","localized_name":"pixels","name":"pixels","type":"IMAGE","link":307},{"label":"vae","localized_name":"vae","name":"vae","type":"VAE","link":353}],"outputs":[{"label":"LATENT","localized_name":"LATENT","name":"LATENT","type":"LATENT","slot_index":0,"links":[316]}],"properties":{"cnr_id":"comfy-core","ver":"0.3.57","Node name for S&R":"VAEEncode","ue_properties":{"version":"7.3","widget_ue_connectable":{},"input_ue_unconnectable":{}}},"widgets_values":[],"color":"#432","bgcolor":"#653"},{"id":69,"type":"BasicScheduler","pos":[-2665.8664306892647,-957.7796076110178],"size":[243.2842669880729,106],"flags":{},"order":7,"mode":4,"inputs":[{"label":"model","localized_name":"model","name":"model","type":"MODEL","link":356},{"label":"scheduler","localized_name":"scheduler","name":"scheduler","type":"COMBO","widget":{"name":"scheduler"}},{"label":"steps","localized_name":"steps","name":"steps","type":"INT","widget":{"name":"steps"}},{"label":"denoise","localized_name":"denoise","name":"denoise","type":"FLOAT","widget":{"name":"denoise"}}],"outputs":[{"label":"SIGMAS","localized_name":"SIGMAS","name":"SIGMAS","type":"SIGMAS","links":[90]}],"properties":{"cnr_id":"comfy-core","ver":"0.3.57","Node name for S&R":"BasicScheduler","ue_properties":{"version":"7.3","widget_ue_connectable":{},"input_ue_unconnectable":{}}},"widgets_values":["simple",10,0.3],"color":"#323","bgcolor":"#535"},{"id":72,"type":"BasicGuider","pos":[-2663.9522520228597,-1043.1844868772362],"size":[239.40105176985185,46],"flags":{},"order":8,"mode":4,"inputs":[{"label":"model","localized_name":"model","name":"model","type":"MODEL","link":357},{"label":"conditioning","localized_name":"conditioning","name":"conditioning","type":"CONDITIONING","link":302}],"outputs":[{"label":"GUIDER","localized_name":"GUIDER","name":"GUIDER","type":"GUIDER","slot_index":0,"links":[94,312]}],"properties":{"cnr_id":"comfy-core","ver":"0.3.57","Node name for S&R":"BasicGuider","ue_properties":{"version":"7.3","widget_ue_connectable":{},"input_ue_unconnectable":{}}},"widgets_values":[],"color":"#323","bgcolor":"#535"},{"id":73,"type":"FluxGuidance","pos":[-2661.168548306951,-1142.7565015677353],"size":[231.419071634974,58],"flags":{},"order":9,"mode":4,"inputs":[{"label":"conditioning","localized_name":"conditioning","name":"conditioning","type":"CONDITIONING","link":308},{"label":"guidance","localized_name":"guidance","name":"guidance","type":"FLOAT","widget":{"name":"guidance"}}],"outputs":[{"label":"CONDITIONING","localized_name":"CONDITIONING","name":"CONDITIONING","type":"CONDITIONING","slot_index":0,"links":[302]}],"properties":{"cnr_id":"comfy-core","ver":"0.3.57","Node name for S&R":"FluxGuidance","ue_properties":{"version":"7.3","widget_ue_connectable":{},"input_ue_unconnectable":{}}},"widgets_values":[2.5],"color":"#323","bgcolor":"#535"},{"id":70,"type":"RandomNoise","pos":[-2704.8470622688315,-579.5066765875542],"size":[274.7954406738281,85.92839050292969],"flags":{},"order":2,"mode":4,"inputs":[{"label":"noise_seed","localized_name":"noise_seed","name":"noise_seed","type":"INT","widget":{"name":"noise_seed"}}],"outputs":[{"label":"NOISE","localized_name":"NOISE","name":"NOISE","type":"NOISE","links":[87]}],"properties":{"cnr_id":"comfy-core","ver":"0.3.57","Node name for S&R":"RandomNoise","ue_properties":{"version":"7.3","widget_ue_connectable":{},"input_ue_unconnectable":{}}},"widgets_values":[316430325547060,"fixed"],"color":"#232","bgcolor":"#353"},{"id":36,"type":"TTP_Image_Tile_Batch","pos":[-3322.185465840694,-759.6191701074441],"size":[250.18014526367188,142],"flags":{},"order":4,"mode":4,"inputs":[{"label":"image","localized_name":"image","name":"image","type":"IMAGE","link":363},{"label":"tile_width","localized_name":"tile_width","name":"tile_width","type":"INT","widget":{"name":"tile_width"},"link":181},{"label":"tile_height","localized_name":"tile_height","name":"tile_height","type":"INT","widget":{"name":"tile_height"},"link":182}],"outputs":[{"label":"IMAGES","localized_name":"IMAGES","name":"IMAGES","type":"IMAGE","slot_index":0,"links":[285]},{"label":"POSITIONS","localized_name":"POSITIONS","name":"POSITIONS","type":"LIST","slot_index":1,"links":[218,339]},{"label":"ORIGINAL_SIZE","localized_name":"ORIGINAL_SIZE","name":"ORIGINAL_SIZE","type":"TUPLE","slot_index":2,"links":[219]},{"label":"GRID_SIZE","localized_name":"GRID_SIZE","name":"GRID_SIZE","type":"TUPLE","slot_index":3,"links":[220]}],"properties":{"cnr_id":"comfyui_ttp_toolset","ver":"1.0.5","Node name for S&R":"TTP_Image_Tile_Batch","ue_properties":{"version":"7.3","widget_ue_connectable":{"tile_width":true,"tile_height":true},"input_ue_unconnectable":{}}},"widgets_values":[1024,1024],"color":"#223","bgcolor":"#335"},{"id":111,"type":"TTP_Tile_image_size","pos":[-3615.6849549036665,-751.1216578313444],"size":[257.0339050292969,126],"flags":{},"order":11,"mode":4,"inputs":[{"label":"image","localized_name":"image","name":"image","type":"IMAGE","link":364},{"label":"width_factor","localized_name":"width_factor","name":"width_factor","type":"INT","widget":{"name":"width_factor"}},{"label":"height_factor","localized_name":"height_factor","name":"height_factor","type":"INT","widget":{"name":"height_factor"}},{"label":"overlap_rate","localized_name":"overlap_rate","name":"overlap_rate","type":"FLOAT","widget":{"name":"overlap_rate"}}],"outputs":[{"label":"tile_width","localized_name":"tile_width","name":"tile_width","type":"INT","slot_index":0,"links":[181]},{"label":"tile_height","localized_name":"tile_height","name":"tile_height","type":"INT","slot_index":1,"links":[182]}],"properties":{"cnr_id":"comfyui_ttp_toolset","ver":"1.0.5","Node name for S&R":"TTP_Tile_image_size","ue_properties":{"version":"7.3","widget_ue_connectable":{},"input_ue_unconnectable":{}}},"widgets_values":[2,3,0.05],"color":"#223","bgcolor":"#335"},{"id":127,"type":"TTP_Image_Assy","pos":[-3030.129882224786,-758.1147725274992],"size":[268.5188334081813,118],"flags":{},"order":12,"mode":4,"inputs":[{"label":"tiles","localized_name":"tiles","name":"tiles","type":"IMAGE","link":289},{"label":"positions","localized_name":"positions","name":"positions","type":"LIST","link":218},{"label":"original_size","localized_name":"original_size","name":"original_size","type":"TUPLE","link":219},{"label":"grid_size","localized_name":"grid_size","name":"grid_size","type":"TUPLE","link":220},{"label":"padding","localized_name":"padding","name":"padding","type":"INT","widget":{"name":"padding"}}],"outputs":[{"label":"RECONSTRUCTED_IMAGE","localized_name":"RECONSTRUCTED_IMAGE","name":"RECONSTRUCTED_IMAGE","type":"IMAGE","slot_index":0,"links":[221,222,349]}],"properties":{"cnr_id":"comfyui_ttp_toolset","ver":"1.0.5","Node name for S&R":"TTP_Image_Assy","ue_properties":{"version":"7.3","widget_ue_connectable":{},"input_ue_unconnectable":{}}},"widgets_values":[128],"color":"#223","bgcolor":"#335"},{"id":164,"type":"easy imageListToImageBatch","pos":[-3317.3772431542707,-570.6794185517022],"size":[237.4164116927666,26],"flags":{},"order":15,"mode":4,"inputs":[{"label":"images","localized_name":"images","name":"images","type":"IMAGE","link":288}],"outputs":[{"label":"IMAGE","localized_name":"IMAGE","name":"IMAGE","type":"IMAGE","slot_index":0,"links":[289]}],"properties":{"cnr_id":"comfyui-easy-use","ver":"e7320ec0c463641f702add48b87363c7a9713a1c","Node name for S&R":"easy imageListToImageBatch","ue_properties":{"version":"7.3","widget_ue_connectable":{},"input_ue_unconnectable":{}}},"widgets_values":[],"color":"#223","bgcolor":"#335"},{"id":96,"type":"VAEDecodeTiled","pos":[-3612.9258801222027,-1226.0623396135975],"size":[229.70699373928028,150],"flags":{},"order":10,"mode":4,"inputs":[{"label":"samples","localized_name":"samples","name":"samples","type":"LATENT","link":160},{"label":"vae","localized_name":"vae","name":"vae","type":"VAE","link":352},{"label":"tile_size","localized_name":"tile_size","name":"tile_size","type":"INT","widget":{"name":"tile_size"}},{"label":"overlap","localized_name":"overlap","name":"overlap","type":"INT","widget":{"name":"overlap"}},{"label":"temporal_size","localized_name":"temporal_size","name":"temporal_size","type":"INT","widget":{"name":"temporal_size"}},{"label":"temporal_overlap","localized_name":"temporal_overlap","name":"temporal_overlap","type":"INT","widget":{"name":"temporal_overlap"}}],"outputs":[{"label":"IMAGE","localized_name":"IMAGE","name":"IMAGE","type":"IMAGE","slot_index":0,"links":[288]}],"properties":{"cnr_id":"comfy-core","ver":"0.3.57","Node name for S&R":"VAEDecodeTiled","ue_properties":{"version":"7.3","widget_ue_connectable":{},"input_ue_unconnectable":{}}},"widgets_values":[1024,64,64,8],"color":"#432","bgcolor":"#653"},{"id":42,"type":"CLIPTextEncode","pos":[-4064.922101718154,-775.4010825004351],"size":[308.0869090353041,88],"flags":{},"order":5,"mode":4,"inputs":[{"label":"clip","localized_name":"clip","name":"clip","type":"CLIP","link":359},{"label":"text","localized_name":"text","name":"text","type":"STRING","widget":{"name":"text"},"link":292}],"outputs":[{"label":"CONDITIONING","localized_name":"CONDITIONING","name":"CONDITIONING","type":"CONDITIONING","slot_index":0,"links":[308,338]}],"properties":{"cnr_id":"comfy-core","ver":"0.3.57","Node name for S&R":"CLIPTextEncode","ue_properties":{"version":"7.3","widget_ue_connectable":{"text":true},"input_ue_unconnectable":{}}},"widgets_values":[""],"color":"#232","bgcolor":"#353"},{"id":67,"type":"SamplerCustomAdvanced","pos":[-2300.9675642789875,-849.8598672232155],"size":[349.8709059121338,424.0136718016407],"flags":{},"order":6,"mode":4,"inputs":[{"label":"noise","localized_name":"noise","name":"noise","type":"NOISE","link":87},{"label":"guider","localized_name":"guider","name":"guider","type":"GUIDER","link":94},{"label":"sampler","localized_name":"sampler","name":"sampler","type":"SAMPLER","link":89},{"label":"sigmas","localized_name":"sigmas","name":"sigmas","type":"SIGMAS","link":90},{"label":"latent_image","localized_name":"latent_image","name":"latent_image","type":"LATENT","link":316}],"outputs":[{"label":"output","localized_name":"output","name":"output","type":"LATENT","slot_index":0,"links":[160]},{"label":"denoised_output","localized_name":"denoised_output","name":"denoised_output","type":"LATENT"}],"properties":{"cnr_id":"comfy-core","ver":"0.3.57","Node name for S&R":"SamplerCustomAdvanced","ue_properties":{"version":"7.3","widget_ue_connectable":{},"input_ue_unconnectable":{}}},"widgets_values":[],"color":"#2a363b","bgcolor":"#3f5159"}],"groups":[],"links":[{"id":288,"origin_id":96,"origin_slot":0,"target_id":164,"target_slot":0,"type":"IMAGE"},{"id":237,"origin_id":133,"origin_slot":0,"target_id":134,"target_slot":0,"type":"UPSCALE_MODEL"},{"id":302,"origin_id":73,"origin_slot":0,"target_id":72,"target_slot":1,"type":"CONDITIONING"},{"id":308,"origin_id":42,"origin_slot":0,"target_id":73,"target_slot":0,"type":"CONDITIONING"},{"id":363,"origin_id":34,"origin_slot":0,"target_id":36,"target_slot":0,"type":"IMAGE"},{"id":181,"origin_id":111,"origin_slot":0,"target_id":36,"target_slot":1,"type":"INT"},{"id":182,"origin_id":111,"origin_slot":1,"target_id":36,"target_slot":2,"type":"INT"},{"id":239,"origin_id":134,"origin_slot":0,"target_id":34,"target_slot":0,"type":"IMAGE"},{"id":364,"origin_id":34,"origin_slot":0,"target_id":111,"target_slot":0,"type":"IMAGE"},{"id":160,"origin_id":67,"origin_slot":0,"target_id":96,"target_slot":0,"type":"LATENT"},{"id":289,"origin_id":164,"origin_slot":0,"target_id":127,"target_slot":0,"type":"IMAGE"},{"id":218,"origin_id":36,"origin_slot":1,"target_id":127,"target_slot":1,"type":"LIST"},{"id":219,"origin_id":36,"origin_slot":2,"target_id":127,"target_slot":2,"type":"TUPLE"},{"id":220,"origin_id":36,"origin_slot":3,"target_id":127,"target_slot":3,"type":"TUPLE"},{"id":87,"origin_id":70,"origin_slot":0,"target_id":67,"target_slot":0,"type":"NOISE"},{"id":94,"origin_id":72,"origin_slot":0,"target_id":67,"target_slot":1,"type":"GUIDER"},{"id":89,"origin_id":68,"origin_slot":0,"target_id":67,"target_slot":2,"type":"SAMPLER"},{"id":90,"origin_id":69,"origin_slot":0,"target_id":67,"target_slot":3,"type":"SIGMAS"},{"id":316,"origin_id":129,"origin_slot":0,"target_id":67,"target_slot":4,"type":"LATENT"},{"id":343,"origin_id":-10,"origin_slot":0,"target_id":134,"target_slot":1,"type":"IMAGE"},{"id":357,"origin_id":-10,"origin_slot":1,"target_id":72,"target_slot":0,"type":"MODEL"},{"id":356,"origin_id":-10,"origin_slot":1,"target_id":69,"target_slot":0,"type":"MODEL"},{"id":307,"origin_id":-10,"origin_slot":2,"target_id":129,"target_slot":0,"type":"IMAGE"},{"id":353,"origin_id":-10,"origin_slot":3,"target_id":129,"target_slot":1,"type":"VAE"},{"id":352,"origin_id":-10,"origin_slot":3,"target_id":96,"target_slot":1,"type":"VAE"},{"id":359,"origin_id":-10,"origin_slot":4,"target_id":42,"target_slot":0,"type":"CLIP"},{"id":292,"origin_id":-10,"origin_slot":5,"target_id":42,"target_slot":1,"type":"STRING"},{"id":285,"origin_id":36,"origin_slot":0,"target_id":-20,"target_slot":0,"type":"IMAGE"},{"id":339,"origin_id":36,"origin_slot":1,"target_id":-20,"target_slot":1,"type":"LIST"},{"id":338,"origin_id":42,"origin_slot":0,"target_id":-20,"target_slot":2,"type":"CONDITIONING"},{"id":221,"origin_id":127,"origin_slot":0,"target_id":-20,"target_slot":3,"type":"IMAGE"},{"id":222,"origin_id":127,"origin_slot":0,"target_id":-20,"target_slot":3,"type":"IMAGE"},{"id":349,"origin_id":127,"origin_slot":0,"target_id":-20,"target_slot":3,"type":"IMAGE"}],"extra":{"ue_links":[],"links_added_by_ue":[],"workflowRendererVersion":"LG"}},{"id":"8cc071ed-8bd7-424d-a9eb-bce3fd785a3d","version":1,"state":{"lastGroupId":4,"lastNodeId":194,"lastLinkId":377,"lastRerouteId":0},"revision":0,"config":{},"name":"Vision Model","inputNode":{"id":-10,"bounding":[-2626.517275632431,-214.5635900217178,120,60]},"outputNode":{"id":-20,"bounding":[-1378.1795544122847,-224.5635900217178,120,80]},"inputs":[{"id":"373d2ad8-07d7-42d6-9d78-aa7caffa016f","name":"image","type":"IMAGE","linkIds":[372],"localized_name":"image","label":"image","pos":[-2526.517275632431,-194.5635900217178]}],"outputs":[{"id":"da79f01c-db0a-4ad7-9efe-3080ef842b06","name":"STRING","type":"STRING","linkIds":[370],"localized_name":"STRING","label":"text","shape":6,"pos":[-1358.1795544122847,-204.5635900217178]},{"id":"741d7e1e-64ef-491d-adf0-af37ea1bc33a","name":"IMAGE","type":"IMAGE","linkIds":[310,367],"localized_name":"IMAGE","label":"IMAGE","shape":6,"pos":[-1358.1795544122847,-184.5635900217178]}],"widgets":[],"nodes":[{"id":58,"type":"ShowText|pysssss","pos":[-2015.7126704650484,-614.5389897229508],"size":[577.5331160527637,889.9507994024659],"flags":{},"order":2,"mode":4,"inputs":[{"label":"text","localized_name":"text","name":"text","type":"STRING","link":271}],"outputs":[{"label":"text","localized_name":"STRING","name":"STRING","shape":6,"type":"STRING","slot_index":0,"links":[370]}],"properties":{"cnr_id":"comfyui-custom-scripts","ver":"aac13aa7ce35b07d43633c3bbe654a38c00d74f5","Node name for S&R":"ShowText|pysssss","ue_properties":{"version":"7.3","widget_ue_connectable":{},"input_ue_unconnectable":{}}},"widgets_values":["The image shows a woman with long blonde hair and a white shirt standing in front of a gray background. Her face is clearly visible, with her eyes looking directly at the viewer and her lips slightly parted. Her hair is pulled back in a neat bun, and her skin is a warm, golden hue.","The image shows a woman with blonde hair and a white shirt standing in front of a wall with lights in the background. Her face is the focus of the image, with the wall providing a backdrop.","The image shows a woman wearing a black dress standing in front of a gray background. Her face is clearly visible, with her eyes looking directly at the viewer.","The image shows a woman's face with a gray background. Her features are slightly blurred, giving the image a dreamy, ethereal quality.","The image shows a gray background with a white border, and in the center of the image is a light gray color with a black border.","The image shows a gray background with a white border, and in the center of the image there is a text that reads \"Stonewall Paint Colors Sherwin Williams\". The colors of the paint range from light to dark grey, creating a subtle yet striking contrast."],"color":"#222","bgcolor":"#000"},{"id":45,"type":"Florence2Run","pos":[-2446.517275632431,-102.32789427352432],"size":[400,364],"flags":{},"order":1,"mode":4,"inputs":[{"label":"image","localized_name":"image","name":"image","type":"IMAGE","link":286},{"label":"florence2_model","localized_name":"florence2_model","name":"florence2_model","type":"FL2MODEL","link":50},{"label":"text_input","localized_name":"text_input","name":"text_input","type":"STRING","widget":{"name":"text_input"}},{"label":"task","localized_name":"task","name":"task","type":"COMBO","widget":{"name":"task"}},{"label":"fill_mask","localized_name":"fill_mask","name":"fill_mask","type":"BOOLEAN","widget":{"name":"fill_mask"}},{"label":"keep_model_loaded","localized_name":"keep_model_loaded","name":"keep_model_loaded","shape":7,"type":"BOOLEAN","widget":{"name":"keep_model_loaded"}},{"label":"max_new_tokens","localized_name":"max_new_tokens","name":"max_new_tokens","shape":7,"type":"INT","widget":{"name":"max_new_tokens"}},{"label":"num_beams","localized_name":"num_beams","name":"num_beams","shape":7,"type":"INT","widget":{"name":"num_beams"}},{"label":"do_sample","localized_name":"do_sample","name":"do_sample","shape":7,"type":"BOOLEAN","widget":{"name":"do_sample"}},{"label":"output_mask_select","localized_name":"output_mask_select","name":"output_mask_select","shape":7,"type":"STRING","widget":{"name":"output_mask_select"}},{"label":"seed","localized_name":"seed","name":"seed","shape":7,"type":"INT","widget":{"name":"seed"}}],"outputs":[{"label":"image","localized_name":"image","name":"image","type":"IMAGE"},{"label":"mask","localized_name":"mask","name":"mask","type":"MASK"},{"label":"caption","localized_name":"caption","name":"caption","type":"STRING","slot_index":2,"links":[271]},{"label":"data","localized_name":"data","name":"data","type":"JSON"}],"properties":{"cnr_id":"comfyui-florence2","ver":"1.0.5","Node name for S&R":"Florence2Run","ue_properties":{"version":"7.3","widget_ue_connectable":{},"input_ue_unconnectable":{}}},"widgets_values":["","detailed_caption",true,false,1024,3,true,"",1118877715456454,"fixed"],"color":"#222","bgcolor":"#000"},{"id":46,"type":"DownloadAndLoadFlorence2Model","pos":[-2444.6984553725574,-296.9838878132651],"size":[381.9399719238281,130],"flags":{},"order":0,"mode":4,"inputs":[{"label":"lora","localized_name":"lora","name":"lora","shape":7,"type":"PEFTLORA"},{"label":"model","localized_name":"model","name":"model","type":"COMBO","widget":{"name":"model"}},{"label":"precision","localized_name":"precision","name":"precision","type":"COMBO","widget":{"name":"precision"}},{"label":"attention","localized_name":"attention","name":"attention","type":"COMBO","widget":{"name":"attention"}},{"label":"convert_to_safetensors","localized_name":"convert_to_safetensors","name":"convert_to_safetensors","shape":7,"type":"BOOLEAN","widget":{"name":"convert_to_safetensors"}}],"outputs":[{"label":"florence2_model","localized_name":"florence2_model","name":"florence2_model","type":"FL2MODEL","slot_index":0,"links":[50]}],"properties":{"cnr_id":"comfyui-florence2","ver":"1.0.5","Node name for S&R":"DownloadAndLoadFlorence2Model","ue_properties":{"version":"7.3","widget_ue_connectable":{},"input_ue_unconnectable":{}}},"widgets_values":["microsoft/Florence-2-large","bf16","sdpa",false],"color":"#222","bgcolor":"#000"},{"id":163,"type":"easy imageBatchToImageList","pos":[-2387.908106179011,-612.8623556950982],"size":[210,26],"flags":{},"order":3,"mode":4,"inputs":[{"label":"image","localized_name":"image","name":"image","type":"IMAGE","link":372}],"outputs":[{"label":"IMAGE","localized_name":"IMAGE","name":"IMAGE","shape":6,"type":"IMAGE","slot_index":0,"links":[286,310,367]}],"properties":{"cnr_id":"comfyui-easy-use","ver":"e7320ec0c463641f702add48b87363c7a9713a1c","Node name for S&R":"easy imageBatchToImageList","ue_properties":{"version":"7.3","widget_ue_connectable":{},"input_ue_unconnectable":{}}},"widgets_values":[],"color":"#222","bgcolor":"#000"}],"groups":[],"links":[{"id":271,"origin_id":45,"origin_slot":2,"target_id":58,"target_slot":0,"type":"STRING"},{"id":286,"origin_id":163,"origin_slot":0,"target_id":45,"target_slot":0,"type":"IMAGE"},{"id":50,"origin_id":46,"origin_slot":0,"target_id":45,"target_slot":1,"type":"FL2MODEL"},{"id":372,"origin_id":-10,"origin_slot":0,"target_id":163,"target_slot":0,"type":"IMAGE"},{"id":370,"origin_id":58,"origin_slot":0,"target_id":-20,"target_slot":0,"type":"STRING"},{"id":310,"origin_id":163,"origin_slot":0,"target_id":-20,"target_slot":1,"type":"IMAGE"},{"id":367,"origin_id":163,"origin_slot":0,"target_id":-20,"target_slot":1,"type":"IMAGE"}],"extra":{"ue_links":[],"links_added_by_ue":[],"workflowRendererVersion":"LG"}},{"id":"b793d2c3-2854-43fa-8879-9fca46a28d8d","version":1,"state":{"lastGroupId":4,"lastNodeId":199,"lastLinkId":383,"lastRerouteId":0},"revision":0,"config":{},"name":"Texture Detailer","inputNode":{"id":-10,"bounding":[-4132.330402562125,-2373.94112746019,120.73046875,180]},"outputNode":{"id":-20,"bounding":[-2856.0234574278084,-2265.2371997228943,120,60]},"inputs":[{"id":"2bc8053d-144f-4d48-bf61-97db561898c9","name":"model","type":"MODEL","linkIds":[358],"localized_name":"model","label":"model","pos":[-4031.5999338121246,-2353.94112746019]},{"id":"e2fee7ff-89ad-4666-b5a4-995d9c15836b","name":"Positions","type":"LIST","linkIds":[373],"localized_name":"Positions","label":"Positions","pos":[-4031.5999338121246,-2333.94112746019]},{"id":"d948ebe3-696c-43ec-bb8d-15c6841aaa65","name":"image","type":"IMAGE","linkIds":[377],"localized_name":"image","label":"image","pos":[-4031.5999338121246,-2313.94112746019]},{"id":"fb4157c9-a76a-479e-8aa1-4cb1ad070592","name":"conditionings","type":"CONDITIONING","linkIds":[374],"localized_name":"conditionings","label":"conditionings","pos":[-4031.5999338121246,-2293.94112746019]},{"id":"c47c75fb-ab77-4f65-bbcc-34165f405c7f","name":"conditioning_to","type":"CONDITIONING","linkIds":[362,361],"localized_name":"conditioning_to","label":"conditioning_to","pos":[-4031.5999338121246,-2273.94112746019]},{"id":"7cf15641-e2ff-46ed-9519-d2c6165bc5be","name":"vae","type":"VAE","linkIds":[355,354],"localized_name":"vae","label":"vae","pos":[-4031.5999338121246,-2253.94112746019]},{"id":"fdd05479-4486-4980-b82f-30c8fce180ed","name":"noise_strength","type":"FLOAT","linkIds":[383],"pos":[-4031.5999338121246,-2233.94112746019]}],"outputs":[{"id":"6aeff311-4c0d-4e09-b036-df2808d29340","name":"IMAGE","type":"IMAGE","linkIds":[324],"localized_name":"IMAGE","label":"IMAGE","pos":[-2836.0234574278084,-2245.2371997228943]}],"widgets":[],"nodes":[{"id":184,"type":"TTP_CoordinateSplitter","pos":[-3774.372531400457,-2723.9418907727672],"size":[267,26],"flags":{},"order":6,"mode":4,"inputs":[{"label":"Positions","localized_name":"Positions","name":"Positions","type":"LIST","link":373}],"outputs":[{"label":"Coordinates","localized_name":"Coordinates","name":"Coordinates","type":"LIST","slot_index":0,"links":[337]}],"properties":{"cnr_id":"comfyui_ttp_toolset","ver":"1.0.5","Node name for S&R":"TTP_CoordinateSplitter","ue_properties":{"version":"7.3","widget_ue_connectable":{},"input_ue_unconnectable":{}}},"widgets_values":[],"color":"#233","bgcolor":"#355"},{"id":186,"type":"TTP_condsetarea_merge","pos":[-3457.346583464594,-2778.2657902781734],"size":[262,78],"flags":{},"order":8,"mode":4,"inputs":[{"label":"conditioning_batch","localized_name":"conditioning_batch","name":"conditioning_batch","type":"CONDITIONING","link":336},{"label":"coordinates","localized_name":"coordinates","name":"coordinates","type":"LIST","link":337},{"label":"strength","localized_name":"strength","name":"strength","type":"FLOAT","widget":{"name":"strength"}}],"outputs":[{"label":"CONDITIONING","localized_name":"CONDITIONING","name":"CONDITIONING","type":"CONDITIONING","slot_index":0,"links":[340]}],"properties":{"cnr_id":"comfyui_ttp_toolset","ver":"1.0.5","Node name for S&R":"TTP_condsetarea_merge","ue_properties":{"version":"7.3","widget_ue_connectable":{},"input_ue_unconnectable":{}}},"widgets_values":[1],"color":"#233","bgcolor":"#355"},{"id":177,"type":"VAEDecodeTiled","pos":[-3135.916129513006,-2302.284292419524],"size":[219.89267208519777,150],"flags":{},"order":1,"mode":4,"inputs":[{"label":"samples","localized_name":"samples","name":"samples","type":"LATENT","link":323},{"label":"vae","localized_name":"vae","name":"vae","type":"VAE","link":354},{"label":"tile_size","localized_name":"tile_size","name":"tile_size","type":"INT","widget":{"name":"tile_size"}},{"label":"overlap","localized_name":"overlap","name":"overlap","type":"INT","widget":{"name":"overlap"}},{"label":"temporal_size","localized_name":"temporal_size","name":"temporal_size","type":"INT","widget":{"name":"temporal_size"}},{"label":"temporal_overlap","localized_name":"temporal_overlap","name":"temporal_overlap","type":"INT","widget":{"name":"temporal_overlap"}}],"outputs":[{"label":"IMAGE","localized_name":"IMAGE","name":"IMAGE","type":"IMAGE","slot_index":0,"links":[324]}],"properties":{"cnr_id":"comfy-core","ver":"0.3.57","Node name for S&R":"VAEDecodeTiled","ue_properties":{"version":"7.3","widget_ue_connectable":{},"input_ue_unconnectable":{}}},"widgets_values":[1024,64,64,8],"color":"#432","bgcolor":"#653"},{"id":176,"type":"KSamplerAdvanced //Inspire","pos":[-3483.804838926235,-2300.3464589410587],"size":[300.87152099609375,730.0000000000001],"flags":{},"order":0,"mode":4,"inputs":[{"label":"model","localized_name":"model","name":"model","type":"MODEL","link":331},{"label":"positive","localized_name":"positive","name":"positive","type":"CONDITIONING","link":335},{"label":"negative","localized_name":"negative","name":"negative","type":"CONDITIONING","link":361},{"label":"latent_image","localized_name":"latent_image","name":"latent_image","type":"LATENT","link":326},{"label":"noise_opt","localized_name":"noise_opt","name":"noise_opt","shape":7,"type":"NOISE_IMAGE"},{"label":"scheduler_func_opt","localized_name":"scheduler_func_opt","name":"scheduler_func_opt","shape":7,"type":"SCHEDULER_FUNC"},{"label":"add_noise","localized_name":"add_noise","name":"add_noise","type":"BOOLEAN","widget":{"name":"add_noise"}},{"label":"noise_seed","localized_name":"noise_seed","name":"noise_seed","type":"INT","widget":{"name":"noise_seed"}},{"label":"steps","localized_name":"steps","name":"steps","type":"INT","widget":{"name":"steps"}},{"label":"cfg","localized_name":"cfg","name":"cfg","type":"FLOAT","widget":{"name":"cfg"}},{"label":"sampler_name","localized_name":"sampler_name","name":"sampler_name","type":"COMBO","widget":{"name":"sampler_name"}},{"label":"scheduler","localized_name":"scheduler","name":"scheduler","type":"COMBO","widget":{"name":"scheduler"}},{"label":"start_at_step","localized_name":"start_at_step","name":"start_at_step","type":"INT","widget":{"name":"start_at_step"}},{"label":"end_at_step","localized_name":"end_at_step","name":"end_at_step","type":"INT","widget":{"name":"end_at_step"}},{"label":"noise_mode","localized_name":"noise_mode","name":"noise_mode","type":"COMBO","widget":{"name":"noise_mode"}},{"label":"return_with_leftover_noise","localized_name":"return_with_leftover_noise","name":"return_with_leftover_noise","type":"BOOLEAN","widget":{"name":"return_with_leftover_noise"}},{"label":"batch_seed_mode","localized_name":"batch_seed_mode","name":"batch_seed_mode","type":"COMBO","widget":{"name":"batch_seed_mode"}},{"label":"variation_seed","localized_name":"variation_seed","name":"variation_seed","type":"INT","widget":{"name":"variation_seed"}},{"label":"variation_strength","localized_name":"variation_strength","name":"variation_strength","type":"FLOAT","widget":{"name":"variation_strength"}},{"label":"variation_method","localized_name":"variation_method","name":"variation_method","shape":7,"type":"COMBO","widget":{"name":"variation_method"}},{"localized_name":"internal_seed","name":"internal_seed","shape":7,"type":"INT","widget":{"name":"internal_seed"},"link":null}],"outputs":[{"label":"LATENT","localized_name":"LATENT","name":"LATENT","type":"LATENT","slot_index":0,"links":[323]}],"properties":{"cnr_id":"comfyui-inspire-pack","ver":"1.20.0","Node name for S&R":"KSamplerAdvanced //Inspire","ue_properties":{"version":"7.3","widget_ue_connectable":{},"input_ue_unconnectable":{}}},"widgets_values":[true,833195543994351,"fixed",20,1,"euler","simple",18,10000,"GPU(=A1111)",false,"incremental",0,0,"linear",0],"color":"#2a363b","bgcolor":"#3f5159"},{"id":179,"type":"InjectLatentNoise+","pos":[-3151.500041523608,-2690.4565089417138],"size":[265.5225524902344,150],"flags":{},"order":2,"mode":4,"inputs":[{"label":"latent","localized_name":"latent","name":"latent","type":"LATENT","link":328},{"label":"noise_seed","localized_name":"noise_seed","name":"noise_seed","type":"INT","widget":{"name":"noise_seed"}},{"label":"noise_strength","localized_name":"noise_strength","name":"noise_strength","type":"FLOAT","widget":{"name":"noise_strength"},"link":383},{"label":"mask","localized_name":"mask","name":"mask","shape":7,"type":"MASK"}],"outputs":[{"label":"LATENT","localized_name":"LATENT","name":"LATENT","type":"LATENT","slot_index":0,"links":[326]}],"properties":{"cnr_id":"comfyui_essentials","ver":"1.1.0","Node name for S&R":"InjectLatentNoise+","ue_properties":{"version":"7.3","widget_ue_connectable":{},"input_ue_unconnectable":{}}},"widgets_values":[0,"fixed",0.05],"color":"#233","bgcolor":"#355"},{"id":185,"type":"TTP_condtobatch","pos":[-3774.372531400457,-2798.900520126817],"size":[267.92712261080715,28.03377342224121],"flags":{},"order":7,"mode":4,"inputs":[{"label":"conditionings","localized_name":"conditionings","name":"conditionings","type":"CONDITIONING","link":374}],"outputs":[{"label":"CONDITIONING","localized_name":"CONDITIONING","name":"CONDITIONING","type":"CONDITIONING","slot_index":0,"links":[336]}],"properties":{"cnr_id":"comfyui_ttp_toolset","ver":"1.0.5","Node name for S&R":"TTP_condtobatch","ue_properties":{"version":"7.3","widget_ue_connectable":{},"input_ue_unconnectable":{}}},"widgets_values":[],"color":"#233","bgcolor":"#355"},{"id":183,"type":"ConditioningConcat","pos":[-3150.736353741304,-2777.05650543514],"size":[260.00852331039505,48.261848212461246],"flags":{},"order":5,"mode":4,"inputs":[{"label":"conditioning_to","localized_name":"conditioning_to","name":"conditioning_to","type":"CONDITIONING","link":362},{"label":"conditioning_from","localized_name":"conditioning_from","name":"conditioning_from","type":"CONDITIONING","link":340}],"outputs":[{"label":"CONDITIONING","localized_name":"CONDITIONING","name":"CONDITIONING","type":"CONDITIONING","slot_index":0,"links":[335]}],"properties":{"cnr_id":"comfy-core","ver":"0.3.57","Node name for S&R":"ConditioningConcat","ue_properties":{"version":"7.3","widget_ue_connectable":{},"input_ue_unconnectable":{}}},"widgets_values":[],"color":"#233","bgcolor":"#355"},{"id":180,"type":"VAEEncode","pos":[-3367.0220699033894,-2655.308153977776],"size":[169.4751164666293,46],"flags":{},"order":3,"mode":4,"inputs":[{"label":"pixels","localized_name":"pixels","name":"pixels","type":"IMAGE","link":350},{"label":"vae","localized_name":"vae","name":"vae","type":"VAE","link":355}],"outputs":[{"label":"LATENT","localized_name":"LATENT","name":"LATENT","type":"LATENT","slot_index":0,"links":[328]}],"properties":{"cnr_id":"comfy-core","ver":"0.3.57","Node name for S&R":"VAEEncode","ue_properties":{"version":"7.3","widget_ue_connectable":{},"input_ue_unconnectable":{}}},"widgets_values":[],"color":"#432","bgcolor":"#653"},{"id":181,"type":"TiledDiffusion","pos":[-3787.925339218075,-2299.48350467604],"size":[268.2269287109375,154],"flags":{},"order":4,"mode":4,"inputs":[{"label":"model","localized_name":"model","name":"model","type":"MODEL","link":358},{"label":"method","localized_name":"method","name":"method","type":"COMBO","widget":{"name":"method"}},{"label":"tile_width","localized_name":"tile_width","name":"tile_width","type":"INT","widget":{"name":"tile_width"}},{"label":"tile_height","localized_name":"tile_height","name":"tile_height","type":"INT","widget":{"name":"tile_height"}},{"label":"tile_overlap","localized_name":"tile_overlap","name":"tile_overlap","type":"INT","widget":{"name":"tile_overlap"}},{"label":"tile_batch_size","localized_name":"tile_batch_size","name":"tile_batch_size","type":"INT","widget":{"name":"tile_batch_size"}}],"outputs":[{"label":"MODEL","localized_name":"MODEL","name":"MODEL","type":"MODEL","slot_index":0,"links":[331]}],"properties":{"cnr_id":"ComfyUI-TiledDiffusion","ver":"a155b1bac39147381aeaa52b9be42e545626a44f","Node name for S&R":"TiledDiffusion","ue_properties":{"version":"7.3","widget_ue_connectable":{},"input_ue_unconnectable":{}}},"widgets_values":["Mixture of Diffusers",1024,1024,96,4],"color":"#223","bgcolor":"#335"},{"id":192,"type":"ImageSmartSharpen+","pos":[-3774.372531400457,-2644.1752885295446],"size":[265.993164099179,130],"flags":{"collapsed":false},"order":9,"mode":4,"inputs":[{"label":"image","localized_name":"image","name":"image","type":"IMAGE","link":377},{"localized_name":"noise_radius","name":"noise_radius","type":"INT","widget":{"name":"noise_radius"},"link":null},{"localized_name":"preserve_edges","name":"preserve_edges","type":"FLOAT","widget":{"name":"preserve_edges"},"link":null},{"localized_name":"sharpen","name":"sharpen","type":"FLOAT","widget":{"name":"sharpen"},"link":null},{"localized_name":"ratio","name":"ratio","type":"FLOAT","widget":{"name":"ratio"},"link":null}],"outputs":[{"label":"IMAGE","localized_name":"IMAGE","name":"IMAGE","type":"IMAGE","slot_index":0,"links":[350]}],"properties":{"cnr_id":"comfyui_essentials","ver":"1.1.0","Node name for S&R":"ImageSmartSharpen+","ue_properties":{"version":"7.3","widget_ue_connectable":{},"input_ue_unconnectable":{}}},"widgets_values":[4,0.75,1,0.5],"color":"#233","bgcolor":"#355"}],"groups":[],"links":[{"id":340,"origin_id":186,"origin_slot":0,"target_id":183,"target_slot":1,"type":"CONDITIONING"},{"id":336,"origin_id":185,"origin_slot":0,"target_id":186,"target_slot":0,"type":"CONDITIONING"},{"id":337,"origin_id":184,"origin_slot":0,"target_id":186,"target_slot":1,"type":"LIST"},{"id":328,"origin_id":180,"origin_slot":0,"target_id":179,"target_slot":0,"type":"LATENT"},{"id":350,"origin_id":192,"origin_slot":0,"target_id":180,"target_slot":0,"type":"IMAGE"},{"id":323,"origin_id":176,"origin_slot":0,"target_id":177,"target_slot":0,"type":"LATENT"},{"id":331,"origin_id":181,"origin_slot":0,"target_id":176,"target_slot":0,"type":"MODEL"},{"id":335,"origin_id":183,"origin_slot":0,"target_id":176,"target_slot":1,"type":"CONDITIONING"},{"id":326,"origin_id":179,"origin_slot":0,"target_id":176,"target_slot":3,"type":"LATENT"},{"id":358,"origin_id":-10,"origin_slot":0,"target_id":181,"target_slot":0,"type":"MODEL"},{"id":373,"origin_id":-10,"origin_slot":1,"target_id":184,"target_slot":0,"type":"LIST"},{"id":377,"origin_id":-10,"origin_slot":2,"target_id":192,"target_slot":0,"type":"IMAGE"},{"id":374,"origin_id":-10,"origin_slot":3,"target_id":185,"target_slot":0,"type":"CONDITIONING"},{"id":362,"origin_id":-10,"origin_slot":4,"target_id":183,"target_slot":0,"type":"CONDITIONING"},{"id":361,"origin_id":-10,"origin_slot":4,"target_id":176,"target_slot":2,"type":"CONDITIONING"},{"id":355,"origin_id":-10,"origin_slot":5,"target_id":180,"target_slot":1,"type":"VAE"},{"id":354,"origin_id":-10,"origin_slot":5,"target_id":177,"target_slot":1,"type":"VAE"},{"id":324,"origin_id":177,"origin_slot":0,"target_id":-20,"target_slot":0,"type":"IMAGE"},{"id":383,"origin_id":-10,"origin_slot":6,"target_id":179,"target_slot":2,"type":"FLOAT"}],"extra":{"ue_links":[],"links_added_by_ue":[],"workflowRendererVersion":"LG"}},{"id":"a0f2ad07-f68d-4213-a9f5-b5d97db9e0cd","version":1,"state":{"lastGroupId":0,"lastNodeId":202,"lastLinkId":396,"lastRerouteId":0},"revision":0,"config":{},"name":"UpScale-TTP","inputNode":{"id":-10,"bounding":[-2596.6241179642593,-1278.1598066534561,120.73046875,140]},"outputNode":{"id":-20,"bounding":[-2032.8981169178878,-1238.1598066534561,120,60]},"inputs":[{"id":"68a1e483-cb7a-44d5-a85c-2551782f414d","name":"image","type":"IMAGE","linkIds":[392],"localized_name":"image","label":"image","pos":[-2495.8936492142593,-1258.1598066534561]},{"id":"f1aa6e03-ac90-4456-bcc5-32ac64444dc7","name":"model","type":"MODEL","linkIds":[366,383],"localized_name":"model","label":"model","pos":[-2495.8936492142593,-1238.1598066534561]},{"id":"a11f0c15-f117-41e9-8346-512c2800477d","name":"vae","type":"VAE","linkIds":[368,388],"localized_name":"vae","label":"vae","pos":[-2495.8936492142593,-1218.1598066534561]},{"id":"b2031f81-4e0f-4866-af3a-172dc46458c6","name":"clip","type":"CLIP","linkIds":[369],"localized_name":"clip","label":"clip","pos":[-2495.8936492142593,-1198.1598066534561]},{"id":"77b9e8e7-0678-4864-927e-88b17e2bd782","name":"conditioning_to","type":"CONDITIONING","linkIds":[387],"localized_name":"conditioning_to","label":"conditioning_to","pos":[-2495.8936492142593,-1178.1598066534561]}],"outputs":[{"id":"c4b83c1e-9b0d-4a8f-9854-1f0a3079c5eb","name":"IMAGE","type":"IMAGE","linkIds":[390,391],"localized_name":"IMAGE","label":"IMAGE","pos":[-2012.8981169178878,-1218.1598066534561]}],"widgets":[],"nodes":[{"id":195,"type":"8cc071ed-8bd7-424d-a9eb-bce3fd785a3d","pos":[-2408.825152586731,-1756.8564518530802],"size":[315.927035668843,78],"flags":{},"order":1,"mode":4,"inputs":[{"label":"image","localized_name":"image","name":"image","type":"IMAGE","link":378}],"outputs":[{"label":"text","localized_name":"STRING","name":"STRING","type":"STRING","links":[379]},{"label":"IMAGE","localized_name":"IMAGE","name":"IMAGE","type":"IMAGE","links":[381]}],"properties":{"proxyWidgets":[["45","seed"]],"cnr_id":"comfy-core","ver":"0.3.66","ue_properties":{"widget_ue_connectable":{},"input_ue_unconnectable":{},"version":"7.3"}},"widgets_values":[],"color":"#222","bgcolor":"#000"},{"id":194,"type":"da172348-b4fa-49cf-94b6-10765e6232c2","pos":[-2413.3898347689105,-1629.513706114289],"size":[318.0924255732325,460.60791774835116],"flags":{},"order":0,"mode":4,"inputs":[{"label":"image","localized_name":"image","name":"image","type":"IMAGE","link":392},{"label":"model","localized_name":"model","name":"model","type":"MODEL","link":366},{"label":"pixels","localized_name":"pixels","name":"pixels","type":"IMAGE","link":381},{"label":"vae","localized_name":"vae","name":"vae","type":"VAE","link":368},{"label":"clip","localized_name":"clip","name":"clip","type":"CLIP","link":369},{"label":"text","localized_name":"text","name":"text","type":"STRING","widget":{"name":"text"},"link":379}],"outputs":[{"label":"IMAGES","localized_name":"IMAGES","name":"IMAGES","type":"IMAGE","links":[378]},{"label":"POSITIONS","localized_name":"POSITIONS","name":"POSITIONS","type":"LIST","links":[384]},{"label":"CONDITIONING","localized_name":"CONDITIONING","name":"CONDITIONING","type":"CONDITIONING","links":[386]},{"label":"RECONSTRUCTED_IMAGE","localized_name":"RECONSTRUCTED_IMAGE","name":"RECONSTRUCTED_IMAGE","type":"IMAGE","links":[385]}],"properties":{"proxyWidgets":[["-1","text"]],"cnr_id":"comfy-core","ver":"0.3.66","ue_properties":{"widget_ue_connectable":{"text":true},"input_ue_unconnectable":{},"version":"7.3"}},"widgets_values":[""],"color":"#223","bgcolor":"#335"},{"id":200,"type":"b793d2c3-2854-43fa-8879-9fca46a28d8d","pos":[-2415.8936492142593,-1115.644531972724],"size":[321.68717262231803,486.1813705188922],"flags":{},"order":2,"mode":4,"inputs":[{"label":"model","localized_name":"model","name":"model","type":"MODEL","link":383},{"label":"Positions","localized_name":"Positions","name":"Positions","type":"LIST","link":384},{"label":"image","localized_name":"image","name":"image","type":"IMAGE","link":385},{"label":"conditionings","localized_name":"conditionings","name":"conditionings","type":"CONDITIONING","link":386},{"label":"conditioning_to","localized_name":"conditioning_to","name":"conditioning_to","type":"CONDITIONING","link":387},{"label":"vae","localized_name":"vae","name":"vae","type":"VAE","link":388},{"name":"noise_strength","type":"FLOAT","widget":{"name":"noise_strength"},"link":null}],"outputs":[{"label":"IMAGE","localized_name":"IMAGE","name":"IMAGE","type":"IMAGE","links":[390,391]}],"properties":{"proxyWidgets":[["-1","noise_strength"]],"cnr_id":"comfy-core","ver":"0.3.66","ue_properties":{"widget_ue_connectable":{},"input_ue_unconnectable":{},"version":"7.3"}},"widgets_values":[0.05],"color":"#233","bgcolor":"#355"}],"groups":[],"links":[{"id":378,"origin_id":194,"origin_slot":0,"target_id":195,"target_slot":0,"type":"IMAGE"},{"id":381,"origin_id":195,"origin_slot":1,"target_id":194,"target_slot":2,"type":"IMAGE"},{"id":379,"origin_id":195,"origin_slot":0,"target_id":194,"target_slot":5,"type":"STRING"},{"id":384,"origin_id":194,"origin_slot":1,"target_id":200,"target_slot":1,"type":"LIST"},{"id":385,"origin_id":194,"origin_slot":3,"target_id":200,"target_slot":2,"type":"IMAGE"},{"id":386,"origin_id":194,"origin_slot":2,"target_id":200,"target_slot":3,"type":"CONDITIONING"},{"id":392,"origin_id":-10,"origin_slot":0,"target_id":194,"target_slot":0,"type":"IMAGE"},{"id":366,"origin_id":-10,"origin_slot":1,"target_id":194,"target_slot":1,"type":"MODEL"},{"id":383,"origin_id":-10,"origin_slot":1,"target_id":200,"target_slot":0,"type":"MODEL"},{"id":368,"origin_id":-10,"origin_slot":2,"target_id":194,"target_slot":3,"type":"VAE"},{"id":388,"origin_id":-10,"origin_slot":2,"target_id":200,"target_slot":5,"type":"VAE"},{"id":369,"origin_id":-10,"origin_slot":3,"target_id":194,"target_slot":4,"type":"CLIP"},{"id":387,"origin_id":-10,"origin_slot":4,"target_id":200,"target_slot":4,"type":"CONDITIONING"},{"id":390,"origin_id":200,"origin_slot":0,"target_id":-20,"target_slot":0,"type":"IMAGE"},{"id":391,"origin_id":200,"origin_slot":0,"target_id":-20,"target_slot":0,"type":"*"}],"extra":{"workflowRendererVersion":"LG","ue_links":[],"links_added_by_ue":[]}}]},"config":{},"extra":{"links_added_by_ue":[],"ue_links":[],"0246.VERSION":[0,0,4],"frontendVersion":"1.33.13","groupNodes":{},"VHS_latentpreview":false,"VHS_latentpreviewrate":0,"VHS_MetadataImage":true,"VHS_KeepIntermediate":true,"workflowRendererVersion":"LG","ds":{"scale":0.7513148009015777,"offset":[4166.376382306882,2098.8875045350424]}},"version":0.4}

**tool:**

All the files uploaded by the user have been fully loaded. Searching won't provide additional information.

**Emily:**

You’re *very* close — and the symptoms you’re describing (chin “chopped”, grey face area refusing to fill at low denoise, and then “weird” face when you push denoise up) almost always come from **two separate, compounding issues**:

1) **Mask geometry is too tight (and/or being shrunk in a way you don’t intend)**  
2) **You’re accidentally feeding the *painted* image (with the grey “mask paint” baked in) as the inpaint “pixels” input**, so at low denoise the model simply preserves that grey patch.

Here’s how to fix it cleanly.

---

## 1) The big gotcha: don’t inpaint from the “grey-painted” image

When you use Comfy’s built-in MaskEditor, it produces an image that *looks like* your original, but with that grey overlay painted onto it. If you then feed that into **VAE Encode (for inpaint)** as the **pixels** source, the sampler is literally starting from a face that is already greyed-out.

That explains this perfectly:

- **denoise = 0.2 → you get only the grey area** (the model doesn’t have enough freedom to “escape” the baked-in grey pixels)
- **denoise = 0.3+ → it starts to regenerate**, but you’re now fighting the mask edges and blend

✅ **Correct wiring (the “gold standard” way):**

- **LoadImage (original photo)** → use its **IMAGE** output as the “pixels” source for inpainting  
- **LoadImage (the MaskEditor export / painted image)** → use **ONLY its MASK output** (ignore its image output completely)

So you effectively do:

- `Original Image → VAE Encode (for inpaint): pixels`
- `Painted/MaskEditor Image → (mask processing chain) → VAE Encode (for inpaint): mask`

This single change usually fixes the “grey face won’t fill below X denoise” problem outright.

---

## 2) Why the chin gets “whacked off”

That’s almost always **mask boundary + mask shrink + blur + grow_mask_by** interacting.

If the mask edge crosses the jawline/chin, the model treats that boundary like a “cut here” seam. Then any of these will make it worse:

- **MaskEdgeShrink** shrinking your already-tight jaw edge upward
- **Blur** feathering the edge so the jawline becomes ambiguous
- **grow_mask_by** expanding into areas you *didn’t mean* (or expanding a bad boundary)

### The practical rule for faces
For face inpaint, the mask should include:

- forehead to just under chin
- **a little neck** (even 1–2 cm equivalent in-frame)
- a *tiny* amount into hairline (or not, depending on whether you want hair untouched)

If you mask *right along* the jaw, you’ll get jaw/chin casualties.

✅ What I’d do with your exact goal (“just face, keep hair”):

**Mask shape:**  
- include full chin + a touch of neck  
- avoid cutting along the jawline  
- stop *before* the hair mass, but allow a tiny overlap at the hairline if you’re seeing seams

---

## 3) Your blur + shrink settings: what I’d change

### GaussianBlurMask
- **kernel_size 25** / **sigma 4** is not crazy, but it can still leave a visibly “firm” transition on a 2048 image.
- For a face mask on 2048, I typically prefer:
  - **sigma ~ 6–10**  
  - kernel size scaled accordingly (e.g. 31–45)

But: **don’t blur to compensate for a mask that’s too tight**. Fix the geometry first.

### MaskEdgeShrink
This node is *very* easy to misconfigure because of invert logic and because it’s doing multiple “edge” behaviours at once.

If you’re already doing `grow_mask_by` and blur, **you often don’t need MaskEdgeShrink at all**.

If you do keep it:
- make sure **invert_mask is definitely OFF** for a normal “white=inpaint” mask workflow
- keep shrink gentle (or zero)
- don’t shrink faces aggressively — it’s more useful for clothing seams than jawlines

**My recommendation for your case (face-only):**
- Temporarily remove MaskEdgeShrink again
- Do only:
  1) *small expand/dilate* (or rely on `grow_mask_by`)
  2) blur/feather

---

## 4) grow_mask_by: your “6” is fine, but use it intentionally

For face-only:
- **grow_mask_by 6** is okay
- if the chin is getting clipped, try **grow_mask_by 10–16** *only after* you’ve ensured the mask includes the chin and a hint of neck

If you grow too much *without* adjusting mask placement, you can start affecting collar/neckline (which you’re trying to preserve).

---

## 5) Denoise + Guidance: why 0.3 looks “less natural” now

Once you stop feeding the grey-painted pixels, you should be able to get useful results at lower denoise again.

For SRPO face inpaint, I’d try:

- **denoise 0.28–0.38** (start at 0.33)
- **guidance**: drop from **3.9 → ~3.0–3.4** for face-only inpaint  
  - 3.9 can push features into that slightly “asserted” / cosmetically forced look, especially when the mask is small and the prompt/LoRA is trying to steer identity hard.

(Keeping **CFG=1** is totally fine for Flux, agreed.)

---

## 6) A quick diagnostic you can do in 10 seconds

Add a **mask preview** right before the VAE Encode for inpaint:

- If the mask edge crosses the jawline, you’ll keep seeing chin problems.
- If the mask looks inverted (background is white), everything will behave “maddeningly wrong.”

If Comfy has a simple “Preview Mask” / “Mask to Image” node, use that and confirm:

- **white = will be regenerated**
- **black = will be preserved**

---

## The exact “fix recipe” I’d try next (minimal changes)

1) **Use original image for pixels** (this is the big one)
2) Mask includes chin + a touch of neck
3) Remove MaskEdgeShrink (for now)
4) GaussianBlurMask: **sigma 8**, kernel **41** (ballpark)
5) VAE Encode (inpaint): **grow_mask_by 10**
6) Denoise: **0.33**
7) Guidance: **3.2**

If that behaves, *then* reintroduce MaskEdgeShrink only if you still see seams — and keep it very gentle.

---

## About the “different dimensions” thing

Your outputs here are all 2048×2048, but if you’re seeing variability across runs, it’s usually because one of these is in play:

- a “scale to total pixels / megapixels” node (outputs vary with aspect ratio)
- a “keep aspect ratio” resize feeding into TTP
- different source image sizes coming out of Z-Image-Turbo

If you want it locked: drop a single resize early (e.g. **Resize to 2048×2048** or whatever target) right after loading the Z-image result, *before* SRPO/TTP.

---

If you want, paste just the **mask/inpaint portion** of your graph (a screenshot is fine) showing the nodes feeding into **VAE Encode (for inpaint)** — I can tell you *exactly* which wire is currently causing the grey-face “stuck” behaviour and whether the mask polarity is flipped anywhere.

**Robyn:**

<img src="/assets/images/chat/bc98966.jpg" style="float: right; width: 420px; margin-left: 20px" />

"So you effectively do:
Original Image → VAE Encode (for inpaint): pixels
Painted/MaskEditor Image → (mask processing chain) → VAE Encode (for inpaint): mask
This single change usually fixes the “grey face won’t fill below X denoise” problem outright."
I'm pretty sure I had that wired right.

That's the mask created by MaskEditor, btw.

**Emily:**

Aha — that mask image tells us *a lot*.

### 1) Your mask is effectively “inside-out” **and** it has holes
In the mask you posted, the face region is **black**, with little “specks” inside it. In ComfyUI inpainting, you generally want:

- **white = area to be regenerated**
- **black = area to be preserved**

So if your face is black in the mask stream that hits **VAE Encode (for inpaint)**, you’re either:
- feeding an inverted mask somewhere, **or**
- one of your mask-processing nodes is inverting it (very likely MaskEdgeShrink, because you mentioned it has an `invert_mask` toggle that defaults to True).

Those little holes/specks inside the face are also a problem: they create tiny “preserve islands” inside the inpaint region, which can produce odd facial artifacts or that “not quite natural” look.

**What to do right now**
- Make the mask *explicitly correct and clean* **before** blur/feathering.

---

## 2) The most likely culprit: `invert_mask` in MaskEdgeShrink
You previously said MaskEdgeShrink has an `invert_mask` toggle defaulted to **True**.

That’s a classic trap.

### Fix:
- Set **invert_mask = False** (assuming you want to inpaint the face)

Then confirm with a preview: the face area should be **white** in the mask preview.

If you can’t change that toggle (or it’s ambiguous), add a dedicated **Invert Mask** node immediately after MaskEditor and then keep everything downstream consistent.

---

## 3) Clean the mask (remove holes) before feathering
Do this chain (simple and robust):

1) **MaskEditor mask**
2) **ThresholdMask** (0.5 is fine)  
   This binarizes the mask and kills the speckles.
3) **(Optional) Dilate/Grow** a tiny bit (2–6 px)  
   Helps include chin/jawline properly.
4) **Gaussian Blur Mask**  
   - sigma **5–6**, kernel **31–37** (your current 4/25 is okay, but 5/31 often blends faces better at 2K)
5) Into **VAE Encode (for inpaint): mask**

This one change (Threshold before Blur) often removes that “uncanny patch” feeling.

---

## 4) Guidance at **3.9** explains the “face looks asserted”
Even with CFG=1 (fine for Flux), a guidance of **3.9** can push faces into a slightly “cosmetic / over-idealised” solve, especially with inpaint masks.

For face-only identity swaps, try:
- **Guidance 3.0–3.3** (start at **3.2**)

That usually restores “natural” without losing identity.

---

## 5) About denoise: your “floor” being ~0.3 is plausible
With your current setup, **0.2 producing nothing** is consistent with either:
- the mask being inverted / grey baked / or
- the inpaint latent starting too “empty” for that low a denoise

Once the mask polarity is corrected and cleaned, you may find **0.24–0.28** starts working again. But if it doesn’t, that’s fine — just treat **0.30–0.36** as your build pass.

---

## A concrete “next run” preset
- **MaskEdgeShrink**: keep it *gentle* while you’re stabilising
  - invert_mask: **False**
  - shrink_level: **2–3** (not 6)
  - soft: **6**
- **ThresholdMask**: **0.5**
- **GaussianBlurMask**: **sigma 5**, **kernel 31**
- **VAE grow_mask_by**: **4** (you can return to 6 later)
- **Denoise**: **0.33**
- **Guidance**: **3.2**

---

## One tiny check that will settle this in 10 seconds
Right before VAE Encode, preview the mask (Mask → Image / Preview). You want to see:

- **white face**
- **black everything else**
- **no holes inside the face**

If you show me that preview (or tell me whether the face is white at that point), I can tell you immediately whether the inversion is happening in MaskEdgeShrink, in the blur node, or earlier in the MaskEditor output chain.

**Robyn:**

<img src="/assets/images/chat/89cae4c.jpg" style="float: right; width: 420px; margin-left: 20px" />

When I toggled it to True, all I got was this. Not even the face in the LoRA and th rest of the image is grey.

**Emily:**

Ah — that result (everything grey except a little “face cut-out”) is **not** “Flux failed to generate a face.”

That image is the telltale signature of **a mask being used as a *matte/composite* (or being saved from the wrong branch)** rather than being used purely as the inpaint mask. It usually happens when:

- you flip `invert_mask` on **MaskEdgeShrink** and it stops behaving like a simple invert (it can output an *edge-band / reserved region* depending on its internal logic), **or**
- your **SaveImage** is temporarily connected to a mask-preview / masked-composite node (often used for debugging), so you’re literally saving “image with mask applied,” not the inpaint result.

So: don’t treat that as “the invert didn’t work.” Treat it as “I’m not looking at the inpaint output.”

---

## The goal state (what the mask must look like right before VAE Encode)
Right before **VAE Encode (for inpaint)**, you want the mask to be:

- **WHITE = the face region you want regenerated**
- **BLACK = everything else**
- **no holes/speckles** inside the face

If you can get *that one thing*, the chin stops getting mangled and the grey “stuck” behaviour goes away.

---

## What I want you to change (and why)

### 1) Stop using MaskEdgeShrink’s `invert_mask` as your invert
That toggle is convenient, but it’s also *capricious* because that node isn’t just an invert node — it’s doing edge logic (`edge_shrink`, `edge_reserve`) and in some configurations it effectively outputs an “edge region” rather than a straightforward inverted fill. That’s how you end up with “only a small region visible.”

✅ Do this instead:

- Put a dedicated **Mask Invert** node immediately after MaskEditor.
  - (Impact Pack and/or other packs usually include something named `InvertMask`, `Mask Invert`, or similar.)

Then keep **MaskEdgeShrink invert_mask = False** forever, and only use it for shrinking/softening.

This makes the pipeline predictable.

---

### 2) Add a “clean up” step before blur
Your earlier mask screenshot had speckles/holes inside the face. That *will* make faces look odd.

Add:
- **ThresholdMask** (0.5) **before** Gaussian blur

Chain should be:

**MaskEditor → (Mask Invert if needed) → ThresholdMask(0.5) → MaskEdgeShrink(gentle) → GaussianBlurMask → VAE Encode mask**

This one change often makes the face instantly more natural.

---

### 3) Your current numbers (quick tuning)
Since you’re face-only:

- **MaskEdgeShrink shrink_level:** drop from **6 → 2 or 3**
- **GaussianBlurMask:** your **sigma 4 / kernel 25** is fine; if you still see seams, go **sigma 5 / kernel 31**
- **VAE grow_mask_by:** reduce **6 → 3 or 4** (because you’re already blurring and possibly shrinking)
- **Guidance:** 3.9 is high for face-inpaint; try **3.2–3.4** for this stage

Denoise: if your setup truly won’t “take” below 0.3, that’s okay — just make **0.30–0.36** your build range and use mask hygiene + lower guidance to keep it natural.

---

## The single best debug move (takes 10 seconds)
Right before VAE Encode, insert any node that previews the mask:

- `Mask to Image` / `Preview Mask` / `Mask Preview`

If that preview shows **white face / black background**, you’re correct.

If it shows **black face / white background**, you still need one invert (exactly one).

If it shows **only an edge ring** or weird cut-outs, MaskEdgeShrink is outputting something unintended — reduce its shrink_level and keep `invert_mask=False`, and do the invert with a dedicated invert node.

---

## One more thing: make sure you’re saving the *final decoded image*
That “grey everywhere” output is also what you get if SaveImage is connected to:
- the MaskEditor overlay image, or
- a node that applies the mask to a flat grey background for preview.

So double-check: **SaveImage must be connected to your final VAE Decode / final image output**, not any mask preview branch.

---

If you can screenshot just the small area of your graph showing:
- MaskEditor
- any invert/shrink/blur nodes
- VAE Encode (for inpaint)
- and what SaveImage is connected to

…I’ll tell you *exactly* where the “matte output” is coming from and which single wire to move.

**Robyn:**


<img src="/assets/images/chat/2799de0.jpg" style="float: right; width: 420px; margin-left: 20px" />


<img src="/assets/images/chat/5e5b63e.jpg" style="float: right; width: 420px; margin-left: 20px" />

Current setup. I've completely removed up-scale ttp all that. And that 2nd image is from the VAE Decode after the sampler.

**Emily:**

Got it. Your screenshot is *exactly* the clue we needed — and it points to **two separate problems** happening at once:

## 1) Your mask polarity is backwards at the VAE Encode (for inpainting)

That “grey everywhere with only a little face cut-out preserved” output means:

- **the sampler is inpainting the *background*** (huge area), and  
- **the face is being treated as the “do not touch / preserve” island**.

In ComfyUI inpaint land (VAE Encode for inpainting), you generally want:

- **WHITE = region to inpaint (regenerate)**  
- **BLACK = keep original**

Your result shows the opposite is happening at the moment the mask hits **VAE Encode (for inpainting)**.

### What to do (quickest)
Add a **Mask Preview** right before the VAE Encode mask input (any of: `Mask to Image`, `Preview Mask`, etc.). Then:

- If the **face is BLACK** there → you need to invert once.
- If the **face is WHITE** there → you must **remove** the InvertMask (you’re double-inverting).

Right now, given your output, I’d bet you’re **double-inverting**:
- MaskEditor is *already giving you a “face white” mask* in this particular chain, and your `InvertMask` flips it to “face black”.

So: **temporarily bypass the InvertMask node** and rerun once. If the grey-matte symptom disappears, that was it.

(Also: keep `invert_mask` inside MaskEdgeShrink = **false**, like you’ve set it. Good.)

---

## 2) You’re inpainting at **512×512**, not at full image resolution

In your graph screenshot, the Sampler preview says **512×512**.

That means your **latent_image** coming out of **VAE Encode (for inpainting)** is 512×512 — i.e. the *pixels* you’re feeding into VAE Encode are 512×512 (or have been resized somewhere upstream).

That’s why the output looks like “a tiny face floating in grey nothingness”: the model is effectively working on a little 512 canvas.

### What to do
Make sure that:

- The image feeding **VAE Encode (for inpainting) → pixels** is your **full Z-Image output** (2048×2048, 1536×1024, whatever you’re using).
- The **mask** feeding VAE Encode is the **same dimensions** as that pixels image.

If MaskEditor is producing a mask at a different size than your image, insert a scaler node:

- **ImageMaskScaleAs** (or similar)  
  - input: mask  
  - “scale as”: the original image  
  - output: mask resized to match pixels

Then VAE Encode gets a same-size pixels+mask pair, and the sampler will show your full resolution instead of 512×512.

---

## The minimal “known-good” chain for your case

**Mask path**
1. MaskEditor MASK
2. *(Only if needed)* InvertMask (exactly once)
3. ThresholdMask (0.5)
4. MaskEdgeShrink (shrink_level **2–3**, soft **6**, invert=false)
5. GaussianBlurMask (sigma **5**, kernel **31**)
6. *(If sizes differ)* ImageMaskScaleAs → match pixels image size
7. VAE Encode (for inpainting) → mask

**Pixels path**
- The **original full image** (Z-Image output) → VAE Encode (for inpainting) → pixels  
  (Do **not** feed the “grey-overlay” MaskEditor image as pixels.)

---

## Two tiny parameter tweaks once the above is fixed
- Guidance: **3.9 → 3.2–3.4** for face-only inpaint (reduces “asserted mannequin face”)
- grow_mask_by: you’re at **4** now — keep it there until the mask polarity/size are correct.

---

### What I need from you (no extra screenshots, just one fact)
In the mask preview **right before VAE Encode**, is the face **white** or **black**?

If you tell me that single detail, I can tell you definitively:
- “remove InvertMask” **or**
- “keep InvertMask and invert earlier/later”.

But even without that: based on your current result, **bypass InvertMask for one test run** and you’ll likely see the whole image return immediately (no more grey void), then we refine from there.

**Robyn:**

The face is now white, the rest black. no InvertMask in the pipeline. I'm not clear as to how I invoke the ImageMaskScaleAs - where does that go?

**Emily:**

Great — **face white / background black** is exactly the polarity you want. So now we’re down to the other (bigger) issue: **your pixels+mask are not the same size**, which is why the sampler is ending up at **512×512** and you get that “tiny face in a grey world” output.

`ImageMaskScaleAs` is the glue that forces the *mask* to match the *image* dimensions.

---

## Where `ImageMaskScaleAs` goes
Put it **right before** `VAE Encode (for inpainting)` — on the **mask** line.

### Wiring (exactly)
- `VAE Encode (for inpainting) → pixels` should come from your **full-size image** (Z-Image output), *not* the MaskEditor overlay image.
- `ImageMaskScaleAs` does this:

**Inputs**
- `mask`  ⟵ your processed mask (after Threshold / Shrink / Blur)
- `image` ⟵ the same full-size image you feed into VAE Encode’s `pixels`

**Output**
- scaled mask ⟶ `VAE Encode (for inpainting) → mask`

So the structure becomes:

`MaskEditor → (Threshold) → (MaskEdgeShrink) → (GaussianBlurMask) → ImageMaskScaleAs → VAE Encode(mask)`

and in parallel:

`Z-Image output → ImageMaskScaleAs(image input) + VAE Encode(pixels)`

That’s it.

---

## Why it fixes the “512×512 tiny face” problem
If MaskEditor (or any mask node) is operating at a different resolution than your image, ComfyUI will often “collapse” the latent to the smaller size when it tries to reconcile them. Then the sampler runs at that small latent size (you saw **512×512** in the sampler preview).

Once `ImageMaskScaleAs` forces the mask to match the image, the sampler preview should jump back to your real size (e.g. 2048×2048).

---

## Quick sanity checks after you add it
1) The Sampler preview should show your full size (not 512×512).
2) The output should no longer be a grey field with a small cutout.
3) If you still see a faint seam:
   - keep denoise ~**0.30–0.36**
   - set MaskEdgeShrink shrink_level **2–3**
   - GaussianBlurMask **sigma 5 / kernel 31**
   - grow_mask_by **4**

---

## If you can’t find `ImageMaskScaleAs`
Some installs name it slightly differently. Search for:
- “MaskScale”
- “Scale Mask”
- “Resize Mask”
- “Mask to Image Size”
- “ScaleAs”  

Any node that takes **(mask, image)** and outputs a **mask** will work.

---

If you want to be 100% sure before you run: tell me what your **full image resolution** is at the point it hits VAE Encode (for inpainting) (and whether you’re feeding it from the Z-Image output node or from something else). Then I’ll tell you the exact expected size you should see in the sampler once ImageMaskScaleAs is in place.

**Robyn:**


<img src="/assets/images/chat/2d3e025.jpg" style="float: right; width: 420px; margin-left: 20px" />

Reasonable. Not wow, but certainly within the parameters of normal. As you can see from the workflow, all the UpScale/ttp stuff is gone. No invert node. Values as stated earlier. And working. Time to tweak, maybe?

**tool:**

Make sure to include fileciteturn2file0 in your response to cite this file, or to surface it as a link.

{"id":"50f9db2c-a1c8-435f-8848-3afa79ac4dc3","revision":0,"last_node_id":203,"last_link_id":406,"nodes":[{"id":57,"type":"CR Aspect Ratio","pos":[-3315.0923366697402,-1908.3584430902677],"size":[309.8594228076063,322],"flags":{"collapsed":true},"order":0,"mode":4,"inputs":[{"label":"width","localized_name":"width","name":"width","type":"INT","widget":{"name":"width"}},{"label":"height","localized_name":"height","name":"height","type":"INT","widget":{"name":"height"}},{"label":"aspect_ratio","localized_name":"aspect_ratio","name":"aspect_ratio","type":"COMBO","widget":{"name":"aspect_ratio"}},{"label":"swap_dimensions","localized_name":"swap_dimensions","name":"swap_dimensions","type":"COMBO","widget":{"name":"swap_dimensions"}},{"label":"upscale_factor","localized_name":"upscale_factor","name":"upscale_factor","type":"FLOAT","widget":{"name":"upscale_factor"}},{"label":"prescale_factor","localized_name":"prescale_factor","name":"prescale_factor","type":"FLOAT","widget":{"name":"prescale_factor"}},{"label":"batch_size","localized_name":"batch_size","name":"batch_size","type":"INT","widget":{"name":"batch_size"}}],"outputs":[{"label":"width","localized_name":"width","name":"width","type":"INT"},{"label":"height","localized_name":"height","name":"height","type":"INT"},{"label":"upscale_factor","localized_name":"upscale_factor","name":"upscale_factor","type":"FLOAT"},{"label":"prescale_factor","localized_name":"prescale_factor","name":"prescale_factor","type":"FLOAT"},{"label":"batch_size","localized_name":"batch_size","name":"batch_size","type":"INT"},{"label":"empty_latent","localized_name":"empty_latent","name":"empty_latent","type":"LATENT","slot_index":5,"links":[]},{"label":"show_help","localized_name":"show_help","name":"show_help","type":"STRING","slot_index":6}],"properties":{"cnr_id":"ComfyUI_Comfyroll_CustomNodes","ver":"d78b780ae43fcf8c6b7c6505e6ffb4584281ceca","Node name for S&R":"CR Aspect Ratio","ue_properties":{"version":"7.3","widget_ue_connectable":{},"input_ue_unconnectable":{}}},"widgets_values":[832,1216,"SDXL - 1:1 square 1024x1024","On",1,1,1],"color":"#323","bgcolor":"#535"},{"id":198,"type":"Label (rgthree)","pos":[-3010.1979728162983,-1978.238431039258],"size":[438.466796875,70],"flags":{"allow_interaction":true},"order":1,"mode":0,"inputs":[],"outputs":[],"title":"SRPO - TTP Figure Enhancer","properties":{"fontSize":30,"fontFamily":"Arial","fontColor":"#ffffff","textAlign":"center","backgroundColor":"#113398","padding":20,"borderRadius":16,"angle":0,"ue_properties":{"version":"7.1","widget_ue_connectable":{},"input_ue_unconnectable":{}}},"color":"#fff0","bgcolor":"#fff0"},{"id":196,"type":"easy cleanGpuUsed","pos":[-2193.096752400721,-1651.1229704487034],"size":[211.55775451660156,26],"flags":{"collapsed":true},"order":8,"mode":0,"inputs":[{"localized_name":"anything","name":"anything","type":"*","link":406}],"outputs":[{"localized_name":"output","name":"output","type":"*","links":null}],"properties":{"cnr_id":"comfyui-easy-use","ver":"1.3.0","Node name for S&R":"easy cleanGpuUsed","ue_properties":{"version":"7.0.1","widget_ue_connectable":{}}},"widgets_values":[],"color":"#222","bgcolor":"#000"},{"id":201,"type":"PrimitiveInt","pos":[-2424.457078414061,-1558.0203956449557],"size":[227.58060545155513,82],"flags":{},"order":2,"mode":0,"inputs":[{"localized_name":"value","name":"value","type":"INT","widget":{"name":"value"},"link":null}],"outputs":[{"localized_name":"INT","name":"INT","type":"INT","links":[394]}],"title":"Noise Seed","properties":{"cnr_id":"comfy-core","ver":"0.3.66","Node name for S&R":"PrimitiveInt","ue_properties":{"widget_ue_connectable":{},"input_ue_unconnectable":{},"version":"7.3"}},"widgets_values":[1118877715461163,"increment"],"color":"#232","bgcolor":"#353"},{"id":197,"type":"PrimitiveNode","pos":[-2418.763751885516,-1699.1616883204706],"size":[210,58],"flags":{"collapsed":false},"order":3,"mode":0,"inputs":[],"outputs":[{"name":"STRING","type":"STRING","widget":{"name":"filename_prefix"},"links":[382]}],"title":"File Name","properties":{"Run widget replace on values":false,"ue_properties":{"version":"7.0.1","widget_ue_connectable":{}}},"widgets_values":["SRPO-Costume1780s"],"color":"#232","bgcolor":"#353"},{"id":119,"type":"Note","pos":[-2369.7801101962746,-1304.6466007064973],"size":[363.6155518609485,451.6980647725004],"flags":{"collapsed":true},"order":4,"mode":0,"inputs":[],"outputs":[],"properties":{"text":"","ue_properties":{"version":"7.3","widget_ue_connectable":{},"input_ue_unconnectable":{}}},"widgets_values":["Two women in a train station, The woman on the left is mid-plea; her dark brown hair cascades down her shoulders as wisps frames a face etched with intensity as she gestures open-handed toward the other woman—an pleading motion frozen in time. Her attire speaks to casual practicality: a soft blue blouse made from a slightly textured cotton blend hugs her shoulders, its simple design contrasting with the body-hugging black leggings, which emphasize her womanly hips and thighs. She completes the look with light blue sneakers whose laces crisscross over worn soles, hinting at daily activity.\n\nOpposite her stands a woman moimeme4 exuding vintage charm, with her light brown hair styled in a loose updo with wisps of stray strands of hair framing her face dressed entirely in a 1760s period-inspired ensemble. Her dress is long-sleeved and floor length with a wide full skirt held out by voluminous petticoats underneath the skirt, black with a square neckline exposing the tops of her modest-sized breasts, intricate lace details on the bodice and sleeves, and it features multiple layers of ruffled fabric at the hem covering her shoes so they are barely visible. The dress buttons up the front. She is clearly abashed as he looks down. A battered old leather suitcase is on the platform beside her.\n"],"color":"#432","bgcolor":"#653"},{"id":202,"type":"LoadImage","pos":[-3661.9931888820356,-1853.7820883145043],"size":[374.81145530148297,552.5036349758159],"flags":{"collapsed":false},"order":5,"mode":0,"inputs":[{"localized_name":"image","name":"image","type":"COMBO","widget":{"name":"image"},"link":null},{"localized_name":"choose file to upload","name":"upload","type":"IMAGEUPLOAD","widget":{"name":"upload"},"link":null}],"outputs":[{"localized_name":"IMAGE","name":"IMAGE","type":"IMAGE","links":[404]},{"localized_name":"MASK","name":"MASK","type":"MASK","links":[396]}],"properties":{"cnr_id":"comfy-core","ver":"0.3.66","Node name for S&R":"LoadImage","ue_properties":{"widget_ue_connectable":{},"input_ue_unconnectable":{},"version":"7.3"},"image":"clipspace/clipspace-painted-masked-1766553866777.png [input]"},"widgets_values":["clipspace/clipspace-painted-masked-1766553866777.png [input]","image"],"color":"#223","bgcolor":"#335"},{"id":199,"type":"Image Save","pos":[-2183.9223185990763,-1877.179214580262],"size":[869.9654100749628,1269.1524618530016],"flags":{},"order":7,"mode":0,"inputs":[{"localized_name":"images","name":"images","type":"IMAGE","link":405},{"localized_name":"output_path","name":"output_path","type":"STRING","widget":{"name":"output_path"},"link":null},{"localized_name":"filename_prefix","name":"filename_prefix","type":"STRING","widget":{"name":"filename_prefix"},"link":382},{"localized_name":"filename_delimiter","name":"filename_delimiter","type":"STRING","widget":{"name":"filename_delimiter"},"link":null},{"localized_name":"filename_number_padding","name":"filename_number_padding","type":"INT","widget":{"name":"filename_number_padding"},"link":null},{"localized_name":"filename_number_start","name":"filename_number_start","type":"COMBO","widget":{"name":"filename_number_start"},"link":null},{"localized_name":"extension","name":"extension","type":"COMBO","widget":{"name":"extension"},"link":null},{"localized_name":"dpi","name":"dpi","type":"INT","widget":{"name":"dpi"},"link":null},{"localized_name":"quality","name":"quality","type":"INT","widget":{"name":"quality"},"link":null},{"localized_name":"optimize_image","name":"optimize_image","type":"COMBO","widget":{"name":"optimize_image"},"link":null},{"localized_name":"lossless_webp","name":"lossless_webp","type":"COMBO","widget":{"name":"lossless_webp"},"link":null},{"localized_name":"overwrite_mode","name":"overwrite_mode","type":"COMBO","widget":{"name":"overwrite_mode"},"link":null},{"localized_name":"show_history","name":"show_history","type":"COMBO","widget":{"name":"show_history"},"link":null},{"localized_name":"show_history_by_prefix","name":"show_history_by_prefix","type":"COMBO","widget":{"name":"show_history_by_prefix"},"link":null},{"localized_name":"embed_workflow","name":"embed_workflow","type":"COMBO","widget":{"name":"embed_workflow"},"link":null},{"localized_name":"show_previews","name":"show_previews","type":"COMBO","widget":{"name":"show_previews"},"link":null}],"outputs":[{"localized_name":"images","name":"images","type":"IMAGE","links":[]},{"localized_name":"files","name":"files","type":"STRING","links":null}],"properties":{"cnr_id":"was-node-suite-comfyui","ver":"1.0.2","Node name for S&R":"Image Save","ue_properties":{"version":"7.0.1","widget_ue_connectable":{"filename_prefix":true}}},"widgets_values":["[time(%Y-%m-%d)]","SRPO-Costume1780s","_",4,"false","png",300,100,"true","false","false","false","true","true","true"],"color":"#223","bgcolor":"#335"},{"id":193,"type":"102a6a5f-c689-4e9a-baf7-791a8d26b614","pos":[-3212.969248680462,-1861.0843421630298],"size":[775.6970154083658,1230.1946709315398],"flags":{},"order":6,"mode":0,"inputs":[{"name":"lora_name","type":"COMBO","widget":{"name":"lora_name"},"link":null},{"name":"strength_model","type":"FLOAT","widget":{"name":"strength_model"},"link":null},{"name":"strength_clip","type":"FLOAT","widget":{"name":"strength_clip"},"link":null},{"name":"noise_seed","type":"INT","widget":{"name":"noise_seed"},"link":394},{"name":"sampler_name","type":"COMBO","widget":{"name":"sampler_name"},"link":null},{"name":"scheduler","type":"COMBO","widget":{"name":"scheduler"},"link":null},{"name":"steps","type":"INT","widget":{"name":"steps"},"link":null},{"name":"guidance","type":"FLOAT","widget":{"name":"guidance"},"link":null},{"name":"mask","type":"MASK","link":396},{"name":"pixels","type":"IMAGE","link":404},{"name":"denoise","type":"FLOAT","widget":{"name":"denoise"},"link":null},{"name":"text","type":"STRING","widget":{"name":"text"},"link":null}],"outputs":[{"label":"VAE","localized_name":"VAE","name":"VAE","type":"VAE","links":[]},{"label":"MODEL","localized_name":"MODEL","name":"MODEL","type":"MODEL","links":[]},{"label":"CLIP","localized_name":"CLIP","name":"CLIP","type":"CLIP","links":[]},{"label":"IMAGE","localized_name":"IMAGE","name":"IMAGE","type":"IMAGE","links":[405,406]},{"label":"CONDITIONING","localized_name":"CONDITIONING","name":"CONDITIONING","type":"CONDITIONING","links":[]}],"properties":{"proxyWidgets":[["-1","lora_name"],["-1","strength_model"],["-1","strength_clip"],["-1","noise_seed"],["-1","sampler_name"],["-1","scheduler"],["-1","steps"],["-1","guidance"],["-1","denoise"],["-1","text"],["23","text"]],"cnr_id":"comfy-core","ver":"0.3.66","ue_properties":{"widget_ue_connectable":{},"input_ue_unconnectable":{},"version":"7.3"}},"widgets_values":["MoiMeme/moim3m3grl-08.safetensors",0.95,0.2,61998485097205,"dpmpp_sde","linear_quadratic",24,2.2,0.3,"This photograph depicts a beautiful woman moim3m3grl sitting on a wrought-iron bench at a dimly lit train station during a very foggy early morning. She exudes vintage charm, with light brown hair styled in a loose updo, wisps of stray strands of hair framing her face. She is dressed in a black wool day dress with a fitted bodice and square neckline; underneath, a white linen chemise with a softly gathered neckline. The chemise is a separate garment and is visible only at the collar and cuffs. This is authentic period costume detail, no modern styling. \n\nAtmosphere? The background features blurred train carriages on either side and a ominously gray sky. The station's overhead lights cast a soft glow, highlighting the woman's poised and sophisticated demeanor as she gazes into the distance. Her posture is slightly forward-leaning, with her hands resting delicately on her knees. The bench is simple and metallic, while the platform has yellow safety lines near the edges. This image evokes a sense of vintage elegance and quiet anticipation."],"color":"#232","bgcolor":"#353"}],"links":[[382,197,0,199,2,"STRING"],[394,201,0,193,3,"INT"],[396,202,1,193,8,"MASK"],[404,202,0,193,9,"IMAGE"],[405,193,3,199,0,"IMAGE"],[406,193,3,196,0,"*"]],"groups":[],"definitions":{"subgraphs":[{"id":"102a6a5f-c689-4e9a-baf7-791a8d26b614","version":1,"state":{"lastGroupId":4,"lastNodeId":206,"lastLinkId":422,"lastRerouteId":0},"revision":0,"config":{},"name":"Base Image","inputNode":{"id":-10,"bounding":[-3947.4097183151807,-1456.903504115257,122.716796875,280]},"outputNode":{"id":-20,"bounding":[-1707.930726911814,-1508.3527853483006,128.6640625,140]},"inputs":[{"id":"ed140707-ae20-4107-bf2a-29a72d12a92f","name":"lora_name","type":"COMBO","linkIds":[359],"pos":[-3844.6929214401807,-1436.903504115257]},{"id":"8f9f876d-143d-42c0-9021-de7ed8f70bbe","name":"strength_model","type":"FLOAT","linkIds":[360],"pos":[-3844.6929214401807,-1416.903504115257]},{"id":"8eabdab4-ccd5-4052-8014-22d4c956e39e","name":"strength_clip","type":"FLOAT","linkIds":[361],"pos":[-3844.6929214401807,-1396.903504115257]},{"id":"4813b1f3-7ee9-452f-b428-6b908b30dd64","name":"noise_seed","type":"INT","linkIds":[362],"pos":[-3844.6929214401807,-1376.903504115257]},{"id":"5e46e7b8-d04b-4cab-b301-079e6b35138b","name":"sampler_name","type":"COMBO","linkIds":[378],"pos":[-3844.6929214401807,-1356.903504115257]},{"id":"54bff51f-277c-4185-9f93-29d138e484a6","name":"scheduler","type":"COMBO","linkIds":[379],"pos":[-3844.6929214401807,-1336.903504115257]},{"id":"adc9aa3f-42ae-451a-908b-677212782182","name":"steps","type":"INT","linkIds":[380],"pos":[-3844.6929214401807,-1316.903504115257]},{"id":"f675f749-a1cc-4ea1-9fce-84810e007210","name":"guidance","type":"FLOAT","linkIds":[382],"pos":[-3844.6929214401807,-1296.903504115257]},{"id":"af00bcf2-81f2-414b-88c2-1368cb49e5c8","name":"mask","type":"MASK","linkIds":[418],"pos":[-3844.6929214401807,-1276.903504115257]},{"id":"f963f0cf-65f4-4927-970f-8d48057baae7","name":"pixels","type":"IMAGE","linkIds":[410,422],"pos":[-3844.6929214401807,-1256.903504115257]},{"id":"562049c4-b8c7-4ed1-bbb3-38f995bc15cc","name":"denoise","type":"FLOAT","linkIds":[399],"pos":[-3844.6929214401807,-1236.903504115257]},{"id":"690db69f-6657-4cc9-9a90-a80bb6147d31","name":"text","type":"STRING","linkIds":[400],"pos":[-3844.6929214401807,-1216.903504115257]}],"outputs":[{"id":"ec7fd00b-2a10-4203-a44b-15594909045c","name":"VAE","type":"VAE","linkIds":[357,224,322,329],"localized_name":"VAE","label":"VAE","pos":[-1687.930726911814,-1488.3527853483006]},{"id":"d63143d3-1ed9-4314-8792-aca7f7ba54bc","name":"MODEL","type":"MODEL","linkIds":[277,278,330],"localized_name":"MODEL","label":"MODEL","pos":[-1687.930726911814,-1468.3527853483006]},{"id":"1df75bb4-eb53-47a8-953a-4eec6d03a1ce","name":"CLIP","type":"CLIP","linkIds":[283],"localized_name":"CLIP","label":"CLIP","pos":[-1687.930726911814,-1448.3527853483006]},{"id":"02ac74a1-e1d2-411f-8409-a1659e4a0f06","name":"IMAGE","type":"IMAGE","linkIds":[347],"localized_name":"IMAGE","label":"IMAGE","pos":[-1687.930726911814,-1428.3527853483006]},{"id":"04df92ba-17d5-4c59-9eb1-6facaa2c4397","name":"CONDITIONING","type":"CONDITIONING","linkIds":[319,334],"localized_name":"CONDITIONING","label":"CONDITIONING","pos":[-1687.930726911814,-1408.3527853483006]}],"widgets":[],"nodes":[{"id":63,"type":"KSamplerSelect","pos":[-2750.830226237335,-1136.5816012137575],"size":[270.6041060656903,58],"flags":{},"order":5,"mode":0,"inputs":[{"label":"sampler_name","localized_name":"sampler_name","name":"sampler_name","type":"COMBO","widget":{"name":"sampler_name"},"link":378}],"outputs":[{"label":"SAMPLER","localized_name":"SAMPLER","name":"SAMPLER","type":"SAMPLER","links":[80]}],"properties":{"cnr_id":"comfy-core","ver":"0.3.57","Node name for S&R":"KSamplerSelect","ue_properties":{"version":"7.3","widget_ue_connectable":{},"input_ue_unconnectable":{}}},"widgets_values":["dpmpp_sde"],"color":"#323","bgcolor":"#535"},{"id":64,"type":"BasicScheduler","pos":[-2750.830226237335,-1044.3552948249617],"size":[270.6822790203312,108.33662599654258],"flags":{},"order":6,"mode":0,"inputs":[{"label":"model","localized_name":"model","name":"model","type":"MODEL","link":279},{"label":"scheduler","localized_name":"scheduler","name":"scheduler","type":"COMBO","widget":{"name":"scheduler"},"link":379},{"label":"steps","localized_name":"steps","name":"steps","type":"INT","widget":{"name":"steps"},"link":380},{"label":"denoise","localized_name":"denoise","name":"denoise","type":"FLOAT","widget":{"name":"denoise"},"link":399}],"outputs":[{"label":"SIGMAS","localized_name":"SIGMAS","name":"SIGMAS","type":"SIGMAS","links":[81]}],"properties":{"cnr_id":"comfy-core","ver":"0.3.57","Node name for S&R":"BasicScheduler","ue_properties":{"version":"7.3","widget_ue_connectable":{},"input_ue_unconnectable":{}}},"widgets_values":["linear_quadratic",24,0.3],"color":"#323","bgcolor":"#535"},{"id":162,"type":"LoraLoader","pos":[-3432.164407223918,-1496.0964595171665],"size":[315,126],"flags":{},"order":9,"mode":0,"inputs":[{"label":"model","localized_name":"model","name":"model","type":"MODEL","link":351},{"label":"clip","localized_name":"clip","name":"clip","type":"CLIP","link":352},{"label":"lora_name","localized_name":"lora_name","name":"lora_name","type":"COMBO","widget":{"name":"lora_name"},"link":359},{"label":"strength_model","localized_name":"strength_model","name":"strength_model","type":"FLOAT","widget":{"name":"strength_model"},"link":360},{"label":"strength_clip","localized_name":"strength_clip","name":"strength_clip","type":"FLOAT","widget":{"name":"strength_clip"},"link":361}],"outputs":[{"label":"MODEL","localized_name":"MODEL","name":"MODEL","type":"MODEL","slot_index":0,"links":[276,277,278,279,330]},{"label":"CLIP","localized_name":"CLIP","name":"CLIP","type":"CLIP","slot_index":1,"links":[281,283]}],"properties":{"cnr_id":"comfy-core","ver":"0.3.57","Node name for S&R":"LoraLoader","ue_properties":{"version":"7.3","widget_ue_connectable":{},"input_ue_unconnectable":{}}},"widgets_values":["MoiMeme/moim3m3grl-08.safetensors",0.95,0.2],"color":"#232","bgcolor":"#353"},{"id":66,"type":"BasicGuider","pos":[-2750.830226237335,-1216.45507665215],"size":[270.0590018165942,46],"flags":{},"order":8,"mode":0,"inputs":[{"label":"model","localized_name":"model","name":"model","type":"MODEL","link":276},{"label":"conditioning","localized_name":"conditioning","name":"conditioning","type":"CONDITIONING","link":106}],"outputs":[{"label":"GUIDER","localized_name":"GUIDER","name":"GUIDER","type":"GUIDER","slot_index":0,"links":[83]}],"properties":{"cnr_id":"comfy-core","ver":"0.3.57","Node name for S&R":"BasicGuider","ue_properties":{"version":"7.3","widget_ue_connectable":{},"input_ue_unconnectable":{}}},"widgets_values":[],"color":"#323","bgcolor":"#535"},{"id":65,"type":"RandomNoise","pos":[-2750.830226237335,-1423.544897192512],"size":[269.1920619153343,82],"flags":{},"order":7,"mode":0,"inputs":[{"label":"noise_seed","localized_name":"noise_seed","name":"noise_seed","type":"INT","widget":{"name":"noise_seed"},"link":362}],"outputs":[{"label":"NOISE","localized_name":"NOISE","name":"NOISE","type":"NOISE","links":[194]}],"properties":{"cnr_id":"comfy-core","ver":"0.3.57","Node name for S&R":"RandomNoise","ue_properties":{"version":"7.3","widget_ue_connectable":{},"input_ue_unconnectable":{}}},"widgets_values":[61998485097205,"fixed"],"color":"#323","bgcolor":"#535"},{"id":62,"type":"SamplerCustomAdvanced","pos":[-2371.4082561051764,-1423.6817137973228],"size":[304.5598087498504,417.9072891973424],"flags":{},"order":4,"mode":0,"inputs":[{"label":"noise","localized_name":"noise","name":"noise","type":"NOISE","link":194},{"label":"guider","localized_name":"guider","name":"guider","type":"GUIDER","link":83},{"label":"sampler","localized_name":"sampler","name":"sampler","type":"SAMPLER","link":80},{"label":"sigmas","localized_name":"sigmas","name":"sigmas","type":"SIGMAS","link":81},{"label":"latent_image","localized_name":"latent_image","name":"latent_image","type":"LATENT","link":388}],"outputs":[{"label":"output","localized_name":"output","name":"output","type":"LATENT","slot_index":0,"links":[117]},{"label":"denoised_output","localized_name":"denoised_output","name":"denoised_output","type":"LATENT","slot_index":1,"links":[]}],"properties":{"cnr_id":"comfy-core","ver":"0.3.57","Node name for S&R":"SamplerCustomAdvanced","ue_properties":{"version":"7.3","widget_ue_connectable":{},"input_ue_unconnectable":{}}},"widgets_values":[],"color":"#2a363b","bgcolor":"#3f5159"},{"id":27,"type":"FluxGuidance","pos":[-2750.830226237335,-1310.2495633508597],"size":[268.7532043457031,58],"flags":{},"order":2,"mode":0,"inputs":[{"label":"conditioning","localized_name":"conditioning","name":"conditioning","type":"CONDITIONING","link":37},{"label":"guidance","localized_name":"guidance","name":"guidance","type":"FLOAT","widget":{"name":"guidance"},"link":382}],"outputs":[{"label":"CONDITIONING","localized_name":"CONDITIONING","name":"CONDITIONING","type":"CONDITIONING","slot_index":0,"links":[106,319,334]}],"properties":{"cnr_id":"comfy-core","ver":"0.3.57","Node name for S&R":"FluxGuidance","ue_properties":{"version":"7.3","widget_ue_connectable":{},"input_ue_unconnectable":{}}},"widgets_values":[2.2],"color":"#323","bgcolor":"#535"},{"id":193,"type":"db12cd62-e6c3-4cd5-8b31-eaef3225bc2a","pos":[-3733.551332759439,-1495.0751519199289],"size":[229.96010086689057,130.25721490492197],"flags":{},"order":0,"mode":0,"inputs":[],"outputs":[{"label":"MODEL","localized_name":"MODEL","name":"MODEL","type":"MODEL","links":[351]},{"label":"CLIP","localized_name":"CLIP","name":"CLIP","type":"CLIP","links":[352]},{"label":"VAE","localized_name":"VAE","name":"VAE","type":"VAE","links":[353,357,384]}],"properties":{"proxyWidgets":[],"cnr_id":"comfy-core","ver":"0.3.66","ue_properties":{"widget_ue_connectable":{},"input_ue_unconnectable":{}}},"widgets_values":[],"color":"#432","bgcolor":"#653"},{"id":24,"type":"CLIPTextEncode","pos":[-3433.6445393695953,-1328.9928362721773],"size":[312.86617761887373,88],"flags":{},"order":1,"mode":0,"inputs":[{"label":"clip","localized_name":"clip","name":"clip","type":"CLIP","link":281},{"label":"text","localized_name":"text","name":"text","type":"STRING","widget":{"name":"text"},"link":400}],"outputs":[{"label":"CONDITIONING","localized_name":"CONDITIONING","name":"CONDITIONING","type":"CONDITIONING","slot_index":0,"links":[37]}],"properties":{"cnr_id":"comfy-core","ver":"0.3.57","Node name for S&R":"CLIPTextEncode","ue_properties":{"version":"7.3","widget_ue_connectable":{"text":true},"input_ue_unconnectable":{}}},"widgets_values":["This photograph depicts a beautiful woman moim3m3grl sitting on a wrought-iron bench at a dimly lit train station during a very foggy early morning. She exudes vintage charm, with light brown hair styled in a loose updo, wisps of stray strands of hair framing her face. She is dressed in a black wool day dress with a fitted bodice and square neckline; underneath, a white linen chemise with a softly gathered neckline. The chemise is a separate garment and is visible only at the collar and cuffs. This is authentic period costume detail, no modern styling. \n\nAtmosphere? The background features blurred train carriages on either side and a ominously gray sky. The station's overhead lights cast a soft glow, highlighting the woman's poised and sophisticated demeanor as she gazes into the distance. Her posture is slightly forward-leaning, with her hands resting delicately on her knees. The bench is simple and metallic, while the platform has yellow safety lines near the edges. This image evokes a sense of vintage elegance and quiet anticipation."],"color":"#232","bgcolor":"#353"},{"id":32,"type":"VAEDecode","pos":[-1954.2287347998422,-1423.1655407998899],"size":[167.94073206223288,46],"flags":{},"order":3,"mode":0,"inputs":[{"label":"samples","localized_name":"samples","name":"samples","type":"LATENT","link":117},{"label":"vae","localized_name":"vae","name":"vae","type":"VAE","link":353}],"outputs":[{"label":"IMAGE","localized_name":"IMAGE","name":"IMAGE","type":"IMAGE","slot_index":0,"links":[347]}],"properties":{"cnr_id":"comfy-core","ver":"0.3.57","Node name for S&R":"VAEDecode","ue_properties":{"version":"7.3","widget_ue_connectable":{},"input_ue_unconnectable":{}}},"widgets_values":[],"color":"#432","bgcolor":"#653"},{"id":205,"type":"MaskPreview+","pos":[-2370.108997720793,-694.8712133332556],"size":[283.10177062988294,352.6999999999998],"flags":{},"order":14,"mode":0,"inputs":[{"localized_name":"mask","name":"mask","type":"MASK","link":406}],"outputs":[],"properties":{"cnr_id":"comfyui_essentials","ver":"1.1.0","Node name for S&R":"MaskPreview+","ue_properties":{"widget_ue_connectable":{},"input_ue_unconnectable":{}}},"widgets_values":[]},{"id":201,"type":"LayerMask: MaskEdgeShrink","pos":[-3794.7512488129,-885.66763032823],"size":[281.9751953125,154],"flags":{},"order":12,"mode":0,"inputs":[{"localized_name":"mask","name":"mask","type":"MASK","link":404},{"localized_name":"invert_mask","name":"invert_mask","type":"BOOLEAN","widget":{"name":"invert_mask"},"link":null},{"localized_name":"shrink_level","name":"shrink_level","type":"INT","widget":{"name":"shrink_level"},"link":null},{"localized_name":"soft","name":"soft","type":"INT","widget":{"name":"soft"},"link":null},{"localized_name":"edge_shrink","name":"edge_shrink","type":"INT","widget":{"name":"edge_shrink"},"link":null},{"localized_name":"edge_reserve","name":"edge_reserve","type":"INT","widget":{"name":"edge_reserve"},"link":null}],"outputs":[{"localized_name":"mask","name":"mask","type":"MASK","links":[393,405]}],"properties":{"cnr_id":"comfyui_layerstyle","ver":"1.0.90","Node name for S&R":"LayerMask: MaskEdgeShrink","ue_properties":{"widget_ue_connectable":{},"input_ue_unconnectable":{}}},"widgets_values":[false,3,6,1,25],"color":"#233","bgcolor":"#355"},{"id":204,"type":"ThresholdMask","pos":[-3762.4157997837788,-1021.5712133332551],"size":[210,58],"flags":{},"order":13,"mode":0,"inputs":[{"localized_name":"mask","name":"mask","type":"MASK","link":418},{"localized_name":"value","name":"value","type":"FLOAT","widget":{"name":"value"},"link":null}],"outputs":[{"localized_name":"MASK","name":"MASK","type":"MASK","links":[404]}],"properties":{"cnr_id":"comfy-core","ver":"0.4.0","Node name for S&R":"ThresholdMask","ue_properties":{"widget_ue_connectable":{},"input_ue_unconnectable":{}}},"widgets_values":[0.5],"color":"#233","bgcolor":"#355"},{"id":200,"type":"ImpactGaussianBlurMask","pos":[-3456.462110645443,-886.930316877951],"size":[219.58551530996965,82],"flags":{},"order":11,"mode":0,"inputs":[{"localized_name":"mask","name":"mask","type":"MASK","link":405},{"localized_name":"kernel_size","name":"kernel_size","type":"INT","widget":{"name":"kernel_size"},"link":null},{"localized_name":"sigma","name":"sigma","type":"FLOAT","widget":{"name":"sigma"},"link":null}],"outputs":[{"localized_name":"MASK","name":"MASK","type":"MASK","links":[406,419]}],"properties":{"cnr_id":"comfyui-impact-pack","ver":"8.28.0","Node name for S&R":"ImpactGaussianBlurMask","ue_properties":{"widget_ue_connectable":{},"input_ue_unconnectable":{}}},"widgets_values":[25,4],"color":"#233","bgcolor":"#355"},{"id":199,"type":"VAEEncodeForInpaint","pos":[-2739.8788699814913,-888.8874452827388],"size":[261.9181176757819,98],"flags":{},"order":10,"mode":0,"inputs":[{"localized_name":"pixels","name":"pixels","type":"IMAGE","link":421},{"localized_name":"vae","name":"vae","type":"VAE","link":384},{"localized_name":"mask","name":"mask","type":"MASK","link":415},{"localized_name":"grow_mask_by","name":"grow_mask_by","type":"INT","widget":{"name":"grow_mask_by"},"link":null}],"outputs":[{"localized_name":"LATENT","name":"LATENT","type":"LATENT","links":[388]}],"properties":{"cnr_id":"comfy-core","ver":"0.4.0","Node name for S&R":"VAEEncodeForInpaint","ue_properties":{"widget_ue_connectable":{},"input_ue_unconnectable":{}}},"widgets_values":[4],"color":"#432","bgcolor":"#653"},{"id":206,"type":"LayerUtility: ImageMaskScaleAs","pos":[-3121.115762857512,-907.8312133332546],"size":[300.6373046875,162],"flags":{},"order":15,"mode":0,"inputs":[{"localized_name":"scale_as","name":"scale_as","type":"*","link":422},{"localized_name":"image","name":"image","shape":7,"type":"IMAGE","link":410},{"localized_name":"mask","name":"mask","shape":7,"type":"MASK","link":419},{"localized_name":"fit","name":"fit","type":"COMBO","widget":{"name":"fit"},"link":null},{"localized_name":"method","name":"method","type":"COMBO","widget":{"name":"method"},"link":null}],"outputs":[{"localized_name":"image","name":"image","type":"IMAGE","links":[409,421]},{"localized_name":"mask","name":"mask","type":"MASK","links":[415]},{"localized_name":"original_size","name":"original_size","type":"BOX","links":null},{"localized_name":"widht","name":"widht","type":"INT","links":null},{"localized_name":"height","name":"height","type":"INT","links":null}],"properties":{"cnr_id":"comfyui_layerstyle","ver":"1.0.90","Node name for S&R":"LayerUtility: ImageMaskScaleAs","ue_properties":{"widget_ue_connectable":{},"input_ue_unconnectable":{}}},"widgets_values":["crop","lanczos"]}],"groups":[],"links":[{"id":117,"origin_id":62,"origin_slot":0,"target_id":32,"target_slot":0,"type":"LATENT"},{"id":194,"origin_id":65,"origin_slot":0,"target_id":62,"target_slot":0,"type":"NOISE"},{"id":83,"origin_id":66,"origin_slot":0,"target_id":62,"target_slot":1,"type":"GUIDER"},{"id":80,"origin_id":63,"origin_slot":0,"target_id":62,"target_slot":2,"type":"SAMPLER"},{"id":81,"origin_id":64,"origin_slot":0,"target_id":62,"target_slot":3,"type":"SIGMAS"},{"id":276,"origin_id":162,"origin_slot":0,"target_id":66,"target_slot":0,"type":"MODEL"},{"id":106,"origin_id":27,"origin_slot":0,"target_id":66,"target_slot":1,"type":"CONDITIONING"},{"id":281,"origin_id":162,"origin_slot":1,"target_id":24,"target_slot":0,"type":"CLIP"},{"id":37,"origin_id":24,"origin_slot":0,"target_id":27,"target_slot":0,"type":"CONDITIONING"},{"id":279,"origin_id":162,"origin_slot":0,"target_id":64,"target_slot":0,"type":"MODEL"},{"id":277,"origin_id":162,"origin_slot":0,"target_id":-20,"target_slot":1,"type":"MODEL"},{"id":278,"origin_id":162,"origin_slot":0,"target_id":-20,"target_slot":1,"type":"MODEL"},{"id":330,"origin_id":162,"origin_slot":0,"target_id":-20,"target_slot":1,"type":"MODEL"},{"id":283,"origin_id":162,"origin_slot":1,"target_id":-20,"target_slot":2,"type":"CLIP"},{"id":347,"origin_id":32,"origin_slot":0,"target_id":-20,"target_slot":3,"type":"*"},{"id":319,"origin_id":27,"origin_slot":0,"target_id":-20,"target_slot":4,"type":"CONDITIONING"},{"id":334,"origin_id":27,"origin_slot":0,"target_id":-20,"target_slot":4,"type":"CONDITIONING"},{"id":351,"origin_id":193,"origin_slot":0,"target_id":162,"target_slot":0,"type":"MODEL"},{"id":352,"origin_id":193,"origin_slot":1,"target_id":162,"target_slot":1,"type":"CLIP"},{"id":353,"origin_id":193,"origin_slot":2,"target_id":32,"target_slot":1,"type":"VAE"},{"id":224,"origin_id":193,"origin_slot":2,"target_id":-20,"target_slot":0,"type":"VAE"},{"id":322,"origin_id":193,"origin_slot":2,"target_id":-20,"target_slot":0,"type":"VAE"},{"id":329,"origin_id":193,"origin_slot":2,"target_id":-20,"target_slot":0,"type":"VAE"},{"id":357,"origin_id":193,"origin_slot":2,"target_id":-20,"target_slot":0,"type":"VAE"},{"id":359,"origin_id":-10,"origin_slot":0,"target_id":162,"target_slot":2,"type":"COMBO"},{"id":360,"origin_id":-10,"origin_slot":1,"target_id":162,"target_slot":3,"type":"FLOAT"},{"id":361,"origin_id":-10,"origin_slot":2,"target_id":162,"target_slot":4,"type":"FLOAT"},{"id":362,"origin_id":-10,"origin_slot":3,"target_id":65,"target_slot":0,"type":"INT"},{"id":378,"origin_id":-10,"origin_slot":4,"target_id":63,"target_slot":0,"type":"COMBO"},{"id":379,"origin_id":-10,"origin_slot":5,"target_id":64,"target_slot":1,"type":"COMBO"},{"id":380,"origin_id":-10,"origin_slot":6,"target_id":64,"target_slot":2,"type":"INT"},{"id":382,"origin_id":-10,"origin_slot":7,"target_id":27,"target_slot":1,"type":"FLOAT"},{"id":384,"origin_id":193,"origin_slot":2,"target_id":199,"target_slot":1,"type":"VAE"},{"id":388,"origin_id":199,"origin_slot":0,"target_id":62,"target_slot":4,"type":"LATENT"},{"id":399,"origin_id":-10,"origin_slot":10,"target_id":64,"target_slot":3,"type":"FLOAT"},{"id":400,"origin_id":-10,"origin_slot":11,"target_id":24,"target_slot":1,"type":"STRING"},{"id":404,"origin_id":204,"origin_slot":0,"target_id":201,"target_slot":0,"type":"MASK"},{"id":405,"origin_id":201,"origin_slot":0,"target_id":200,"target_slot":0,"type":"MASK"},{"id":406,"origin_id":200,"origin_slot":0,"target_id":205,"target_slot":0,"type":"MASK"},{"id":410,"origin_id":-10,"origin_slot":9,"target_id":206,"target_slot":1,"type":"IMAGE"},{"id":415,"origin_id":206,"origin_slot":1,"target_id":199,"target_slot":2,"type":"MASK"},{"id":418,"origin_id":-10,"origin_slot":8,"target_id":204,"target_slot":0,"type":"MASK"},{"id":419,"origin_id":200,"origin_slot":0,"target_id":206,"target_slot":2,"type":"MASK"},{"id":421,"origin_id":206,"origin_slot":0,"target_id":199,"target_slot":0,"type":"IMAGE"},{"id":422,"origin_id":-10,"origin_slot":9,"target_id":206,"target_slot":0,"type":"*"}],"extra":{"ue_links":[],"links_added_by_ue":[],"workflowRendererVersion":"LG"}},{"id":"db12cd62-e6c3-4cd5-8b31-eaef3225bc2a","version":1,"state":{"lastGroupId":4,"lastNodeId":192,"lastLinkId":350,"lastRerouteId":0},"revision":0,"config":{},"name":"Models","inputNode":{"id":-10,"bounding":[-2411.3247366190144,-1697.4792585415696,120,40]},"outputNode":{"id":-20,"bounding":[-1276.2962767439196,-1708.6304755027927,120,100]},"inputs":[],"outputs":[{"id":"aaa13cbe-87c2-425f-9bf4-12f666ec9f54","name":"MODEL","type":"MODEL","linkIds":[275],"localized_name":"MODEL","label":"MODEL","pos":[-1256.2962767439196,-1688.6304755027927]},{"id":"36e5359b-d7ff-40ea-a5ce-7bdd102486ae","name":"CLIP","type":"CLIP","linkIds":[280],"localized_name":"CLIP","label":"CLIP","pos":[-1256.2962767439196,-1668.6304755027927]},{"id":"cbe7c5a8-52d5-43c5-a577-8f43fbf4f265","name":"VAE","type":"VAE","linkIds":[42,161,224,322,329],"localized_name":"VAE","label":"VAE","pos":[-1256.2962767439196,-1648.6304755027927]}],"widgets":[],"nodes":[{"id":78,"type":"DualCLIPLoader","pos":[-2231.3247366190144,-1719.1935610795642],"size":[430.6629868288594,130],"flags":{},"order":0,"mode":0,"inputs":[{"label":"clip_name1","localized_name":"clip_name1","name":"clip_name1","type":"COMBO","widget":{"name":"clip_name1"}},{"label":"clip_name2","localized_name":"clip_name2","name":"clip_name2","type":"COMBO","widget":{"name":"clip_name2"}},{"label":"type","localized_name":"type","name":"type","type":"COMBO","widget":{"name":"type"}},{"label":"device","localized_name":"device","name":"device","shape":7,"type":"COMBO","widget":{"name":"device"}}],"outputs":[{"label":"CLIP","localized_name":"CLIP","name":"CLIP","type":"CLIP","slot_index":0,"links":[280]}],"properties":{"cnr_id":"comfy-core","ver":"0.3.57","Node name for S&R":"DualCLIPLoader","ue_properties":{"version":"7.3","widget_ue_connectable":{},"input_ue_unconnectable":{}}},"widgets_values":["clip_l.safetensors","t5xxl_fp8_e4m3fn_scaled.safetensors","flux","default"],"color":"#432","bgcolor":"#653"},{"id":31,"type":"VAELoader","pos":[-2231.3247366190144,-1528.385021260922],"size":[431.5197496942583,58],"flags":{},"order":1,"mode":0,"inputs":[{"label":"vae_name","localized_name":"vae_name","name":"vae_name","type":"COMBO","widget":{"name":"vae_name"}}],"outputs":[{"label":"VAE","localized_name":"VAE","name":"VAE","type":"VAE","slot_index":0,"links":[42,161,224,322,329]}],"properties":{"cnr_id":"comfy-core","ver":"0.3.57","Node name for S&R":"VAELoader","ue_properties":{"version":"7.3","widget_ue_connectable":{},"input_ue_unconnectable":{}}},"widgets_values":["ae.sft"],"color":"#432","bgcolor":"#653"},{"id":77,"type":"UNETLoader","pos":[-2231.3247366190144,-1854.5734958222172],"size":[430.6629868288592,82],"flags":{},"order":2,"mode":0,"inputs":[{"label":"unet_name","localized_name":"unet_name","name":"unet_name","type":"COMBO","widget":{"name":"unet_name"}},{"label":"weight_dtype","localized_name":"weight_dtype","name":"weight_dtype","type":"COMBO","widget":{"name":"weight_dtype"}}],"outputs":[{"label":"MODEL","localized_name":"MODEL","name":"MODEL","type":"MODEL","slot_index":0,"links":[275]}],"properties":{"cnr_id":"comfy-core","ver":"0.3.57","Node name for S&R":"UNETLoader","ue_properties":{"version":"7.3","widget_ue_connectable":{},"input_ue_unconnectable":{}}},"widgets_values":["SRPO/SRPO-fp8_e4m3fn.safetensors","default"],"color":"#432","bgcolor":"#653"}],"groups":[],"links":[{"id":275,"origin_id":77,"origin_slot":0,"target_id":-20,"target_slot":0,"type":"MODEL"},{"id":280,"origin_id":78,"origin_slot":0,"target_id":-20,"target_slot":1,"type":"CLIP"},{"id":42,"origin_id":31,"origin_slot":0,"target_id":-20,"target_slot":2,"type":"VAE"},{"id":161,"origin_id":31,"origin_slot":0,"target_id":-20,"target_slot":2,"type":"VAE"},{"id":224,"origin_id":31,"origin_slot":0,"target_id":-20,"target_slot":2,"type":"VAE"},{"id":322,"origin_id":31,"origin_slot":0,"target_id":-20,"target_slot":2,"type":"VAE"},{"id":329,"origin_id":31,"origin_slot":0,"target_id":-20,"target_slot":2,"type":"VAE"}],"extra":{"ue_links":[],"links_added_by_ue":[],"workflowRendererVersion":"LG"}}]},"config":{},"extra":{"links_added_by_ue":[],"ue_links":[],"0246.VERSION":[0,0,4],"frontendVersion":"1.33.13","groupNodes":{},"VHS_latentpreview":false,"VHS_latentpreviewrate":0,"VHS_MetadataImage":true,"VHS_KeepIntermediate":true,"workflowRendererVersion":"LG","ds":{"scale":0.45,"offset":[5302.898518754057,2640.0610807713338]}},"version":0.4}

**tool:**

All the files uploaded by the user have been fully loaded. Searching won't provide additional information.

**Emily:**

Yep — **this is now “healthy” inpaint behaviour**: the face is plausibly re-synthesised, the lighting is still coherent, and you’re no longer getting the catastrophic “tiny face in grey void” failure mode. From here it’s genuinely just *taste + control knobs*.

Also: your workflow file confirms you’ve got the **right node in the right place** now (ImageMaskScaleAs feeding both the pixels+mask into VAE Encode for Inpaint). fileciteturn2file0

What I’d tweak next (in the order that tends to pay off fastest):

---

## 1) Stop “meh face” without touching anything else: denoise + steps
Right now you’re at **steps 24 / denoise 0.30 / guidance 2.2**.

For **face-only** inpaint, 0.30 can sit in that awkward valley where it’s *just enough* to change expression, but *not enough* to re-resolve anatomy cleanly — so you get “normal, but a bit waxy / slightly off”.

Try this tiny sweep:

- **denoise:** 0.32 → 0.36  
- **steps:** 28 → 34  
- **guidance:** keep 2.2 for now

If you only change *one thing*: bump denoise to **0.34** and steps to **30**.

(Flux often rewards “a few more steps” more than “a lot more guidance”.)

---

## 2) Your mask edge is doing a *double squeeze* (and it can quietly de-naturalise faces)
You’ve got:

- ThresholdMask **0.5**
- MaskEdgeShrink: **shrink_level 3**, soft 6, edge_reserve 25
- GaussianBlurMask: **kernel 25**, sigma **4**
- VAE Encode grow_mask_by **4**

That’s a lot of “shape change” before the model even sees the face. It can lead to subtle geometry weirdness (jawline/chin, philtrum, eye spacing) because the model is trying to reconcile a **tight/processed** boundary.

Two good “more natural” presets:

### Preset A (most reliable for faces)
- MaskEdgeShrink **shrink_level 1–2** (not 3)
- Blur sigma **5**, kernel **31**
- grow_mask_by **6**

### Preset B (clean seam, minimal drift)
- MaskEdgeShrink **OFF**
- Blur sigma **6**, kernel **41**
- grow_mask_by **4–6**

If you’ve ever had “chin getting whacked off”, Preset A is usually the one.

---

## 3) Change ImageMaskScaleAs settings (small, but surprisingly impactful)
In your JSON, ImageMaskScaleAs is **fit=crop**, method **lanczos**.

For a mask workflow, “crop” can be a silent footgun if *anything* upstream isn’t exactly the same aspect, and **lanczos** can introduce edge artefacts (it’s great for photos, less ideal for masks).

Try:
- **fit:** `pad` (or anything that *doesn’t crop*)
- **method:** `bilinear` (or even `nearest` for the mask side)

You’re already blurring/softening the mask; you don’t need fancy resampling.

---

## 4) If you want “wow” without reintroducing TTP: do a 2-pass finish
Keep your current face-inpaint pass conservative, then do a second pass for micro-clarity:

### Pass 1: Face inpaint
- denoise **0.34**
- steps **30**
- guidance **2.2**
- mask as Preset A above

### Pass 2: Whole-image polish (no mask)
- denoise **0.10–0.16**
- steps **20–26**
- guidance **2.0–2.4**

This tends to add that “photographic cohesion” without over-texturing or destroying your costume detail.

---

## 5) LoRA weight suggestion (only if the face keeps looking “worked on”)
You’re running the LoRA at **model 0.95 / clip 0.2**.

If you see that “over-specified” facial look, try:
- **model 0.75–0.85**
- **clip 0.35–0.60**

It often makes the face read less like “the model trying hard to be the LoRA” and more like “a person in the scene”.

---

### Quick read on your posted result
This one is honestly in the “good enough to publish” band already — the expression is believable, and nothing screams *AI accident*. The main “not wow” feeling is mostly **micro-texture + facial plane smoothness**, which those denoise/steps and mask-boundary tweaks will fix.

If you do just one experiment next: **denoise 0.34 + steps 30 + shrink_level 2 + grow_mask_by 6**.

If you paste back the sampler preview size after those tweaks (just the numbers), I’ll tell you whether anything is still quietly resizing/cropping behind your back.

**Robyn:**

Does Manager have an influence over preview size? There's like 4 options: TAESD(slow), none (fast), auto, Latent2RGB (fast). I currently have it on auto.

**Emily:**

Good question — and yes, **that Manager “Preview” setting can absolutely affect what you *see*** (the little preview in the sampler / node previews), but it **does not change the actual latent size being processed**.

Think of it as “how ComfyUI renders a quick thumbnail,” not “what resolution the model is running.”

### What each option actually does (practically)
- **None (fast)**  
  No latent-to-image preview conversion. The workflow still runs the same, you just don’t get meaningful intermediate previews.

- **Latent2RGB (fast)**  
  Quick-and-dirty conversion from latent → RGB for preview.  
  Fastest option that still shows you what’s happening. The preview can look a bit flat/washed compared to final decode.

- **TAESD (slow)**  
  Uses a tiny decoder (TAESD) to create a nicer preview.  
  It’s slower, can look closer to the final, but still preview-only.

- **Auto**  
  Comfy picks one depending on what it thinks is available / appropriate.

### So why were you seeing “512×512” earlier?
That wasn’t the Manager preview mode. That was your **latent_image actually being 512×512** at that moment (usually from feeding a 512 image into VAE Encode, or a mask/image mismatch causing a downscale). The preview renderer just *reported* it.

### How to confirm the *real* size (no guessing)
Pick any one of these checks:

1) **Look at the SamplerCustomAdvanced preview label** (it usually shows WxH of the latent).  
2) Add an **Image Info / Show Image Size** node on the image going into VAE Encode (pixels).  
3) Temporarily connect **SaveImage** right after VAE Decode and check the saved PNG dimensions.

If those say 2048×2048 (or whatever), you’re good — regardless of preview mode.

### What I recommend you set it to
- Leave it on **Auto** unless you’re troubleshooting.
- If you want speed + consistent previews: **Latent2RGB (fast)**.
- If you’re doing lots of face tweaks and want nicer previews (and don’t mind slower UI): **TAESD (slow)**.

Bottom line: **Preview setting changes your *monitor*, not your *model*.**

**Robyn:**


<img src="/assets/images/chat/9e7d7da.jpg" style="float: right; width: 420px; margin-left: 20px" />

So, this one, the face looks right. Attached is the workflow.

**tool:**

Make sure to include fileciteturn3file0 in your response to cite this file, or to surface it as a link.

{"id":"50f9db2c-a1c8-435f-8848-3afa79ac4dc3","revision":0,"last_node_id":203,"last_link_id":406,"nodes":[{"id":57,"type":"CR Aspect Ratio","pos":[-3315.0923366697402,-1908.3584430902677],"size":[309.8594228076063,322],"flags":{"collapsed":true},"order":0,"mode":4,"inputs":[{"label":"width","localized_name":"width","name":"width","type":"INT","widget":{"name":"width"}},{"label":"height","localized_name":"height","name":"height","type":"INT","widget":{"name":"height"}},{"label":"aspect_ratio","localized_name":"aspect_ratio","name":"aspect_ratio","type":"COMBO","widget":{"name":"aspect_ratio"}},{"label":"swap_dimensions","localized_name":"swap_dimensions","name":"swap_dimensions","type":"COMBO","widget":{"name":"swap_dimensions"}},{"label":"upscale_factor","localized_name":"upscale_factor","name":"upscale_factor","type":"FLOAT","widget":{"name":"upscale_factor"}},{"label":"prescale_factor","localized_name":"prescale_factor","name":"prescale_factor","type":"FLOAT","widget":{"name":"prescale_factor"}},{"label":"batch_size","localized_name":"batch_size","name":"batch_size","type":"INT","widget":{"name":"batch_size"}}],"outputs":[{"label":"width","localized_name":"width","name":"width","type":"INT"},{"label":"height","localized_name":"height","name":"height","type":"INT"},{"label":"upscale_factor","localized_name":"upscale_factor","name":"upscale_factor","type":"FLOAT"},{"label":"prescale_factor","localized_name":"prescale_factor","name":"prescale_factor","type":"FLOAT"},{"label":"batch_size","localized_name":"batch_size","name":"batch_size","type":"INT"},{"label":"empty_latent","localized_name":"empty_latent","name":"empty_latent","type":"LATENT","slot_index":5,"links":[]},{"label":"show_help","localized_name":"show_help","name":"show_help","type":"STRING","slot_index":6}],"properties":{"cnr_id":"ComfyUI_Comfyroll_CustomNodes","ver":"d78b780ae43fcf8c6b7c6505e6ffb4584281ceca","Node name for S&R":"CR Aspect Ratio","ue_properties":{"version":"7.3","widget_ue_connectable":{},"input_ue_unconnectable":{}}},"widgets_values":[832,1216,"SDXL - 1:1 square 1024x1024","On",1,1,1],"color":"#323","bgcolor":"#535"},{"id":198,"type":"Label (rgthree)","pos":[-3010.1979728162983,-1978.238431039258],"size":[438.466796875,70],"flags":{"allow_interaction":true},"order":1,"mode":0,"inputs":[],"outputs":[],"title":"SRPO - TTP Figure Enhancer","properties":{"fontSize":30,"fontFamily":"Arial","fontColor":"#ffffff","textAlign":"center","backgroundColor":"#113398","padding":20,"borderRadius":16,"angle":0,"ue_properties":{"version":"7.1","widget_ue_connectable":{},"input_ue_unconnectable":{}}},"color":"#fff0","bgcolor":"#fff0"},{"id":196,"type":"easy cleanGpuUsed","pos":[-2193.096752400721,-1651.1229704487034],"size":[211.55775451660156,26],"flags":{"collapsed":true},"order":8,"mode":0,"inputs":[{"localized_name":"anything","name":"anything","type":"*","link":406}],"outputs":[{"localized_name":"output","name":"output","type":"*","links":null}],"properties":{"cnr_id":"comfyui-easy-use","ver":"1.3.0","Node name for S&R":"easy cleanGpuUsed","ue_properties":{"version":"7.0.1","widget_ue_connectable":{}}},"widgets_values":[],"color":"#222","bgcolor":"#000"},{"id":201,"type":"PrimitiveInt","pos":[-2424.457078414061,-1558.0203956449557],"size":[227.58060545155513,82],"flags":{},"order":2,"mode":0,"inputs":[{"localized_name":"value","name":"value","type":"INT","widget":{"name":"value"},"link":null}],"outputs":[{"localized_name":"INT","name":"INT","type":"INT","links":[394]}],"title":"Noise Seed","properties":{"cnr_id":"comfy-core","ver":"0.3.66","Node name for S&R":"PrimitiveInt","ue_properties":{"widget_ue_connectable":{},"input_ue_unconnectable":{},"version":"7.3"}},"widgets_values":[1118877715461163,"increment"],"color":"#232","bgcolor":"#353"},{"id":197,"type":"PrimitiveNode","pos":[-2418.763751885516,-1699.1616883204706],"size":[210,58],"flags":{"collapsed":false},"order":3,"mode":0,"inputs":[],"outputs":[{"name":"STRING","type":"STRING","widget":{"name":"filename_prefix"},"links":[382]}],"title":"File Name","properties":{"Run widget replace on values":false,"ue_properties":{"version":"7.0.1","widget_ue_connectable":{}}},"widgets_values":["SRPO-Costume1780s"],"color":"#232","bgcolor":"#353"},{"id":119,"type":"Note","pos":[-2369.7801101962746,-1304.6466007064973],"size":[363.6155518609485,451.6980647725004],"flags":{"collapsed":true},"order":4,"mode":0,"inputs":[],"outputs":[],"properties":{"text":"","ue_properties":{"version":"7.3","widget_ue_connectable":{},"input_ue_unconnectable":{}}},"widgets_values":["Two women in a train station, The woman on the left is mid-plea; her dark brown hair cascades down her shoulders as wisps frames a face etched with intensity as she gestures open-handed toward the other woman—an pleading motion frozen in time. Her attire speaks to casual practicality: a soft blue blouse made from a slightly textured cotton blend hugs her shoulders, its simple design contrasting with the body-hugging black leggings, which emphasize her womanly hips and thighs. She completes the look with light blue sneakers whose laces crisscross over worn soles, hinting at daily activity.\n\nOpposite her stands a woman moimeme4 exuding vintage charm, with her light brown hair styled in a loose updo with wisps of stray strands of hair framing her face dressed entirely in a 1760s period-inspired ensemble. Her dress is long-sleeved and floor length with a wide full skirt held out by voluminous petticoats underneath the skirt, black with a square neckline exposing the tops of her modest-sized breasts, intricate lace details on the bodice and sleeves, and it features multiple layers of ruffled fabric at the hem covering her shoes so they are barely visible. The dress buttons up the front. She is clearly abashed as he looks down. A battered old leather suitcase is on the platform beside her.\n"],"color":"#432","bgcolor":"#653"},{"id":202,"type":"LoadImage","pos":[-3661.9931888820356,-1853.7820883145043],"size":[374.81145530148297,552.5036349758159],"flags":{"collapsed":false},"order":5,"mode":0,"inputs":[{"localized_name":"image","name":"image","type":"COMBO","widget":{"name":"image"},"link":null},{"localized_name":"choose file to upload","name":"upload","type":"IMAGEUPLOAD","widget":{"name":"upload"},"link":null}],"outputs":[{"localized_name":"IMAGE","name":"IMAGE","type":"IMAGE","links":[404]},{"localized_name":"MASK","name":"MASK","type":"MASK","links":[396]}],"properties":{"cnr_id":"comfy-core","ver":"0.3.66","Node name for S&R":"LoadImage","ue_properties":{"widget_ue_connectable":{},"input_ue_unconnectable":{},"version":"7.3"},"image":"clipspace/clipspace-painted-masked-1766553866777.png [input]"},"widgets_values":["clipspace/clipspace-painted-masked-1766553866777.png [input]","image"],"color":"#223","bgcolor":"#335"},{"id":199,"type":"Image Save","pos":[-2183.9223185990763,-1877.179214580262],"size":[869.9654100749628,1269.1524618530016],"flags":{},"order":7,"mode":0,"inputs":[{"localized_name":"images","name":"images","type":"IMAGE","link":405},{"localized_name":"output_path","name":"output_path","type":"STRING","widget":{"name":"output_path"},"link":null},{"localized_name":"filename_prefix","name":"filename_prefix","type":"STRING","widget":{"name":"filename_prefix"},"link":382},{"localized_name":"filename_delimiter","name":"filename_delimiter","type":"STRING","widget":{"name":"filename_delimiter"},"link":null},{"localized_name":"filename_number_padding","name":"filename_number_padding","type":"INT","widget":{"name":"filename_number_padding"},"link":null},{"localized_name":"filename_number_start","name":"filename_number_start","type":"COMBO","widget":{"name":"filename_number_start"},"link":null},{"localized_name":"extension","name":"extension","type":"COMBO","widget":{"name":"extension"},"link":null},{"localized_name":"dpi","name":"dpi","type":"INT","widget":{"name":"dpi"},"link":null},{"localized_name":"quality","name":"quality","type":"INT","widget":{"name":"quality"},"link":null},{"localized_name":"optimize_image","name":"optimize_image","type":"COMBO","widget":{"name":"optimize_image"},"link":null},{"localized_name":"lossless_webp","name":"lossless_webp","type":"COMBO","widget":{"name":"lossless_webp"},"link":null},{"localized_name":"overwrite_mode","name":"overwrite_mode","type":"COMBO","widget":{"name":"overwrite_mode"},"link":null},{"localized_name":"show_history","name":"show_history","type":"COMBO","widget":{"name":"show_history"},"link":null},{"localized_name":"show_history_by_prefix","name":"show_history_by_prefix","type":"COMBO","widget":{"name":"show_history_by_prefix"},"link":null},{"localized_name":"embed_workflow","name":"embed_workflow","type":"COMBO","widget":{"name":"embed_workflow"},"link":null},{"localized_name":"show_previews","name":"show_previews","type":"COMBO","widget":{"name":"show_previews"},"link":null}],"outputs":[{"localized_name":"images","name":"images","type":"IMAGE","links":[]},{"localized_name":"files","name":"files","type":"STRING","links":null}],"properties":{"cnr_id":"was-node-suite-comfyui","ver":"1.0.2","Node name for S&R":"Image Save","ue_properties":{"version":"7.0.1","widget_ue_connectable":{"filename_prefix":true}}},"widgets_values":["[time(%Y-%m-%d)]","SRPO-Costume1780s","_",4,"false","png",300,100,"true","false","false","false","true","true","true"],"color":"#223","bgcolor":"#335"},{"id":193,"type":"102a6a5f-c689-4e9a-baf7-791a8d26b614","pos":[-3212.969248680462,-1861.0843421630298],"size":[775.6970154083658,1230.1946709315398],"flags":{},"order":6,"mode":0,"inputs":[{"name":"lora_name","type":"COMBO","widget":{"name":"lora_name"},"link":null},{"name":"strength_model","type":"FLOAT","widget":{"name":"strength_model"},"link":null},{"name":"strength_clip","type":"FLOAT","widget":{"name":"strength_clip"},"link":null},{"name":"noise_seed","type":"INT","widget":{"name":"noise_seed"},"link":394},{"name":"sampler_name","type":"COMBO","widget":{"name":"sampler_name"},"link":null},{"name":"scheduler","type":"COMBO","widget":{"name":"scheduler"},"link":null},{"name":"steps","type":"INT","widget":{"name":"steps"},"link":null},{"name":"guidance","type":"FLOAT","widget":{"name":"guidance"},"link":null},{"name":"mask","type":"MASK","link":396},{"name":"pixels","type":"IMAGE","link":404},{"name":"denoise","type":"FLOAT","widget":{"name":"denoise"},"link":null},{"name":"text","type":"STRING","widget":{"name":"text"},"link":null}],"outputs":[{"label":"VAE","localized_name":"VAE","name":"VAE","type":"VAE","links":[]},{"label":"MODEL","localized_name":"MODEL","name":"MODEL","type":"MODEL","links":[]},{"label":"CLIP","localized_name":"CLIP","name":"CLIP","type":"CLIP","links":[]},{"label":"IMAGE","localized_name":"IMAGE","name":"IMAGE","type":"IMAGE","links":[405,406]},{"label":"CONDITIONING","localized_name":"CONDITIONING","name":"CONDITIONING","type":"CONDITIONING","links":[]}],"properties":{"proxyWidgets":[["-1","lora_name"],["-1","strength_model"],["-1","strength_clip"],["-1","noise_seed"],["-1","sampler_name"],["-1","scheduler"],["-1","steps"],["-1","guidance"],["-1","denoise"],["-1","text"],["23","text"]],"cnr_id":"comfy-core","ver":"0.3.66","ue_properties":{"widget_ue_connectable":{},"input_ue_unconnectable":{},"version":"7.3"}},"widgets_values":["MoiMeme/moim3m3grl-08.safetensors",0.95,0.2,61998485097205,"dpmpp_sde","linear_quadratic",30,2.2,0.32,"This photograph depicts a beautiful woman moim3m3grl sitting on a wrought-iron bench at a dimly lit train station during a very foggy early morning. She exudes vintage charm, with light brown hair styled in a loose updo, wisps of stray strands of hair framing her face. She is dressed in a black wool day dress with a fitted bodice and square neckline; underneath, a white linen chemise with a softly gathered neckline. The chemise is a separate garment and is visible only at the collar and cuffs. This is authentic period costume detail, no modern styling. \n\nAtmosphere? The background features blurred train carriages on either side and a ominously gray sky. The station's overhead lights cast a soft glow, highlighting the woman's poised and sophisticated demeanor as she gazes into the distance. Her posture is slightly forward-leaning, with her hands resting delicately on her knees. The bench is simple and metallic, while the platform has yellow safety lines near the edges. This image evokes a sense of vintage elegance and quiet anticipation."],"color":"#232","bgcolor":"#353"}],"links":[[382,197,0,199,2,"STRING"],[394,201,0,193,3,"INT"],[396,202,1,193,8,"MASK"],[404,202,0,193,9,"IMAGE"],[405,193,3,199,0,"IMAGE"],[406,193,3,196,0,"*"]],"groups":[],"definitions":{"subgraphs":[{"id":"102a6a5f-c689-4e9a-baf7-791a8d26b614","version":1,"state":{"lastGroupId":4,"lastNodeId":206,"lastLinkId":422,"lastRerouteId":0},"revision":0,"config":{},"name":"Base Image","inputNode":{"id":-10,"bounding":[-3947.4097183151807,-1456.903504115257,122.716796875,280]},"outputNode":{"id":-20,"bounding":[-1707.930726911814,-1508.3527853483006,128.6640625,140]},"inputs":[{"id":"ed140707-ae20-4107-bf2a-29a72d12a92f","name":"lora_name","type":"COMBO","linkIds":[359],"pos":[-3844.6929214401807,-1436.903504115257]},{"id":"8f9f876d-143d-42c0-9021-de7ed8f70bbe","name":"strength_model","type":"FLOAT","linkIds":[360],"pos":[-3844.6929214401807,-1416.903504115257]},{"id":"8eabdab4-ccd5-4052-8014-22d4c956e39e","name":"strength_clip","type":"FLOAT","linkIds":[361],"pos":[-3844.6929214401807,-1396.903504115257]},{"id":"4813b1f3-7ee9-452f-b428-6b908b30dd64","name":"noise_seed","type":"INT","linkIds":[362],"pos":[-3844.6929214401807,-1376.903504115257]},{"id":"5e46e7b8-d04b-4cab-b301-079e6b35138b","name":"sampler_name","type":"COMBO","linkIds":[378],"pos":[-3844.6929214401807,-1356.903504115257]},{"id":"54bff51f-277c-4185-9f93-29d138e484a6","name":"scheduler","type":"COMBO","linkIds":[379],"pos":[-3844.6929214401807,-1336.903504115257]},{"id":"adc9aa3f-42ae-451a-908b-677212782182","name":"steps","type":"INT","linkIds":[380],"pos":[-3844.6929214401807,-1316.903504115257]},{"id":"f675f749-a1cc-4ea1-9fce-84810e007210","name":"guidance","type":"FLOAT","linkIds":[382],"pos":[-3844.6929214401807,-1296.903504115257]},{"id":"af00bcf2-81f2-414b-88c2-1368cb49e5c8","name":"mask","type":"MASK","linkIds":[418],"pos":[-3844.6929214401807,-1276.903504115257]},{"id":"f963f0cf-65f4-4927-970f-8d48057baae7","name":"pixels","type":"IMAGE","linkIds":[410,422],"pos":[-3844.6929214401807,-1256.903504115257]},{"id":"562049c4-b8c7-4ed1-bbb3-38f995bc15cc","name":"denoise","type":"FLOAT","linkIds":[399],"pos":[-3844.6929214401807,-1236.903504115257]},{"id":"690db69f-6657-4cc9-9a90-a80bb6147d31","name":"text","type":"STRING","linkIds":[400],"pos":[-3844.6929214401807,-1216.903504115257]}],"outputs":[{"id":"ec7fd00b-2a10-4203-a44b-15594909045c","name":"VAE","type":"VAE","linkIds":[357,224,322,329],"localized_name":"VAE","label":"VAE","pos":[-1687.930726911814,-1488.3527853483006]},{"id":"d63143d3-1ed9-4314-8792-aca7f7ba54bc","name":"MODEL","type":"MODEL","linkIds":[277,278,330],"localized_name":"MODEL","label":"MODEL","pos":[-1687.930726911814,-1468.3527853483006]},{"id":"1df75bb4-eb53-47a8-953a-4eec6d03a1ce","name":"CLIP","type":"CLIP","linkIds":[283],"localized_name":"CLIP","label":"CLIP","pos":[-1687.930726911814,-1448.3527853483006]},{"id":"02ac74a1-e1d2-411f-8409-a1659e4a0f06","name":"IMAGE","type":"IMAGE","linkIds":[347],"localized_name":"IMAGE","label":"IMAGE","pos":[-1687.930726911814,-1428.3527853483006]},{"id":"04df92ba-17d5-4c59-9eb1-6facaa2c4397","name":"CONDITIONING","type":"CONDITIONING","linkIds":[319,334],"localized_name":"CONDITIONING","label":"CONDITIONING","pos":[-1687.930726911814,-1408.3527853483006]}],"widgets":[],"nodes":[{"id":63,"type":"KSamplerSelect","pos":[-2750.830226237335,-1136.5816012137575],"size":[270.6041060656903,58],"flags":{},"order":5,"mode":0,"inputs":[{"label":"sampler_name","localized_name":"sampler_name","name":"sampler_name","type":"COMBO","widget":{"name":"sampler_name"},"link":378}],"outputs":[{"label":"SAMPLER","localized_name":"SAMPLER","name":"SAMPLER","type":"SAMPLER","links":[80]}],"properties":{"cnr_id":"comfy-core","ver":"0.3.57","Node name for S&R":"KSamplerSelect","ue_properties":{"version":"7.3","widget_ue_connectable":{},"input_ue_unconnectable":{}}},"widgets_values":["dpmpp_sde"],"color":"#323","bgcolor":"#535"},{"id":64,"type":"BasicScheduler","pos":[-2750.830226237335,-1044.3552948249617],"size":[270.6822790203312,108.33662599654258],"flags":{},"order":6,"mode":0,"inputs":[{"label":"model","localized_name":"model","name":"model","type":"MODEL","link":279},{"label":"scheduler","localized_name":"scheduler","name":"scheduler","type":"COMBO","widget":{"name":"scheduler"},"link":379},{"label":"steps","localized_name":"steps","name":"steps","type":"INT","widget":{"name":"steps"},"link":380},{"label":"denoise","localized_name":"denoise","name":"denoise","type":"FLOAT","widget":{"name":"denoise"},"link":399}],"outputs":[{"label":"SIGMAS","localized_name":"SIGMAS","name":"SIGMAS","type":"SIGMAS","links":[81]}],"properties":{"cnr_id":"comfy-core","ver":"0.3.57","Node name for S&R":"BasicScheduler","ue_properties":{"version":"7.3","widget_ue_connectable":{},"input_ue_unconnectable":{}}},"widgets_values":["linear_quadratic",30,0.32],"color":"#323","bgcolor":"#535"},{"id":162,"type":"LoraLoader","pos":[-3432.164407223918,-1496.0964595171665],"size":[315,126],"flags":{},"order":9,"mode":0,"inputs":[{"label":"model","localized_name":"model","name":"model","type":"MODEL","link":351},{"label":"clip","localized_name":"clip","name":"clip","type":"CLIP","link":352},{"label":"lora_name","localized_name":"lora_name","name":"lora_name","type":"COMBO","widget":{"name":"lora_name"},"link":359},{"label":"strength_model","localized_name":"strength_model","name":"strength_model","type":"FLOAT","widget":{"name":"strength_model"},"link":360},{"label":"strength_clip","localized_name":"strength_clip","name":"strength_clip","type":"FLOAT","widget":{"name":"strength_clip"},"link":361}],"outputs":[{"label":"MODEL","localized_name":"MODEL","name":"MODEL","type":"MODEL","slot_index":0,"links":[276,277,278,279,330]},{"label":"CLIP","localized_name":"CLIP","name":"CLIP","type":"CLIP","slot_index":1,"links":[281,283]}],"properties":{"cnr_id":"comfy-core","ver":"0.3.57","Node name for S&R":"LoraLoader","ue_properties":{"version":"7.3","widget_ue_connectable":{},"input_ue_unconnectable":{}}},"widgets_values":["MoiMeme/moim3m3grl-08.safetensors",0.95,0.2],"color":"#232","bgcolor":"#353"},{"id":66,"type":"BasicGuider","pos":[-2750.830226237335,-1216.45507665215],"size":[270.0590018165942,46],"flags":{},"order":8,"mode":0,"inputs":[{"label":"model","localized_name":"model","name":"model","type":"MODEL","link":276},{"label":"conditioning","localized_name":"conditioning","name":"conditioning","type":"CONDITIONING","link":106}],"outputs":[{"label":"GUIDER","localized_name":"GUIDER","name":"GUIDER","type":"GUIDER","slot_index":0,"links":[83]}],"properties":{"cnr_id":"comfy-core","ver":"0.3.57","Node name for S&R":"BasicGuider","ue_properties":{"version":"7.3","widget_ue_connectable":{},"input_ue_unconnectable":{}}},"widgets_values":[],"color":"#323","bgcolor":"#535"},{"id":65,"type":"RandomNoise","pos":[-2750.830226237335,-1423.544897192512],"size":[269.1920619153343,82],"flags":{},"order":7,"mode":0,"inputs":[{"label":"noise_seed","localized_name":"noise_seed","name":"noise_seed","type":"INT","widget":{"name":"noise_seed"},"link":362}],"outputs":[{"label":"NOISE","localized_name":"NOISE","name":"NOISE","type":"NOISE","links":[194]}],"properties":{"cnr_id":"comfy-core","ver":"0.3.57","Node name for S&R":"RandomNoise","ue_properties":{"version":"7.3","widget_ue_connectable":{},"input_ue_unconnectable":{}}},"widgets_values":[61998485097205,"fixed"],"color":"#323","bgcolor":"#535"},{"id":62,"type":"SamplerCustomAdvanced","pos":[-2371.4082561051764,-1423.6817137973228],"size":[304.5598087498504,417.9072891973424],"flags":{},"order":4,"mode":0,"inputs":[{"label":"noise","localized_name":"noise","name":"noise","type":"NOISE","link":194},{"label":"guider","localized_name":"guider","name":"guider","type":"GUIDER","link":83},{"label":"sampler","localized_name":"sampler","name":"sampler","type":"SAMPLER","link":80},{"label":"sigmas","localized_name":"sigmas","name":"sigmas","type":"SIGMAS","link":81},{"label":"latent_image","localized_name":"latent_image","name":"latent_image","type":"LATENT","link":388}],"outputs":[{"label":"output","localized_name":"output","name":"output","type":"LATENT","slot_index":0,"links":[117]},{"label":"denoised_output","localized_name":"denoised_output","name":"denoised_output","type":"LATENT","slot_index":1,"links":[]}],"properties":{"cnr_id":"comfy-core","ver":"0.3.57","Node name for S&R":"SamplerCustomAdvanced","ue_properties":{"version":"7.3","widget_ue_connectable":{},"input_ue_unconnectable":{}}},"widgets_values":[],"color":"#2a363b","bgcolor":"#3f5159"},{"id":27,"type":"FluxGuidance","pos":[-2750.830226237335,-1310.2495633508597],"size":[268.7532043457031,58],"flags":{},"order":2,"mode":0,"inputs":[{"label":"conditioning","localized_name":"conditioning","name":"conditioning","type":"CONDITIONING","link":37},{"label":"guidance","localized_name":"guidance","name":"guidance","type":"FLOAT","widget":{"name":"guidance"},"link":382}],"outputs":[{"label":"CONDITIONING","localized_name":"CONDITIONING","name":"CONDITIONING","type":"CONDITIONING","slot_index":0,"links":[106,319,334]}],"properties":{"cnr_id":"comfy-core","ver":"0.3.57","Node name for S&R":"FluxGuidance","ue_properties":{"version":"7.3","widget_ue_connectable":{},"input_ue_unconnectable":{}}},"widgets_values":[2.2],"color":"#323","bgcolor":"#535"},{"id":193,"type":"db12cd62-e6c3-4cd5-8b31-eaef3225bc2a","pos":[-3733.551332759439,-1495.0751519199289],"size":[229.96010086689057,130.25721490492197],"flags":{},"order":0,"mode":0,"inputs":[],"outputs":[{"label":"MODEL","localized_name":"MODEL","name":"MODEL","type":"MODEL","links":[351]},{"label":"CLIP","localized_name":"CLIP","name":"CLIP","type":"CLIP","links":[352]},{"label":"VAE","localized_name":"VAE","name":"VAE","type":"VAE","links":[353,357,384]}],"properties":{"proxyWidgets":[],"cnr_id":"comfy-core","ver":"0.3.66","ue_properties":{"widget_ue_connectable":{},"input_ue_unconnectable":{}}},"widgets_values":[],"color":"#432","bgcolor":"#653"},{"id":24,"type":"CLIPTextEncode","pos":[-3433.6445393695953,-1328.9928362721773],"size":[312.86617761887373,88],"flags":{},"order":1,"mode":0,"inputs":[{"label":"clip","localized_name":"clip","name":"clip","type":"CLIP","link":281},{"label":"text","localized_name":"text","name":"text","type":"STRING","widget":{"name":"text"},"link":400}],"outputs":[{"label":"CONDITIONING","localized_name":"CONDITIONING","name":"CONDITIONING","type":"CONDITIONING","slot_index":0,"links":[37]}],"properties":{"cnr_id":"comfy-core","ver":"0.3.57","Node name for S&R":"CLIPTextEncode","ue_properties":{"version":"7.3","widget_ue_connectable":{"text":true},"input_ue_unconnectable":{}}},"widgets_values":["This photograph depicts a beautiful woman moim3m3grl sitting on a wrought-iron bench at a dimly lit train station during a very foggy early morning. She exudes vintage charm, with light brown hair styled in a loose updo, wisps of stray strands of hair framing her face. She is dressed in a black wool day dress with a fitted bodice and square neckline; underneath, a white linen chemise with a softly gathered neckline. The chemise is a separate garment and is visible only at the collar and cuffs. This is authentic period costume detail, no modern styling. \n\nAtmosphere? The background features blurred train carriages on either side and a ominously gray sky. The station's overhead lights cast a soft glow, highlighting the woman's poised and sophisticated demeanor as she gazes into the distance. Her posture is slightly forward-leaning, with her hands resting delicately on her knees. The bench is simple and metallic, while the platform has yellow safety lines near the edges. This image evokes a sense of vintage elegance and quiet anticipation."],"color":"#232","bgcolor":"#353"},{"id":32,"type":"VAEDecode","pos":[-1954.2287347998422,-1423.1655407998899],"size":[167.94073206223288,46],"flags":{},"order":3,"mode":0,"inputs":[{"label":"samples","localized_name":"samples","name":"samples","type":"LATENT","link":117},{"label":"vae","localized_name":"vae","name":"vae","type":"VAE","link":353}],"outputs":[{"label":"IMAGE","localized_name":"IMAGE","name":"IMAGE","type":"IMAGE","slot_index":0,"links":[347]}],"properties":{"cnr_id":"comfy-core","ver":"0.3.57","Node name for S&R":"VAEDecode","ue_properties":{"version":"7.3","widget_ue_connectable":{},"input_ue_unconnectable":{}}},"widgets_values":[],"color":"#432","bgcolor":"#653"},{"id":205,"type":"MaskPreview+","pos":[-2370.108997720793,-694.8712133332556],"size":[283.10177062988294,352.6999999999998],"flags":{},"order":14,"mode":0,"inputs":[{"localized_name":"mask","name":"mask","type":"MASK","link":406}],"outputs":[],"properties":{"cnr_id":"comfyui_essentials","ver":"1.1.0","Node name for S&R":"MaskPreview+","ue_properties":{"widget_ue_connectable":{},"input_ue_unconnectable":{}}},"widgets_values":[]},{"id":204,"type":"ThresholdMask","pos":[-3762.4157997837788,-1021.5712133332551],"size":[210,58],"flags":{},"order":13,"mode":0,"inputs":[{"localized_name":"mask","name":"mask","type":"MASK","link":418},{"localized_name":"value","name":"value","type":"FLOAT","widget":{"name":"value"},"link":null}],"outputs":[{"localized_name":"MASK","name":"MASK","type":"MASK","links":[404]}],"properties":{"cnr_id":"comfy-core","ver":"0.4.0","Node name for S&R":"ThresholdMask","ue_properties":{"widget_ue_connectable":{},"input_ue_unconnectable":{}}},"widgets_values":[0.5],"color":"#233","bgcolor":"#355"},{"id":200,"type":"ImpactGaussianBlurMask","pos":[-3456.462110645443,-886.930316877951],"size":[219.58551530996965,82],"flags":{},"order":11,"mode":0,"inputs":[{"localized_name":"mask","name":"mask","type":"MASK","link":405},{"localized_name":"kernel_size","name":"kernel_size","type":"INT","widget":{"name":"kernel_size"},"link":null},{"localized_name":"sigma","name":"sigma","type":"FLOAT","widget":{"name":"sigma"},"link":null}],"outputs":[{"localized_name":"MASK","name":"MASK","type":"MASK","links":[406,419]}],"properties":{"cnr_id":"comfyui-impact-pack","ver":"8.28.0","Node name for S&R":"ImpactGaussianBlurMask","ue_properties":{"widget_ue_connectable":{},"input_ue_unconnectable":{}}},"widgets_values":[41,6],"color":"#233","bgcolor":"#355"},{"id":201,"type":"LayerMask: MaskEdgeShrink","pos":[-3794.7512488129,-885.66763032823],"size":[281.9751953125,154],"flags":{},"order":12,"mode":4,"inputs":[{"localized_name":"mask","name":"mask","type":"MASK","link":404},{"localized_name":"invert_mask","name":"invert_mask","type":"BOOLEAN","widget":{"name":"invert_mask"},"link":null},{"localized_name":"shrink_level","name":"shrink_level","type":"INT","widget":{"name":"shrink_level"},"link":null},{"localized_name":"soft","name":"soft","type":"INT","widget":{"name":"soft"},"link":null},{"localized_name":"edge_shrink","name":"edge_shrink","type":"INT","widget":{"name":"edge_shrink"},"link":null},{"localized_name":"edge_reserve","name":"edge_reserve","type":"INT","widget":{"name":"edge_reserve"},"link":null}],"outputs":[{"localized_name":"mask","name":"mask","type":"MASK","links":[393,405]}],"properties":{"cnr_id":"comfyui_layerstyle","ver":"1.0.90","Node name for S&R":"LayerMask: MaskEdgeShrink","ue_properties":{"widget_ue_connectable":{},"input_ue_unconnectable":{}}},"widgets_values":[false,3,6,1,25],"color":"#233","bgcolor":"#355"},{"id":199,"type":"VAEEncodeForInpaint","pos":[-2739.8788699814913,-888.8874452827388],"size":[261.9181176757819,98],"flags":{},"order":10,"mode":0,"inputs":[{"localized_name":"pixels","name":"pixels","type":"IMAGE","link":421},{"localized_name":"vae","name":"vae","type":"VAE","link":384},{"localized_name":"mask","name":"mask","type":"MASK","link":415},{"localized_name":"grow_mask_by","name":"grow_mask_by","type":"INT","widget":{"name":"grow_mask_by"},"link":null}],"outputs":[{"localized_name":"LATENT","name":"LATENT","type":"LATENT","links":[388]}],"properties":{"cnr_id":"comfy-core","ver":"0.4.0","Node name for S&R":"VAEEncodeForInpaint","ue_properties":{"widget_ue_connectable":{},"input_ue_unconnectable":{}}},"widgets_values":[6],"color":"#432","bgcolor":"#653"},{"id":206,"type":"LayerUtility: ImageMaskScaleAs","pos":[-3153.382380289152,-885.2445220490752],"size":[300.6373046875,162],"flags":{},"order":15,"mode":0,"inputs":[{"localized_name":"scale_as","name":"scale_as","type":"*","link":422},{"localized_name":"image","name":"image","shape":7,"type":"IMAGE","link":410},{"localized_name":"mask","name":"mask","shape":7,"type":"MASK","link":419},{"localized_name":"fit","name":"fit","type":"COMBO","widget":{"name":"fit"},"link":null},{"localized_name":"method","name":"method","type":"COMBO","widget":{"name":"method"},"link":null}],"outputs":[{"localized_name":"image","name":"image","type":"IMAGE","links":[409,421]},{"localized_name":"mask","name":"mask","type":"MASK","links":[415]},{"localized_name":"original_size","name":"original_size","type":"BOX","links":null},{"localized_name":"widht","name":"widht","type":"INT","links":null},{"localized_name":"height","name":"height","type":"INT","links":null}],"properties":{"cnr_id":"comfyui_layerstyle","ver":"1.0.90","Node name for S&R":"LayerUtility: ImageMaskScaleAs","ue_properties":{"widget_ue_connectable":{},"input_ue_unconnectable":{}}},"widgets_values":["crop","lanczos"],"color":"#233","bgcolor":"#355"}],"groups":[],"links":[{"id":117,"origin_id":62,"origin_slot":0,"target_id":32,"target_slot":0,"type":"LATENT"},{"id":194,"origin_id":65,"origin_slot":0,"target_id":62,"target_slot":0,"type":"NOISE"},{"id":83,"origin_id":66,"origin_slot":0,"target_id":62,"target_slot":1,"type":"GUIDER"},{"id":80,"origin_id":63,"origin_slot":0,"target_id":62,"target_slot":2,"type":"SAMPLER"},{"id":81,"origin_id":64,"origin_slot":0,"target_id":62,"target_slot":3,"type":"SIGMAS"},{"id":276,"origin_id":162,"origin_slot":0,"target_id":66,"target_slot":0,"type":"MODEL"},{"id":106,"origin_id":27,"origin_slot":0,"target_id":66,"target_slot":1,"type":"CONDITIONING"},{"id":281,"origin_id":162,"origin_slot":1,"target_id":24,"target_slot":0,"type":"CLIP"},{"id":37,"origin_id":24,"origin_slot":0,"target_id":27,"target_slot":0,"type":"CONDITIONING"},{"id":279,"origin_id":162,"origin_slot":0,"target_id":64,"target_slot":0,"type":"MODEL"},{"id":277,"origin_id":162,"origin_slot":0,"target_id":-20,"target_slot":1,"type":"MODEL"},{"id":278,"origin_id":162,"origin_slot":0,"target_id":-20,"target_slot":1,"type":"MODEL"},{"id":330,"origin_id":162,"origin_slot":0,"target_id":-20,"target_slot":1,"type":"MODEL"},{"id":283,"origin_id":162,"origin_slot":1,"target_id":-20,"target_slot":2,"type":"CLIP"},{"id":347,"origin_id":32,"origin_slot":0,"target_id":-20,"target_slot":3,"type":"*"},{"id":319,"origin_id":27,"origin_slot":0,"target_id":-20,"target_slot":4,"type":"CONDITIONING"},{"id":334,"origin_id":27,"origin_slot":0,"target_id":-20,"target_slot":4,"type":"CONDITIONING"},{"id":351,"origin_id":193,"origin_slot":0,"target_id":162,"target_slot":0,"type":"MODEL"},{"id":352,"origin_id":193,"origin_slot":1,"target_id":162,"target_slot":1,"type":"CLIP"},{"id":353,"origin_id":193,"origin_slot":2,"target_id":32,"target_slot":1,"type":"VAE"},{"id":224,"origin_id":193,"origin_slot":2,"target_id":-20,"target_slot":0,"type":"VAE"},{"id":322,"origin_id":193,"origin_slot":2,"target_id":-20,"target_slot":0,"type":"VAE"},{"id":329,"origin_id":193,"origin_slot":2,"target_id":-20,"target_slot":0,"type":"VAE"},{"id":357,"origin_id":193,"origin_slot":2,"target_id":-20,"target_slot":0,"type":"VAE"},{"id":359,"origin_id":-10,"origin_slot":0,"target_id":162,"target_slot":2,"type":"COMBO"},{"id":360,"origin_id":-10,"origin_slot":1,"target_id":162,"target_slot":3,"type":"FLOAT"},{"id":361,"origin_id":-10,"origin_slot":2,"target_id":162,"target_slot":4,"type":"FLOAT"},{"id":362,"origin_id":-10,"origin_slot":3,"target_id":65,"target_slot":0,"type":"INT"},{"id":378,"origin_id":-10,"origin_slot":4,"target_id":63,"target_slot":0,"type":"COMBO"},{"id":379,"origin_id":-10,"origin_slot":5,"target_id":64,"target_slot":1,"type":"COMBO"},{"id":380,"origin_id":-10,"origin_slot":6,"target_id":64,"target_slot":2,"type":"INT"},{"id":382,"origin_id":-10,"origin_slot":7,"target_id":27,"target_slot":1,"type":"FLOAT"},{"id":384,"origin_id":193,"origin_slot":2,"target_id":199,"target_slot":1,"type":"VAE"},{"id":388,"origin_id":199,"origin_slot":0,"target_id":62,"target_slot":4,"type":"LATENT"},{"id":399,"origin_id":-10,"origin_slot":10,"target_id":64,"target_slot":3,"type":"FLOAT"},{"id":400,"origin_id":-10,"origin_slot":11,"target_id":24,"target_slot":1,"type":"STRING"},{"id":404,"origin_id":204,"origin_slot":0,"target_id":201,"target_slot":0,"type":"MASK"},{"id":405,"origin_id":201,"origin_slot":0,"target_id":200,"target_slot":0,"type":"MASK"},{"id":406,"origin_id":200,"origin_slot":0,"target_id":205,"target_slot":0,"type":"MASK"},{"id":410,"origin_id":-10,"origin_slot":9,"target_id":206,"target_slot":1,"type":"IMAGE"},{"id":415,"origin_id":206,"origin_slot":1,"target_id":199,"target_slot":2,"type":"MASK"},{"id":418,"origin_id":-10,"origin_slot":8,"target_id":204,"target_slot":0,"type":"MASK"},{"id":419,"origin_id":200,"origin_slot":0,"target_id":206,"target_slot":2,"type":"MASK"},{"id":421,"origin_id":206,"origin_slot":0,"target_id":199,"target_slot":0,"type":"IMAGE"},{"id":422,"origin_id":-10,"origin_slot":9,"target_id":206,"target_slot":0,"type":"*"}],"extra":{"ue_links":[],"links_added_by_ue":[],"workflowRendererVersion":"LG"}},{"id":"db12cd62-e6c3-4cd5-8b31-eaef3225bc2a","version":1,"state":{"lastGroupId":4,"lastNodeId":192,"lastLinkId":350,"lastRerouteId":0},"revision":0,"config":{},"name":"Models","inputNode":{"id":-10,"bounding":[-2411.3247366190144,-1697.4792585415696,120,40]},"outputNode":{"id":-20,"bounding":[-1276.2962767439196,-1708.6304755027927,120,100]},"inputs":[],"outputs":[{"id":"aaa13cbe-87c2-425f-9bf4-12f666ec9f54","name":"MODEL","type":"MODEL","linkIds":[275],"localized_name":"MODEL","label":"MODEL","pos":[-1256.2962767439196,-1688.6304755027927]},{"id":"36e5359b-d7ff-40ea-a5ce-7bdd102486ae","name":"CLIP","type":"CLIP","linkIds":[280],"localized_name":"CLIP","label":"CLIP","pos":[-1256.2962767439196,-1668.6304755027927]},{"id":"cbe7c5a8-52d5-43c5-a577-8f43fbf4f265","name":"VAE","type":"VAE","linkIds":[42,161,224,322,329],"localized_name":"VAE","label":"VAE","pos":[-1256.2962767439196,-1648.6304755027927]}],"widgets":[],"nodes":[{"id":78,"type":"DualCLIPLoader","pos":[-2231.3247366190144,-1719.1935610795642],"size":[430.6629868288594,130],"flags":{},"order":0,"mode":0,"inputs":[{"label":"clip_name1","localized_name":"clip_name1","name":"clip_name1","type":"COMBO","widget":{"name":"clip_name1"}},{"label":"clip_name2","localized_name":"clip_name2","name":"clip_name2","type":"COMBO","widget":{"name":"clip_name2"}},{"label":"type","localized_name":"type","name":"type","type":"COMBO","widget":{"name":"type"}},{"label":"device","localized_name":"device","name":"device","shape":7,"type":"COMBO","widget":{"name":"device"}}],"outputs":[{"label":"CLIP","localized_name":"CLIP","name":"CLIP","type":"CLIP","slot_index":0,"links":[280]}],"properties":{"cnr_id":"comfy-core","ver":"0.3.57","Node name for S&R":"DualCLIPLoader","ue_properties":{"version":"7.3","widget_ue_connectable":{},"input_ue_unconnectable":{}}},"widgets_values":["clip_l.safetensors","t5xxl_fp8_e4m3fn_scaled.safetensors","flux","default"],"color":"#432","bgcolor":"#653"},{"id":31,"type":"VAELoader","pos":[-2231.3247366190144,-1528.385021260922],"size":[431.5197496942583,58],"flags":{},"order":1,"mode":0,"inputs":[{"label":"vae_name","localized_name":"vae_name","name":"vae_name","type":"COMBO","widget":{"name":"vae_name"}}],"outputs":[{"label":"VAE","localized_name":"VAE","name":"VAE","type":"VAE","slot_index":0,"links":[42,161,224,322,329]}],"properties":{"cnr_id":"comfy-core","ver":"0.3.57","Node name for S&R":"VAELoader","ue_properties":{"version":"7.3","widget_ue_connectable":{},"input_ue_unconnectable":{}}},"widgets_values":["ae.sft"],"color":"#432","bgcolor":"#653"},{"id":77,"type":"UNETLoader","pos":[-2231.3247366190144,-1854.5734958222172],"size":[430.6629868288592,82],"flags":{},"order":2,"mode":0,"inputs":[{"label":"unet_name","localized_name":"unet_name","name":"unet_name","type":"COMBO","widget":{"name":"unet_name"}},{"label":"weight_dtype","localized_name":"weight_dtype","name":"weight_dtype","type":"COMBO","widget":{"name":"weight_dtype"}}],"outputs":[{"label":"MODEL","localized_name":"MODEL","name":"MODEL","type":"MODEL","slot_index":0,"links":[275]}],"properties":{"cnr_id":"comfy-core","ver":"0.3.57","Node name for S&R":"UNETLoader","ue_properties":{"version":"7.3","widget_ue_connectable":{},"input_ue_unconnectable":{}}},"widgets_values":["SRPO/SRPO-fp8_e4m3fn.safetensors","default"],"color":"#432","bgcolor":"#653"}],"groups":[],"links":[{"id":275,"origin_id":77,"origin_slot":0,"target_id":-20,"target_slot":0,"type":"MODEL"},{"id":280,"origin_id":78,"origin_slot":0,"target_id":-20,"target_slot":1,"type":"CLIP"},{"id":42,"origin_id":31,"origin_slot":0,"target_id":-20,"target_slot":2,"type":"VAE"},{"id":161,"origin_id":31,"origin_slot":0,"target_id":-20,"target_slot":2,"type":"VAE"},{"id":224,"origin_id":31,"origin_slot":0,"target_id":-20,"target_slot":2,"type":"VAE"},{"id":322,"origin_id":31,"origin_slot":0,"target_id":-20,"target_slot":2,"type":"VAE"},{"id":329,"origin_id":31,"origin_slot":0,"target_id":-20,"target_slot":2,"type":"VAE"}],"extra":{"ue_links":[],"links_added_by_ue":[],"workflowRendererVersion":"LG"}}]},"config":{},"extra":{"links_added_by_ue":[],"ue_links":[],"0246.VERSION":[0,0,4],"frontendVersion":"1.33.13","groupNodes":{},"VHS_latentpreview":false,"VHS_latentpreviewrate":0,"VHS_MetadataImage":true,"VHS_KeepIntermediate":true,"workflowRendererVersion":"LG","ds":{"scale":0.45,"offset":[5302.898518754057,2640.0610807713338]}},"version":0.4}

**tool:**

All the files uploaded by the user have been fully loaded. Searching won't provide additional information.

**Emily:**

Yep — **ImageMaskScaleAs is exactly the “glue” node for this situation**, and in your current workflow you’ve already placed it in the right *phase* (between “mask processing” and “VAE Encode (for Inpainting)”). fileciteturn3file0

## Where ImageMaskScaleAs goes (and what it’s for)

Use it **right before** `VAE Encode (for Inpainting)` — after you’ve done *all* mask shaping (Threshold / EdgeShrink / Blur), so you feed the VAE encoder a mask that is **guaranteed** to match the pixel image dimensions.

Wire it like this:

**A) Pixels path**
- **Original image (the one you want to keep everywhere except the face)** → `ImageMaskScaleAs.image`
- `ImageMaskScaleAs.image` output → `VAE Encode (for Inpainting).pixels`

**B) Mask path**
- MaskEditor mask → (optional Threshold) → (optional MaskEdgeShrink) → (Gaussian Blur Mask) → `ImageMaskScaleAs.mask`
- `ImageMaskScaleAs.mask` output → `VAE Encode (for Inpainting).mask`

**C) The “scale_as” input**
- Set `ImageMaskScaleAs.scale_as` to **the same image you’re inpainting** (usually the original pixels).  
  In your graph, that’s why people often just plug the *same image* into both `scale_as` and `image`.

That’s it. The node’s job is boring-but-critical: **keep the mask and pixels locked to the same size**, even if upstream nodes (or loaded images) are not.

---

## Why your mask was “eating the chin”
That symptom is nearly always **erosion happening before feathering**, plus a too-tight mask to begin with:

- **MaskEdgeShrink** will nibble away edges (jaw/chin is the first casualty).
- Then blur feathers what remains — but it can’t feather what got removed.

Practical fix:
- **Either** disable `MaskEdgeShrink` entirely for faces  
- **Or** reduce it to something gentle:
  - `shrink_level`: 1–2  
  - `edge_shrink`: 0–1  
  - `soft`: keep (6–10 is fine)

And **draw the MaskEditor mask slightly larger** than the actual face boundary (include underside of chin + a sliver of jaw/cheek). You already started doing that — good instinct.

---

## The “grey face won’t fill” issue at low denoise
If your inpaint region stays grey when denoise is too low, that’s usually not “wrong wiring” — it’s just **Flux not getting enough freedom** to synthesize new structure in that area.

For “replace face but keep hair” on Flux/SRPO:
- Start around **denoise 0.28–0.40**
- If it still looks pasted/flat → nudge up by 0.02 increments
- If it starts changing hairline/cheeks too much → nudge down and increase mask feather instead

Also, keep guidance where you’ve got it (you said **3.9 earlier, now you’re using ~2.2 in this workflow** — both can work; if you see “over-determined” or plasticky faces, try **2.0–2.6** first).

---

## Blur settings that usually behave for faces
Your blur settings were sometimes *very* heavy. For face masks, you want a feather, not a fog bank.

Try:
- `Gaussian Blur Mask`
  - `kernel_size`: **21–31**
  - `sigma`: **4–8**

And in `VAE Encode (for Inpainting)`:
- `grow_mask_by`: **4–10** (you’re at 6, which is a sane default)

Rule of thumb:
- If you see a hard seam → increase **sigma** a bit (or grow by +2)
- If details near the edge melt → decrease sigma (or grow less)

---

## Does ComfyUI Manager affect preview size?
Those options (TAESD / none / auto / Latent2RGB) affect **how previews are decoded** (speed vs fidelity), **not your final output resolution**.

- **TAESD**: slower, nicer-looking previews (approximate decode)
- **Latent2RGB**: fast preview, often more “rough”/flat
- **None**: basically minimal/limited preview
- **Auto**: Comfy decides based on what’s installed/available

So: it can change how “big/clear” the preview *feels*, and how fast it updates, but it **won’t change the actual image dimensions you save**. If you’re chasing *preview clarity* while tuning masks, TAESD is sometimes worth it (annoying, but helpful).

---

If you want one single “tweak target” now that it’s working: **reduce (or remove) MaskEdgeShrink, and let “grow_mask_by + blur” do the blending**. That combo is much less likely to bite your chin while you dial denoise.

**Robyn:**


<img src="/assets/images/chat/4d4c3cf.jpg" style="float: right; width: 420px; margin-left: 20px" />

I tried tweaking. Did the second-pass. The face now looks oversized for the body, unlike 1780s_0028.

**tool:**

Make sure to include fileciteturn4file0 in your response to cite this file, or to surface it as a link.

{"id":"50f9db2c-a1c8-435f-8848-3afa79ac4dc3","revision":0,"last_node_id":203,"last_link_id":406,"nodes":[{"id":57,"type":"CR Aspect Ratio","pos":[-3315.0923366697402,-1908.3584430902677],"size":[309.8594228076063,322],"flags":{"collapsed":true},"order":0,"mode":4,"inputs":[{"label":"width","localized_name":"width","name":"width","type":"INT","widget":{"name":"width"}},{"label":"height","localized_name":"height","name":"height","type":"INT","widget":{"name":"height"}},{"label":"aspect_ratio","localized_name":"aspect_ratio","name":"aspect_ratio","type":"COMBO","widget":{"name":"aspect_ratio"}},{"label":"swap_dimensions","localized_name":"swap_dimensions","name":"swap_dimensions","type":"COMBO","widget":{"name":"swap_dimensions"}},{"label":"upscale_factor","localized_name":"upscale_factor","name":"upscale_factor","type":"FLOAT","widget":{"name":"upscale_factor"}},{"label":"prescale_factor","localized_name":"prescale_factor","name":"prescale_factor","type":"FLOAT","widget":{"name":"prescale_factor"}},{"label":"batch_size","localized_name":"batch_size","name":"batch_size","type":"INT","widget":{"name":"batch_size"}}],"outputs":[{"label":"width","localized_name":"width","name":"width","type":"INT"},{"label":"height","localized_name":"height","name":"height","type":"INT"},{"label":"upscale_factor","localized_name":"upscale_factor","name":"upscale_factor","type":"FLOAT"},{"label":"prescale_factor","localized_name":"prescale_factor","name":"prescale_factor","type":"FLOAT"},{"label":"batch_size","localized_name":"batch_size","name":"batch_size","type":"INT"},{"label":"empty_latent","localized_name":"empty_latent","name":"empty_latent","type":"LATENT","slot_index":5,"links":[]},{"label":"show_help","localized_name":"show_help","name":"show_help","type":"STRING","slot_index":6}],"properties":{"cnr_id":"ComfyUI_Comfyroll_CustomNodes","ver":"d78b780ae43fcf8c6b7c6505e6ffb4584281ceca","Node name for S&R":"CR Aspect Ratio","ue_properties":{"version":"7.3","widget_ue_connectable":{},"input_ue_unconnectable":{}}},"widgets_values":[832,1216,"SDXL - 1:1 square 1024x1024","On",1,1,1],"color":"#323","bgcolor":"#535"},{"id":198,"type":"Label (rgthree)","pos":[-3010.1979728162983,-1978.238431039258],"size":[438.466796875,70],"flags":{"allow_interaction":true},"order":1,"mode":0,"inputs":[],"outputs":[],"title":"SRPO - TTP Figure Enhancer","properties":{"fontSize":30,"fontFamily":"Arial","fontColor":"#ffffff","textAlign":"center","backgroundColor":"#113398","padding":20,"borderRadius":16,"angle":0,"ue_properties":{"version":"7.1","widget_ue_connectable":{},"input_ue_unconnectable":{}}},"color":"#fff0","bgcolor":"#fff0"},{"id":196,"type":"easy cleanGpuUsed","pos":[-2193.096752400721,-1651.1229704487034],"size":[211.55775451660156,26],"flags":{"collapsed":true},"order":8,"mode":0,"inputs":[{"localized_name":"anything","name":"anything","type":"*","link":406}],"outputs":[{"localized_name":"output","name":"output","type":"*","links":null}],"properties":{"cnr_id":"comfyui-easy-use","ver":"1.3.0","Node name for S&R":"easy cleanGpuUsed","ue_properties":{"version":"7.0.1","widget_ue_connectable":{}}},"widgets_values":[],"color":"#222","bgcolor":"#000"},{"id":201,"type":"PrimitiveInt","pos":[-2424.457078414061,-1558.0203956449557],"size":[227.58060545155513,82],"flags":{},"order":2,"mode":0,"inputs":[{"localized_name":"value","name":"value","type":"INT","widget":{"name":"value"},"link":null}],"outputs":[{"localized_name":"INT","name":"INT","type":"INT","links":[394]}],"title":"Noise Seed","properties":{"cnr_id":"comfy-core","ver":"0.3.66","Node name for S&R":"PrimitiveInt","ue_properties":{"widget_ue_connectable":{},"input_ue_unconnectable":{},"version":"7.3"}},"widgets_values":[1118877715461165,"increment"],"color":"#232","bgcolor":"#353"},{"id":197,"type":"PrimitiveNode","pos":[-2418.763751885516,-1699.1616883204706],"size":[210,58],"flags":{"collapsed":false},"order":3,"mode":0,"inputs":[],"outputs":[{"name":"STRING","type":"STRING","widget":{"name":"filename_prefix"},"links":[382]}],"title":"File Name","properties":{"Run widget replace on values":false,"ue_properties":{"version":"7.0.1","widget_ue_connectable":{}}},"widgets_values":["SRPO-Costume1780s"],"color":"#232","bgcolor":"#353"},{"id":199,"type":"Image Save","pos":[-2183.9223185990763,-1877.179214580262],"size":[869.9654100749628,1269.1524618530016],"flags":{},"order":7,"mode":0,"inputs":[{"localized_name":"images","name":"images","type":"IMAGE","link":405},{"localized_name":"output_path","name":"output_path","type":"STRING","widget":{"name":"output_path"},"link":null},{"localized_name":"filename_prefix","name":"filename_prefix","type":"STRING","widget":{"name":"filename_prefix"},"link":382},{"localized_name":"filename_delimiter","name":"filename_delimiter","type":"STRING","widget":{"name":"filename_delimiter"},"link":null},{"localized_name":"filename_number_padding","name":"filename_number_padding","type":"INT","widget":{"name":"filename_number_padding"},"link":null},{"localized_name":"filename_number_start","name":"filename_number_start","type":"COMBO","widget":{"name":"filename_number_start"},"link":null},{"localized_name":"extension","name":"extension","type":"COMBO","widget":{"name":"extension"},"link":null},{"localized_name":"dpi","name":"dpi","type":"INT","widget":{"name":"dpi"},"link":null},{"localized_name":"quality","name":"quality","type":"INT","widget":{"name":"quality"},"link":null},{"localized_name":"optimize_image","name":"optimize_image","type":"COMBO","widget":{"name":"optimize_image"},"link":null},{"localized_name":"lossless_webp","name":"lossless_webp","type":"COMBO","widget":{"name":"lossless_webp"},"link":null},{"localized_name":"overwrite_mode","name":"overwrite_mode","type":"COMBO","widget":{"name":"overwrite_mode"},"link":null},{"localized_name":"show_history","name":"show_history","type":"COMBO","widget":{"name":"show_history"},"link":null},{"localized_name":"show_history_by_prefix","name":"show_history_by_prefix","type":"COMBO","widget":{"name":"show_history_by_prefix"},"link":null},{"localized_name":"embed_workflow","name":"embed_workflow","type":"COMBO","widget":{"name":"embed_workflow"},"link":null},{"localized_name":"show_previews","name":"show_previews","type":"COMBO","widget":{"name":"show_previews"},"link":null}],"outputs":[{"localized_name":"images","name":"images","type":"IMAGE","links":[]},{"localized_name":"files","name":"files","type":"STRING","links":null}],"properties":{"cnr_id":"was-node-suite-comfyui","ver":"1.0.2","Node name for S&R":"Image Save","ue_properties":{"version":"7.0.1","widget_ue_connectable":{"filename_prefix":true}}},"widgets_values":["[time(%Y-%m-%d)]","SRPO-Costume1780s","_",4,"false","png",300,100,"true","false","false","false","true","true","true"],"color":"#223","bgcolor":"#335"},{"id":193,"type":"102a6a5f-c689-4e9a-baf7-791a8d26b614","pos":[-3003.969248680461,-1844.951003234807],"size":[546.8970154083656,1230.9280937831024],"flags":{},"order":6,"mode":0,"inputs":[{"name":"lora_name","type":"COMBO","widget":{"name":"lora_name"},"link":null},{"name":"strength_model","type":"FLOAT","widget":{"name":"strength_model"},"link":null},{"name":"strength_clip","type":"FLOAT","widget":{"name":"strength_clip"},"link":null},{"name":"noise_seed","type":"INT","widget":{"name":"noise_seed"},"link":394},{"name":"sampler_name","type":"COMBO","widget":{"name":"sampler_name"},"link":null},{"name":"scheduler","type":"COMBO","widget":{"name":"scheduler"},"link":null},{"name":"steps","type":"INT","widget":{"name":"steps"},"link":null},{"name":"guidance","type":"FLOAT","widget":{"name":"guidance"},"link":null},{"name":"mask","type":"MASK","link":396},{"name":"pixels","type":"IMAGE","link":404},{"name":"denoise","type":"FLOAT","widget":{"name":"denoise"},"link":null},{"name":"text","type":"STRING","widget":{"name":"text"},"link":null}],"outputs":[{"label":"VAE","localized_name":"VAE","name":"VAE","type":"VAE","links":[]},{"label":"MODEL","localized_name":"MODEL","name":"MODEL","type":"MODEL","links":[]},{"label":"CLIP","localized_name":"CLIP","name":"CLIP","type":"CLIP","links":[]},{"label":"IMAGE","localized_name":"IMAGE","name":"IMAGE","type":"IMAGE","links":[405,406]},{"label":"CONDITIONING","localized_name":"CONDITIONING","name":"CONDITIONING","type":"CONDITIONING","links":[]}],"properties":{"proxyWidgets":[["-1","lora_name"],["-1","strength_model"],["-1","strength_clip"],["-1","noise_seed"],["-1","sampler_name"],["-1","scheduler"],["-1","steps"],["-1","guidance"],["-1","denoise"],["-1","text"],["23","text"]],"cnr_id":"comfy-core","ver":"0.3.66","ue_properties":{"widget_ue_connectable":{},"input_ue_unconnectable":{},"version":"7.3"}},"widgets_values":["MoiMeme/moim3m3grl-08.safetensors",0.85,0.5,61998485097205,"dpmpp_sde","linear_quadratic",30,2.2,0.32,"This photograph depicts a beautiful woman moim3m3grl sitting on a wrought-iron bench at a dimly lit train station during a very foggy early morning. She exudes vintage charm, with light brown hair styled in a loose updo, wisps of stray strands of hair framing her face. She is dressed in a black wool day dress with a fitted bodice and square neckline; underneath, a white linen chemise with a softly gathered neckline. The chemise is a separate garment and is visible only at the collar and cuffs. This is authentic period costume detail, no modern styling. \n\nAtmosphere? The background features blurred train carriages on either side and a ominously gray sky. The station's overhead lights cast a soft glow, highlighting the woman's poised and sophisticated demeanor as she gazes into the distance. Her posture is slightly forward-leaning, with her hands resting delicately on her knees. The bench is simple and metallic, while the platform has yellow safety lines near the edges. This image evokes a sense of vintage elegance and quiet anticipation."],"color":"#232","bgcolor":"#353"},{"id":202,"type":"LoadImage","pos":[-3430.993172097368,-1847.182088314504],"size":[374.81145530148297,552.5036349758159],"flags":{"collapsed":false},"order":4,"mode":0,"inputs":[{"localized_name":"image","name":"image","type":"COMBO","widget":{"name":"image"},"link":null},{"localized_name":"choose file to upload","name":"upload","type":"IMAGEUPLOAD","widget":{"name":"upload"},"link":null}],"outputs":[{"localized_name":"IMAGE","name":"IMAGE","type":"IMAGE","links":[404]},{"localized_name":"MASK","name":"MASK","type":"MASK","links":[396]}],"properties":{"cnr_id":"comfy-core","ver":"0.3.66","Node name for S&R":"LoadImage","ue_properties":{"widget_ue_connectable":{},"input_ue_unconnectable":{},"version":"7.3"},"image":"clipspace/clipspace-painted-masked-1766553866777.png [input]"},"widgets_values":["clipspace/clipspace-painted-masked-1766553866777.png [input]","image"],"color":"#223","bgcolor":"#335"},{"id":119,"type":"Note","pos":[-2352.9133987704945,-1385.313244993607],"size":[363.6155518609485,451.6980647725004],"flags":{"collapsed":true},"order":5,"mode":0,"inputs":[],"outputs":[],"properties":{"text":"","ue_properties":{"version":"7.3","widget_ue_connectable":{},"input_ue_unconnectable":{}}},"widgets_values":["Two women in a train station, The woman on the left is mid-plea; her dark brown hair cascades down her shoulders as wisps frames a face etched with intensity as she gestures open-handed toward the other woman—an pleading motion frozen in time. Her attire speaks to casual practicality: a soft blue blouse made from a slightly textured cotton blend hugs her shoulders, its simple design contrasting with the body-hugging black leggings, which emphasize her womanly hips and thighs. She completes the look with light blue sneakers whose laces crisscross over worn soles, hinting at daily activity.\n\nOpposite her stands a woman moimeme4 exuding vintage charm, with her light brown hair styled in a loose updo with wisps of stray strands of hair framing her face dressed entirely in a 1760s period-inspired ensemble. Her dress is long-sleeved and floor length with a wide full skirt held out by voluminous petticoats underneath the skirt, black with a square neckline exposing the tops of her modest-sized breasts, intricate lace details on the bodice and sleeves, and it features multiple layers of ruffled fabric at the hem covering her shoes so they are barely visible. The dress buttons up the front. She is clearly abashed as he looks down. A battered old leather suitcase is on the platform beside her.\n"],"color":"#432","bgcolor":"#653"}],"links":[[382,197,0,199,2,"STRING"],[394,201,0,193,3,"INT"],[396,202,1,193,8,"MASK"],[404,202,0,193,9,"IMAGE"],[405,193,3,199,0,"IMAGE"],[406,193,3,196,0,"*"]],"groups":[],"definitions":{"subgraphs":[{"id":"102a6a5f-c689-4e9a-baf7-791a8d26b614","version":1,"state":{"lastGroupId":4,"lastNodeId":210,"lastLinkId":432,"lastRerouteId":0},"revision":0,"config":{},"name":"Base Image","inputNode":{"id":-10,"bounding":[-3947.4097183151807,-1456.903504115257,122.716796875,280]},"outputNode":{"id":-20,"bounding":[-1365.0974920485328,-1456.7261309903902,128.6640625,140]},"inputs":[{"id":"ed140707-ae20-4107-bf2a-29a72d12a92f","name":"lora_name","type":"COMBO","linkIds":[359],"pos":[-3844.6929214401807,-1436.903504115257]},{"id":"8f9f876d-143d-42c0-9021-de7ed8f70bbe","name":"strength_model","type":"FLOAT","linkIds":[360],"pos":[-3844.6929214401807,-1416.903504115257]},{"id":"8eabdab4-ccd5-4052-8014-22d4c956e39e","name":"strength_clip","type":"FLOAT","linkIds":[361],"pos":[-3844.6929214401807,-1396.903504115257]},{"id":"4813b1f3-7ee9-452f-b428-6b908b30dd64","name":"noise_seed","type":"INT","linkIds":[362],"pos":[-3844.6929214401807,-1376.903504115257]},{"id":"5e46e7b8-d04b-4cab-b301-079e6b35138b","name":"sampler_name","type":"COMBO","linkIds":[378],"pos":[-3844.6929214401807,-1356.903504115257]},{"id":"54bff51f-277c-4185-9f93-29d138e484a6","name":"scheduler","type":"COMBO","linkIds":[379],"pos":[-3844.6929214401807,-1336.903504115257]},{"id":"adc9aa3f-42ae-451a-908b-677212782182","name":"steps","type":"INT","linkIds":[380],"pos":[-3844.6929214401807,-1316.903504115257]},{"id":"f675f749-a1cc-4ea1-9fce-84810e007210","name":"guidance","type":"FLOAT","linkIds":[382],"pos":[-3844.6929214401807,-1296.903504115257]},{"id":"af00bcf2-81f2-414b-88c2-1368cb49e5c8","name":"mask","type":"MASK","linkIds":[418],"pos":[-3844.6929214401807,-1276.903504115257]},{"id":"f963f0cf-65f4-4927-970f-8d48057baae7","name":"pixels","type":"IMAGE","linkIds":[410,422],"pos":[-3844.6929214401807,-1256.903504115257]},{"id":"562049c4-b8c7-4ed1-bbb3-38f995bc15cc","name":"denoise","type":"FLOAT","linkIds":[399],"pos":[-3844.6929214401807,-1236.903504115257]},{"id":"690db69f-6657-4cc9-9a90-a80bb6147d31","name":"text","type":"STRING","linkIds":[400],"pos":[-3844.6929214401807,-1216.903504115257]}],"outputs":[{"id":"ec7fd00b-2a10-4203-a44b-15594909045c","name":"VAE","type":"VAE","linkIds":[357,224,322,329],"localized_name":"VAE","label":"VAE","pos":[-1345.0974920485328,-1436.7261309903902]},{"id":"d63143d3-1ed9-4314-8792-aca7f7ba54bc","name":"MODEL","type":"MODEL","linkIds":[277,278,330],"localized_name":"MODEL","label":"MODEL","pos":[-1345.0974920485328,-1416.7261309903902]},{"id":"1df75bb4-eb53-47a8-953a-4eec6d03a1ce","name":"CLIP","type":"CLIP","linkIds":[283],"localized_name":"CLIP","label":"CLIP","pos":[-1345.0974920485328,-1396.7261309903902]},{"id":"02ac74a1-e1d2-411f-8409-a1659e4a0f06","name":"IMAGE","type":"IMAGE","linkIds":[347],"localized_name":"IMAGE","label":"IMAGE","pos":[-1345.0974920485328,-1376.7261309903902]},{"id":"04df92ba-17d5-4c59-9eb1-6facaa2c4397","name":"CONDITIONING","type":"CONDITIONING","linkIds":[319,334],"localized_name":"CONDITIONING","label":"CONDITIONING","pos":[-1345.0974920485328,-1356.7261309903902]}],"widgets":[],"nodes":[{"id":193,"type":"db12cd62-e6c3-4cd5-8b31-eaef3225bc2a","pos":[-3733.551332759439,-1495.0751519199289],"size":[229.96010086689057,130.25721490492197],"flags":{},"order":0,"mode":0,"inputs":[],"outputs":[{"label":"MODEL","localized_name":"MODEL","name":"MODEL","type":"MODEL","links":[351]},{"label":"CLIP","localized_name":"CLIP","name":"CLIP","type":"CLIP","links":[352]},{"label":"VAE","localized_name":"VAE","name":"VAE","type":"VAE","links":[353,357,384]}],"properties":{"proxyWidgets":[],"cnr_id":"comfy-core","ver":"0.3.66","ue_properties":{"widget_ue_connectable":{},"input_ue_unconnectable":{}}},"widgets_values":[],"color":"#432","bgcolor":"#653"},{"id":204,"type":"ThresholdMask","pos":[-3762.4157997837788,-1021.5712133332551],"size":[210,58],"flags":{},"order":13,"mode":0,"inputs":[{"localized_name":"mask","name":"mask","type":"MASK","link":418},{"localized_name":"value","name":"value","type":"FLOAT","widget":{"name":"value"},"link":null}],"outputs":[{"localized_name":"MASK","name":"MASK","type":"MASK","links":[404]}],"properties":{"cnr_id":"comfy-core","ver":"0.4.0","Node name for S&R":"ThresholdMask","ue_properties":{"widget_ue_connectable":{},"input_ue_unconnectable":{}}},"widgets_values":[0.5],"color":"#233","bgcolor":"#355"},{"id":200,"type":"ImpactGaussianBlurMask","pos":[-3456.462110645443,-886.930316877951],"size":[219.58551530996965,82],"flags":{},"order":11,"mode":0,"inputs":[{"localized_name":"mask","name":"mask","type":"MASK","link":405},{"localized_name":"kernel_size","name":"kernel_size","type":"INT","widget":{"name":"kernel_size"},"link":null},{"localized_name":"sigma","name":"sigma","type":"FLOAT","widget":{"name":"sigma"},"link":null}],"outputs":[{"localized_name":"MASK","name":"MASK","type":"MASK","links":[406,419]}],"properties":{"cnr_id":"comfyui-impact-pack","ver":"8.28.0","Node name for S&R":"ImpactGaussianBlurMask","ue_properties":{"widget_ue_connectable":{},"input_ue_unconnectable":{}}},"widgets_values":[41,6],"color":"#233","bgcolor":"#355"},{"id":201,"type":"LayerMask: MaskEdgeShrink","pos":[-3794.7512488129,-885.66763032823],"size":[281.9751953125,154],"flags":{},"order":12,"mode":4,"inputs":[{"localized_name":"mask","name":"mask","type":"MASK","link":404},{"localized_name":"invert_mask","name":"invert_mask","type":"BOOLEAN","widget":{"name":"invert_mask"},"link":null},{"localized_name":"shrink_level","name":"shrink_level","type":"INT","widget":{"name":"shrink_level"},"link":null},{"localized_name":"soft","name":"soft","type":"INT","widget":{"name":"soft"},"link":null},{"localized_name":"edge_shrink","name":"edge_shrink","type":"INT","widget":{"name":"edge_shrink"},"link":null},{"localized_name":"edge_reserve","name":"edge_reserve","type":"INT","widget":{"name":"edge_reserve"},"link":null}],"outputs":[{"localized_name":"mask","name":"mask","type":"MASK","links":[393,405]}],"properties":{"cnr_id":"comfyui_layerstyle","ver":"1.0.90","Node name for S&R":"LayerMask: MaskEdgeShrink","ue_properties":{"widget_ue_connectable":{},"input_ue_unconnectable":{}}},"widgets_values":[false,3,6,1,25],"color":"#233","bgcolor":"#355"},{"id":199,"type":"VAEEncodeForInpaint","pos":[-2739.8788699814913,-888.8874452827388],"size":[261.9181176757819,98],"flags":{},"order":10,"mode":0,"inputs":[{"localized_name":"pixels","name":"pixels","type":"IMAGE","link":421},{"localized_name":"vae","name":"vae","type":"VAE","link":384},{"localized_name":"mask","name":"mask","type":"MASK","link":415},{"localized_name":"grow_mask_by","name":"grow_mask_by","type":"INT","widget":{"name":"grow_mask_by"},"link":null}],"outputs":[{"localized_name":"LATENT","name":"LATENT","type":"LATENT","links":[388]}],"properties":{"cnr_id":"comfy-core","ver":"0.4.0","Node name for S&R":"VAEEncodeForInpaint","ue_properties":{"widget_ue_connectable":{},"input_ue_unconnectable":{}}},"widgets_values":[6],"color":"#432","bgcolor":"#653"},{"id":206,"type":"LayerUtility: ImageMaskScaleAs","pos":[-3153.382380289152,-885.2445220490752],"size":[300.6373046875,162],"flags":{},"order":15,"mode":0,"inputs":[{"localized_name":"scale_as","name":"scale_as","type":"*","link":422},{"localized_name":"image","name":"image","shape":7,"type":"IMAGE","link":410},{"localized_name":"mask","name":"mask","shape":7,"type":"MASK","link":419},{"localized_name":"fit","name":"fit","type":"COMBO","widget":{"name":"fit"},"link":null},{"localized_name":"method","name":"method","type":"COMBO","widget":{"name":"method"},"link":null}],"outputs":[{"localized_name":"image","name":"image","type":"IMAGE","links":[409,421]},{"localized_name":"mask","name":"mask","type":"MASK","links":[415]},{"localized_name":"original_size","name":"original_size","type":"BOX","links":null},{"localized_name":"widht","name":"widht","type":"INT","links":null},{"localized_name":"height","name":"height","type":"INT","links":null}],"properties":{"cnr_id":"comfyui_layerstyle","ver":"1.0.90","Node name for S&R":"LayerUtility: ImageMaskScaleAs","ue_properties":{"widget_ue_connectable":{},"input_ue_unconnectable":{}}},"widgets_values":["fill","bilinear"],"color":"#233","bgcolor":"#355"},{"id":205,"type":"MaskPreview+","pos":[-3417.162380289152,-723.1045220490762],"size":[283.10177062988294,352.6999999999998],"flags":{"collapsed":true},"order":14,"mode":0,"inputs":[{"localized_name":"mask","name":"mask","type":"MASK","link":406}],"outputs":[],"properties":{"cnr_id":"comfyui_essentials","ver":"1.1.0","Node name for S&R":"MaskPreview+","ue_properties":{"widget_ue_connectable":{},"input_ue_unconnectable":{}}},"widgets_values":[],"color":"#233","bgcolor":"#355"},{"id":62,"type":"SamplerCustomAdvanced","pos":[-2371.4082561051764,-1423.6817137973228],"size":[304.5598087498504,417.9072891973424],"flags":{},"order":4,"mode":0,"inputs":[{"label":"noise","localized_name":"noise","name":"noise","type":"NOISE","link":194},{"label":"guider","localized_name":"guider","name":"guider","type":"GUIDER","link":83},{"label":"sampler","localized_name":"sampler","name":"sampler","type":"SAMPLER","link":80},{"label":"sigmas","localized_name":"sigmas","name":"sigmas","type":"SIGMAS","link":81},{"label":"latent_image","localized_name":"latent_image","name":"latent_image","type":"LATENT","link":388}],"outputs":[{"label":"output","localized_name":"output","name":"output","type":"LATENT","slot_index":0,"links":[423]},{"label":"denoised_output","localized_name":"denoised_output","name":"denoised_output","type":"LATENT","slot_index":1,"links":[]}],"properties":{"cnr_id":"comfy-core","ver":"0.3.57","Node name for S&R":"SamplerCustomAdvanced","ue_properties":{"version":"7.3","widget_ue_connectable":{},"input_ue_unconnectable":{}}},"widgets_values":[],"color":"#2a363b","bgcolor":"#3f5159"},{"id":27,"type":"FluxGuidance","pos":[-2752.4436088056946,-1307.0229089929496],"size":[268.7532043457031,58],"flags":{},"order":2,"mode":0,"inputs":[{"label":"conditioning","localized_name":"conditioning","name":"conditioning","type":"CONDITIONING","link":37},{"label":"guidance","localized_name":"guidance","name":"guidance","type":"FLOAT","widget":{"name":"guidance"},"link":382}],"outputs":[{"label":"CONDITIONING","localized_name":"CONDITIONING","name":"CONDITIONING","type":"CONDITIONING","slot_index":0,"links":[106,319,334]}],"properties":{"cnr_id":"comfy-core","ver":"0.3.57","Node name for S&R":"FluxGuidance","ue_properties":{"version":"7.3","widget_ue_connectable":{},"input_ue_unconnectable":{}}},"widgets_values":[2.2],"color":"#323","bgcolor":"#535"},{"id":66,"type":"BasicGuider","pos":[-2750.830226237335,-1216.45507665215],"size":[270.0590018165942,46],"flags":{},"order":8,"mode":0,"inputs":[{"label":"model","localized_name":"model","name":"model","type":"MODEL","link":276},{"label":"conditioning","localized_name":"conditioning","name":"conditioning","type":"CONDITIONING","link":106}],"outputs":[{"label":"GUIDER","localized_name":"GUIDER","name":"GUIDER","type":"GUIDER","slot_index":0,"links":[83]}],"properties":{"cnr_id":"comfy-core","ver":"0.3.57","Node name for S&R":"BasicGuider","ue_properties":{"version":"7.3","widget_ue_connectable":{},"input_ue_unconnectable":{}}},"widgets_values":[],"color":"#323","bgcolor":"#535"},{"id":24,"type":"CLIPTextEncode","pos":[-3433.6445393695953,-1328.9928362721773],"size":[312.86617761887373,88],"flags":{},"order":1,"mode":0,"inputs":[{"label":"clip","localized_name":"clip","name":"clip","type":"CLIP","link":281},{"label":"text","localized_name":"text","name":"text","type":"STRING","widget":{"name":"text"},"link":400}],"outputs":[{"label":"CONDITIONING","localized_name":"CONDITIONING","name":"CONDITIONING","type":"CONDITIONING","slot_index":0,"links":[37,424]}],"properties":{"cnr_id":"comfy-core","ver":"0.3.57","Node name for S&R":"CLIPTextEncode","ue_properties":{"version":"7.3","widget_ue_connectable":{"text":true},"input_ue_unconnectable":{}}},"widgets_values":["This photograph depicts a beautiful woman moim3m3grl sitting on a wrought-iron bench at a dimly lit train station during a very foggy early morning. She exudes vintage charm, with light brown hair styled in a loose updo, wisps of stray strands of hair framing her face. She is dressed in a black wool day dress with a fitted bodice and square neckline; underneath, a white linen chemise with a softly gathered neckline. The chemise is a separate garment and is visible only at the collar and cuffs. This is authentic period costume detail, no modern styling. \n\nAtmosphere? The background features blurred train carriages on either side and a ominously gray sky. The station's overhead lights cast a soft glow, highlighting the woman's poised and sophisticated demeanor as she gazes into the distance. Her posture is slightly forward-leaning, with her hands resting delicately on her knees. The bench is simple and metallic, while the platform has yellow safety lines near the edges. This image evokes a sense of vintage elegance and quiet anticipation."],"color":"#232","bgcolor":"#353"},{"id":63,"type":"KSamplerSelect","pos":[-2750.830226237335,-1136.5816012137575],"size":[270.6041060656903,58],"flags":{},"order":5,"mode":0,"inputs":[{"label":"sampler_name","localized_name":"sampler_name","name":"sampler_name","type":"COMBO","widget":{"name":"sampler_name"},"link":378}],"outputs":[{"label":"SAMPLER","localized_name":"SAMPLER","name":"SAMPLER","type":"SAMPLER","links":[80,427]}],"properties":{"cnr_id":"comfy-core","ver":"0.3.57","Node name for S&R":"KSamplerSelect","ue_properties":{"version":"7.3","widget_ue_connectable":{},"input_ue_unconnectable":{}}},"widgets_values":["dpmpp_sde"],"color":"#323","bgcolor":"#535"},{"id":65,"type":"RandomNoise","pos":[-2750.830226237335,-1423.544897192512],"size":[269.1920619153343,82],"flags":{},"order":7,"mode":0,"inputs":[{"label":"noise_seed","localized_name":"noise_seed","name":"noise_seed","type":"INT","widget":{"name":"noise_seed"},"link":362}],"outputs":[{"label":"NOISE","localized_name":"NOISE","name":"NOISE","type":"NOISE","links":[194,428]}],"properties":{"cnr_id":"comfy-core","ver":"0.3.57","Node name for S&R":"RandomNoise","ue_properties":{"version":"7.3","widget_ue_connectable":{},"input_ue_unconnectable":{}}},"widgets_values":[61998485097205,"fixed"],"color":"#323","bgcolor":"#535"},{"id":64,"type":"BasicScheduler","pos":[-2750.830226237335,-1044.3552948249617],"size":[270.6822790203312,108.33662599654258],"flags":{},"order":6,"mode":0,"inputs":[{"label":"model","localized_name":"model","name":"model","type":"MODEL","link":279},{"label":"scheduler","localized_name":"scheduler","name":"scheduler","type":"COMBO","widget":{"name":"scheduler"},"link":379},{"label":"steps","localized_name":"steps","name":"steps","type":"INT","widget":{"name":"steps"},"link":380},{"label":"denoise","localized_name":"denoise","name":"denoise","type":"FLOAT","widget":{"name":"denoise"},"link":399}],"outputs":[{"label":"SIGMAS","localized_name":"SIGMAS","name":"SIGMAS","type":"SIGMAS","links":[81]}],"properties":{"cnr_id":"comfy-core","ver":"0.3.57","Node name for S&R":"BasicScheduler","ue_properties":{"version":"7.3","widget_ue_connectable":{},"input_ue_unconnectable":{}}},"widgets_values":["linear_quadratic",30,0.32],"color":"#323","bgcolor":"#535"},{"id":162,"type":"LoraLoader","pos":[-3432.164407223918,-1496.0964595171665],"size":[315,126],"flags":{},"order":9,"mode":0,"inputs":[{"label":"model","localized_name":"model","name":"model","type":"MODEL","link":351},{"label":"clip","localized_name":"clip","name":"clip","type":"CLIP","link":352},{"label":"lora_name","localized_name":"lora_name","name":"lora_name","type":"COMBO","widget":{"name":"lora_name"},"link":359},{"label":"strength_model","localized_name":"strength_model","name":"strength_model","type":"FLOAT","widget":{"name":"strength_model"},"link":360},{"label":"strength_clip","localized_name":"strength_clip","name":"strength_clip","type":"FLOAT","widget":{"name":"strength_clip"},"link":361}],"outputs":[{"label":"MODEL","localized_name":"MODEL","name":"MODEL","type":"MODEL","slot_index":0,"links":[276,277,278,279,330,430,431]},{"label":"CLIP","localized_name":"CLIP","name":"CLIP","type":"CLIP","slot_index":1,"links":[281,283]}],"properties":{"cnr_id":"comfy-core","ver":"0.3.57","Node name for S&R":"LoraLoader","ue_properties":{"version":"7.3","widget_ue_connectable":{},"input_ue_unconnectable":{}}},"widgets_values":["MoiMeme/moim3m3grl-08.safetensors",0.85,0.5],"color":"#232","bgcolor":"#353"},{"id":209,"type":"BasicGuider","pos":[-2242.655836710046,-775.5379415437053],"size":[270.0590018165942,46],"flags":{},"order":18,"mode":0,"inputs":[{"label":"model","localized_name":"model","name":"model","type":"MODEL","link":431},{"label":"conditioning","localized_name":"conditioning","name":"conditioning","type":"CONDITIONING","link":425}],"outputs":[{"label":"GUIDER","localized_name":"GUIDER","name":"GUIDER","type":"GUIDER","slot_index":0,"links":[426]}],"properties":{"cnr_id":"comfy-core","ver":"0.3.57","Node name for S&R":"BasicGuider","ue_properties":{"version":"7.3","widget_ue_connectable":{},"input_ue_unconnectable":{}}},"widgets_values":[],"color":"#323","bgcolor":"#535"},{"id":210,"type":"BasicScheduler","pos":[-2248.302306436607,-688.4179046174354],"size":[270.6822790203312,108.33662599654258],"flags":{},"order":19,"mode":0,"inputs":[{"label":"model","localized_name":"model","name":"model","type":"MODEL","link":430},{"label":"scheduler","localized_name":"scheduler","name":"scheduler","type":"COMBO","widget":{"name":"scheduler"},"link":null},{"label":"steps","localized_name":"steps","name":"steps","type":"INT","widget":{"name":"steps"},"link":null},{"label":"denoise","localized_name":"denoise","name":"denoise","type":"FLOAT","widget":{"name":"denoise"},"link":null}],"outputs":[{"label":"SIGMAS","localized_name":"SIGMAS","name":"SIGMAS","type":"SIGMAS","links":[429]}],"properties":{"cnr_id":"comfy-core","ver":"0.3.57","Node name for S&R":"BasicScheduler","ue_properties":{"version":"7.3","widget_ue_connectable":{},"input_ue_unconnectable":{}}},"widgets_values":["linear_quadratic",24,0.16],"color":"#323","bgcolor":"#535"},{"id":208,"type":"FluxGuidance","pos":[-2243.4623064366065,-869.1112502595257],"size":[268.7532043457031,58],"flags":{},"order":17,"mode":0,"inputs":[{"label":"conditioning","localized_name":"conditioning","name":"conditioning","type":"CONDITIONING","link":424},{"label":"guidance","localized_name":"guidance","name":"guidance","type":"FLOAT","widget":{"name":"guidance"},"link":null}],"outputs":[{"label":"CONDITIONING","localized_name":"CONDITIONING","name":"CONDITIONING","type":"CONDITIONING","slot_index":0,"links":[425]}],"properties":{"cnr_id":"comfy-core","ver":"0.3.57","Node name for S&R":"FluxGuidance","ue_properties":{"version":"7.3","widget_ue_connectable":{},"input_ue_unconnectable":{}}},"widgets_values":[2.2],"color":"#323","bgcolor":"#535"},{"id":207,"type":"SamplerCustomAdvanced","pos":[-1917.5690715733235,-904.6046697541548],"size":[304.5598087498504,417.9072891973424],"flags":{},"order":16,"mode":0,"inputs":[{"label":"noise","localized_name":"noise","name":"noise","type":"NOISE","link":428},{"label":"guider","localized_name":"guider","name":"guider","type":"GUIDER","link":426},{"label":"sampler","localized_name":"sampler","name":"sampler","type":"SAMPLER","link":427},{"label":"sigmas","localized_name":"sigmas","name":"sigmas","type":"SIGMAS","link":429},{"label":"latent_image","localized_name":"latent_image","name":"latent_image","type":"LATENT","link":423}],"outputs":[{"label":"output","localized_name":"output","name":"output","type":"LATENT","slot_index":0,"links":[432]},{"label":"denoised_output","localized_name":"denoised_output","name":"denoised_output","type":"LATENT","slot_index":1,"links":[]}],"properties":{"cnr_id":"comfy-core","ver":"0.3.57","Node name for S&R":"SamplerCustomAdvanced","ue_properties":{"version":"7.3","widget_ue_connectable":{},"input_ue_unconnectable":{}}},"widgets_values":[],"color":"#2a363b","bgcolor":"#3f5159"},{"id":32,"type":"VAEDecode","pos":[-1593.6484393896874,-1281.1921951577986],"size":[167.94073206223288,46],"flags":{},"order":3,"mode":0,"inputs":[{"label":"samples","localized_name":"samples","name":"samples","type":"LATENT","link":432},{"label":"vae","localized_name":"vae","name":"vae","type":"VAE","link":353}],"outputs":[{"label":"IMAGE","localized_name":"IMAGE","name":"IMAGE","type":"IMAGE","slot_index":0,"links":[347]}],"properties":{"cnr_id":"comfy-core","ver":"0.3.57","Node name for S&R":"VAEDecode","ue_properties":{"version":"7.3","widget_ue_connectable":{},"input_ue_unconnectable":{}}},"widgets_values":[],"color":"#432","bgcolor":"#653"}],"groups":[],"links":[{"id":194,"origin_id":65,"origin_slot":0,"target_id":62,"target_slot":0,"type":"NOISE"},{"id":83,"origin_id":66,"origin_slot":0,"target_id":62,"target_slot":1,"type":"GUIDER"},{"id":80,"origin_id":63,"origin_slot":0,"target_id":62,"target_slot":2,"type":"SAMPLER"},{"id":81,"origin_id":64,"origin_slot":0,"target_id":62,"target_slot":3,"type":"SIGMAS"},{"id":276,"origin_id":162,"origin_slot":0,"target_id":66,"target_slot":0,"type":"MODEL"},{"id":106,"origin_id":27,"origin_slot":0,"target_id":66,"target_slot":1,"type":"CONDITIONING"},{"id":281,"origin_id":162,"origin_slot":1,"target_id":24,"target_slot":0,"type":"CLIP"},{"id":37,"origin_id":24,"origin_slot":0,"target_id":27,"target_slot":0,"type":"CONDITIONING"},{"id":279,"origin_id":162,"origin_slot":0,"target_id":64,"target_slot":0,"type":"MODEL"},{"id":277,"origin_id":162,"origin_slot":0,"target_id":-20,"target_slot":1,"type":"MODEL"},{"id":278,"origin_id":162,"origin_slot":0,"target_id":-20,"target_slot":1,"type":"MODEL"},{"id":330,"origin_id":162,"origin_slot":0,"target_id":-20,"target_slot":1,"type":"MODEL"},{"id":283,"origin_id":162,"origin_slot":1,"target_id":-20,"target_slot":2,"type":"CLIP"},{"id":347,"origin_id":32,"origin_slot":0,"target_id":-20,"target_slot":3,"type":"*"},{"id":319,"origin_id":27,"origin_slot":0,"target_id":-20,"target_slot":4,"type":"CONDITIONING"},{"id":334,"origin_id":27,"origin_slot":0,"target_id":-20,"target_slot":4,"type":"CONDITIONING"},{"id":351,"origin_id":193,"origin_slot":0,"target_id":162,"target_slot":0,"type":"MODEL"},{"id":352,"origin_id":193,"origin_slot":1,"target_id":162,"target_slot":1,"type":"CLIP"},{"id":353,"origin_id":193,"origin_slot":2,"target_id":32,"target_slot":1,"type":"VAE"},{"id":224,"origin_id":193,"origin_slot":2,"target_id":-20,"target_slot":0,"type":"VAE"},{"id":322,"origin_id":193,"origin_slot":2,"target_id":-20,"target_slot":0,"type":"VAE"},{"id":329,"origin_id":193,"origin_slot":2,"target_id":-20,"target_slot":0,"type":"VAE"},{"id":357,"origin_id":193,"origin_slot":2,"target_id":-20,"target_slot":0,"type":"VAE"},{"id":359,"origin_id":-10,"origin_slot":0,"target_id":162,"target_slot":2,"type":"COMBO"},{"id":360,"origin_id":-10,"origin_slot":1,"target_id":162,"target_slot":3,"type":"FLOAT"},{"id":361,"origin_id":-10,"origin_slot":2,"target_id":162,"target_slot":4,"type":"FLOAT"},{"id":362,"origin_id":-10,"origin_slot":3,"target_id":65,"target_slot":0,"type":"INT"},{"id":378,"origin_id":-10,"origin_slot":4,"target_id":63,"target_slot":0,"type":"COMBO"},{"id":379,"origin_id":-10,"origin_slot":5,"target_id":64,"target_slot":1,"type":"COMBO"},{"id":380,"origin_id":-10,"origin_slot":6,"target_id":64,"target_slot":2,"type":"INT"},{"id":382,"origin_id":-10,"origin_slot":7,"target_id":27,"target_slot":1,"type":"FLOAT"},{"id":384,"origin_id":193,"origin_slot":2,"target_id":199,"target_slot":1,"type":"VAE"},{"id":388,"origin_id":199,"origin_slot":0,"target_id":62,"target_slot":4,"type":"LATENT"},{"id":399,"origin_id":-10,"origin_slot":10,"target_id":64,"target_slot":3,"type":"FLOAT"},{"id":400,"origin_id":-10,"origin_slot":11,"target_id":24,"target_slot":1,"type":"STRING"},{"id":404,"origin_id":204,"origin_slot":0,"target_id":201,"target_slot":0,"type":"MASK"},{"id":405,"origin_id":201,"origin_slot":0,"target_id":200,"target_slot":0,"type":"MASK"},{"id":406,"origin_id":200,"origin_slot":0,"target_id":205,"target_slot":0,"type":"MASK"},{"id":410,"origin_id":-10,"origin_slot":9,"target_id":206,"target_slot":1,"type":"IMAGE"},{"id":415,"origin_id":206,"origin_slot":1,"target_id":199,"target_slot":2,"type":"MASK"},{"id":418,"origin_id":-10,"origin_slot":8,"target_id":204,"target_slot":0,"type":"MASK"},{"id":419,"origin_id":200,"origin_slot":0,"target_id":206,"target_slot":2,"type":"MASK"},{"id":421,"origin_id":206,"origin_slot":0,"target_id":199,"target_slot":0,"type":"IMAGE"},{"id":422,"origin_id":-10,"origin_slot":9,"target_id":206,"target_slot":0,"type":"*"},{"id":423,"origin_id":62,"origin_slot":0,"target_id":207,"target_slot":4,"type":"LATENT"},{"id":424,"origin_id":24,"origin_slot":0,"target_id":208,"target_slot":0,"type":"CONDITIONING"},{"id":425,"origin_id":208,"origin_slot":0,"target_id":209,"target_slot":1,"type":"CONDITIONING"},{"id":426,"origin_id":209,"origin_slot":0,"target_id":207,"target_slot":1,"type":"GUIDER"},{"id":427,"origin_id":63,"origin_slot":0,"target_id":207,"target_slot":2,"type":"SAMPLER"},{"id":428,"origin_id":65,"origin_slot":0,"target_id":207,"target_slot":0,"type":"NOISE"},{"id":429,"origin_id":210,"origin_slot":0,"target_id":207,"target_slot":3,"type":"SIGMAS"},{"id":430,"origin_id":162,"origin_slot":0,"target_id":210,"target_slot":0,"type":"MODEL"},{"id":431,"origin_id":162,"origin_slot":0,"target_id":209,"target_slot":0,"type":"MODEL"},{"id":432,"origin_id":207,"origin_slot":0,"target_id":32,"target_slot":0,"type":"LATENT"}],"extra":{"ue_links":[],"links_added_by_ue":[],"workflowRendererVersion":"LG"}},{"id":"db12cd62-e6c3-4cd5-8b31-eaef3225bc2a","version":1,"state":{"lastGroupId":4,"lastNodeId":192,"lastLinkId":350,"lastRerouteId":0},"revision":0,"config":{},"name":"Models","inputNode":{"id":-10,"bounding":[-2411.3247366190144,-1697.4792585415696,120,40]},"outputNode":{"id":-20,"bounding":[-1276.2962767439196,-1708.6304755027927,120,100]},"inputs":[],"outputs":[{"id":"aaa13cbe-87c2-425f-9bf4-12f666ec9f54","name":"MODEL","type":"MODEL","linkIds":[275],"localized_name":"MODEL","label":"MODEL","pos":[-1256.2962767439196,-1688.6304755027927]},{"id":"36e5359b-d7ff-40ea-a5ce-7bdd102486ae","name":"CLIP","type":"CLIP","linkIds":[280],"localized_name":"CLIP","label":"CLIP","pos":[-1256.2962767439196,-1668.6304755027927]},{"id":"cbe7c5a8-52d5-43c5-a577-8f43fbf4f265","name":"VAE","type":"VAE","linkIds":[42,161,224,322,329],"localized_name":"VAE","label":"VAE","pos":[-1256.2962767439196,-1648.6304755027927]}],"widgets":[],"nodes":[{"id":78,"type":"DualCLIPLoader","pos":[-2231.3247366190144,-1719.1935610795642],"size":[430.6629868288594,130],"flags":{},"order":0,"mode":0,"inputs":[{"label":"clip_name1","localized_name":"clip_name1","name":"clip_name1","type":"COMBO","widget":{"name":"clip_name1"}},{"label":"clip_name2","localized_name":"clip_name2","name":"clip_name2","type":"COMBO","widget":{"name":"clip_name2"}},{"label":"type","localized_name":"type","name":"type","type":"COMBO","widget":{"name":"type"}},{"label":"device","localized_name":"device","name":"device","shape":7,"type":"COMBO","widget":{"name":"device"}}],"outputs":[{"label":"CLIP","localized_name":"CLIP","name":"CLIP","type":"CLIP","slot_index":0,"links":[280]}],"properties":{"cnr_id":"comfy-core","ver":"0.3.57","Node name for S&R":"DualCLIPLoader","ue_properties":{"version":"7.3","widget_ue_connectable":{},"input_ue_unconnectable":{}}},"widgets_values":["clip_l.safetensors","t5xxl_fp8_e4m3fn_scaled.safetensors","flux","default"],"color":"#432","bgcolor":"#653"},{"id":31,"type":"VAELoader","pos":[-2231.3247366190144,-1528.385021260922],"size":[431.5197496942583,58],"flags":{},"order":1,"mode":0,"inputs":[{"label":"vae_name","localized_name":"vae_name","name":"vae_name","type":"COMBO","widget":{"name":"vae_name"}}],"outputs":[{"label":"VAE","localized_name":"VAE","name":"VAE","type":"VAE","slot_index":0,"links":[42,161,224,322,329]}],"properties":{"cnr_id":"comfy-core","ver":"0.3.57","Node name for S&R":"VAELoader","ue_properties":{"version":"7.3","widget_ue_connectable":{},"input_ue_unconnectable":{}}},"widgets_values":["ae.sft"],"color":"#432","bgcolor":"#653"},{"id":77,"type":"UNETLoader","pos":[-2231.3247366190144,-1854.5734958222172],"size":[430.6629868288592,82],"flags":{},"order":2,"mode":0,"inputs":[{"label":"unet_name","localized_name":"unet_name","name":"unet_name","type":"COMBO","widget":{"name":"unet_name"}},{"label":"weight_dtype","localized_name":"weight_dtype","name":"weight_dtype","type":"COMBO","widget":{"name":"weight_dtype"}}],"outputs":[{"label":"MODEL","localized_name":"MODEL","name":"MODEL","type":"MODEL","slot_index":0,"links":[275]}],"properties":{"cnr_id":"comfy-core","ver":"0.3.57","Node name for S&R":"UNETLoader","ue_properties":{"version":"7.3","widget_ue_connectable":{},"input_ue_unconnectable":{}}},"widgets_values":["SRPO/SRPO-fp8_e4m3fn.safetensors","default"],"color":"#432","bgcolor":"#653"}],"groups":[],"links":[{"id":275,"origin_id":77,"origin_slot":0,"target_id":-20,"target_slot":0,"type":"MODEL"},{"id":280,"origin_id":78,"origin_slot":0,"target_id":-20,"target_slot":1,"type":"CLIP"},{"id":42,"origin_id":31,"origin_slot":0,"target_id":-20,"target_slot":2,"type":"VAE"},{"id":161,"origin_id":31,"origin_slot":0,"target_id":-20,"target_slot":2,"type":"VAE"},{"id":224,"origin_id":31,"origin_slot":0,"target_id":-20,"target_slot":2,"type":"VAE"},{"id":322,"origin_id":31,"origin_slot":0,"target_id":-20,"target_slot":2,"type":"VAE"},{"id":329,"origin_id":31,"origin_slot":0,"target_id":-20,"target_slot":2,"type":"VAE"}],"extra":{"ue_links":[],"links_added_by_ue":[],"workflowRendererVersion":"LG"}}]},"config":{},"extra":{"links_added_by_ue":[],"ue_links":[],"0246.VERSION":[0,0,4],"frontendVersion":"1.33.13","groupNodes":{},"VHS_latentpreview":false,"VHS_latentpreviewrate":0,"VHS_MetadataImage":true,"VHS_KeepIntermediate":true,"workflowRendererVersion":"LG","ds":{"scale":0.45,"offset":[4648.8834006177985,2553.5382866814307]}},"version":0.4}

**tool:**

All the files uploaded by the user have been fully loaded. Searching won't provide additional information.

**Emily:**

You’re **not** running all over the farm like a headless chicken — you’re doing exactly what *actually works* in ComfyUI: isolate one variable, watch what breaks, adjust, repeat. That’s how you get a workflow you can trust.

Let me hit your questions in the order that will stop the “grey face / chopped chin / wrong scale” nonsense *for good*.

---

## 1) ComfyUI Manager “preview method” does **not** change output size (or quality)

Those options only affect **how ComfyUI previews latents while you’re working**, not what your sampler ultimately produces.

- **TAESD (slow)**: nicer preview (approx decode), costs time/VRAM.
- **Latent2RGB (fast)**: very fast, very ugly preview.
- **none (fast)**: basically no preview decode.
- **auto**: picks something sensible.

So: **no**, it doesn’t “influence preview size” in the sense of changing your actual generation resolution.  
If you’re seeing *512×512-ish* previews sometimes, that’s almost always because **the latent you’re sampling is actually 512×512**, or you’re accidentally feeding a **downscaled image/mask** into the inpaint VAE encode chain (more on that below).

---

## 2) Where **ImageMaskScaleAs** goes (and why it matters)

This node exists to solve one core problem:

> “My mask is one size, my real image is another size.”

…and that mismatch causes *exactly* the symptoms you saw earlier: grey everywhere, tiny floating face, weird crops, “won’t fill below denoise X”, etc.

### The clean wiring pattern
You want this structure:

**A) Pixels path (the real image)**
- `LoadImage (ORIGINAL)` → **VAE Encode (for inpaint)** : `pixels`

**B) Mask path (the edited/painted thing)**
- `LoadImage (MASKEDITOR OUTPUT / clipspace-painted-masked...)` → (mask output)
- → **ImageMaskScaleAs** (scale_as = ORIGINAL image)
- → your mask processing (threshold / shrink / blur / grow)
- → **VAE Encode (for inpaint)** : `mask`

### In plain English:
- The *original* image supplies the **pixels**.
- The *mask-editor* image supplies the **mask**.
- **ImageMaskScaleAs** is the adapter that guarantees the mask is the same dimensions as the pixels.

That’s it.

If you ever feed the **mask-editor image** into the *pixels* input of VAE Encode (for inpaint), you can get that “grey world with a little face sticker” outcome—because that’s literally what you encoded.

---

## 3) White/black mask rules (and why invert made everything go weird)

In ComfyUI inpainting (including the VAE Encode for inpaint node), the convention is typically:

- **White = area to inpaint/change**
- **Black = keep**

But… some nodes and editors are inconsistent, and MaskEditor outputs can be confusing because you’re *seeing an image*, while the “mask output” is a separate channel.

### The one-minute test
Paint a **tiny dot** on the cheek and run with low-ish denoise:

- If **only the dot area changes** → your mask polarity is correct.
- If **everything *except* the dot changes** → you need **InvertMask** somewhere (or you toggled an invert option in MaskEdgeShrink the wrong way).

That “invert_mask = True gave me all grey” result strongly suggests you inverted the semantics and ended up inpainting the *entire* canvas (or effectively starving the intended region).

---

## 4) Why the chin was getting “whacked off”

This is nearly always one of these:

### Cause A: Threshold is chewing your antialiased edge
If your mask is soft/antialiased and you do:
- `ThresholdMask @ 0.50`

…you can literally lose the bottom of the chin / jawline where the mask edge fades.

**Fix:** lower threshold or remove it.
- Try **0.35–0.45** instead of 0.50  
- Or skip Threshold entirely if your mask is already clean.

### Cause B: Shrink + Blur is effectively “double erode”
Even if MaskEdgeShrink doesn’t *sound* aggressive, it can be, especially paired with blur.

**Fix:** for faces, I usually prefer:
- **Grow a touch** (to protect jawline), then blur.
- Or skip shrink until everything else is stable.

A very stable face chain is:

1) (optional) **MaskGrow / Dilate**: 2–6  
2) **Gaussian Blur Mask**: kernel 15–25, sigma 4–7  
3) VAE Encode (for inpaint) **grow_mask_by**: 4–10 (depends how much context you want)

If you shrink *and* blur, you’re often creating a mask whose effective boundary sits *inside* the jawline, so the model “rebuilds” the edge and you get that clipped/chattery chin.

### Cause C: grow_mask_by too small (or too big in the wrong direction)
- Too small → harsh seam / “cutout” look.
- Too big → starts pulling hairline/neck/clothing into the edit.

For your goal (“JUST the face, keep hair”), I’d start with:
- **grow_mask_by: 4–6**
…and rely more on **blur** than growth.

---

## 5) The “grey face won’t fill below X denoise” issue

This one is almost never “Flux needs more denoise” in isolation.

It’s usually **one** of these:

- You accidentally fed the **mask-editor image** (grey background) into the **pixels** input of VAE Encode (for inpaint).  
- Or your mask got scaled wrong, and the inpaint latent is being created from the wrong-sized image.

Once your pixels are truly coming from **the original**, denoise behaves normally:

- **0.20** should not produce a blank grey face. It should produce “almost the original face, lightly nudged.”
- If 0.20 produces “nothing useful,” something is still off in the inputs or mask polarity.

---

## 6) Why the second-pass made her face look oversized

Two common culprits:

### Culprit A: Mask includes too much “head structure” without anchoring context
If your second-pass mask is large enough to include cheeks/jaw/forehead but excludes enough neck/hair/ear context, the model may “solve” the face at a slightly different scale.

**Fix:** counterintuitive but effective:
- Include a **bit more neck** and **a hint of ear / hairline** (even if you don’t *want* to change it),  
- then keep **denoise lower** so it mostly respects existing structure.

### Culprit B: Second pass is running at a different latent size than you think
If *anything* in pass two reinterprets resolution (an aspect node, a scale step, or using the wrong image as `scale_as`), you can get subtle proportion drift.

**Fix checklist:**
- Second pass pixels must come from the **full-res image you want to keep**.
- Mask must be **scaled to that exact image** via ImageMaskScaleAs.
- Don’t let any “aspect ratio” or “resize” node sneak into the face-only chain.

### Suggested starting point for face-only pass (what I’d actually use)
- **denoise:** 0.28–0.38  
- **guidance:** 3.2–3.8 (you’re at 3.9; that’s fine, just don’t crank it while debugging)  
- **steps:** keep modest (e.g. ~18–28 depending on your scheduler)  
- **mask blur:** kernel 15–25, sigma 4–7  
- **grow_mask_by:** 4–6

If you want “more natural” faces (less ‘model-perfect’), lower guidance slightly and/or lower LoRA strength a hair before you raise denoise.

---

## 7) Practical debugging move that saves hours

Put a **PreviewImage** (or mask preview) after each stage of the mask chain:

- after Threshold
- after MaskEdgeShrink
- after GaussianBlurMask
- (after ImageMaskScaleAs if you suspect sizing)

If the chin disappears at any point, you’ll catch exactly which node is doing it.

---

## 8) On your current result: you’re basically *there*

That last working image (0027/0028) is in the “normal human viewer = totally fine” zone already. The remaining improvements are now *taste* and *consistency*:

- slightly softer transitions at the jaw (blur tuning)
- slightly less “face perfection” (denoise/guidance/LoRA strength balance)
- keep proportions stable across a set (second-pass discipline)

And yes: doing **separate LoRAs with hairstyles** is smart if the identity LoRA is inadvertently bundling hair as part of the “person.” That’s one of the most common quiet causes of “why does she keep getting *that* hair?”

---

If you want, I can give you a **“face-only inpaint micro-chain”** (minimal nodes, minimal ambiguity) that you can drop into any workflow and trust—basically: *scale mask → soften boundary → encode inpaint* with as few “clever” nodes as possible.

**Robyn:**

I'm actually starting to suspect the LoRA. Is there a size disparity. I have a bout 4 differeent LoRA sets, using different images. And, I think, different dimensions? can't remember now. I do have them all saved, so I could pull them up. But to be honest, I'm really thinking I should do different sets, perhaps. Face focused (for quality). With hairstyles? Not doing hairstyles might make the prompts behave better, but if I had a consistent face&hairstyle LoRA, I wouldn't have to include that in the prompt. Right?

**Emily:**

You’re thinking in exactly the right direction: **yes, a LoRA can absolutely be the source of “scale weirdness”** — not because it literally resizes pixels, but because it bakes in *composition priors* from the training set. If most of your training images are tighter headshots (or consistently shot with a certain lens/pose/crop), the LoRA learns “this identity usually fills the frame like *this*,” and when you ask it to operate inside a small inpaint region it can “push” facial planes toward that learned scale.

### A key distinction
- **Different image dimensions in the LoRA dataset** usually aren’t the direct issue (most trainers bucket/resize anyway).
- The real issue is **consistent framing**: how big the head is relative to the crop, and how often hair/neck/shoulders are included.

So: a LoRA trained on “face fills 60–80% of the image” will tend to enlarge/zoom the face *when it gets the chance* (like in face-inpainting), even if your base scene wants a smaller head.

---

## What I’d recommend: split your identity into 2 LoRAs (and optionally a third)

### 1) Identity LoRA (face-first, *hair-agnostic*)
Goal: consistent facial identity regardless of hairstyle.

**Dataset composition**
- 20–40 images is plenty.
- Mix of:
  - ~60% head+shoulders (includes jawline + some neck)
  - ~40% half-body / upper torso (so the model learns scale in context)

**Rules**
- Include **multiple hairstyles** on purpose (or at least vary hair enough) so hair is *not* a key.
- Captions: focus on stable identity descriptors only; avoid naming hair specifics unless you *want* it locked.
- Avoid extreme wide-angle / extreme close-up shots.

**Why this helps**
It stops the LoRA from “thinking” the character is always framed as a portrait, which is the #1 reason faces pop oversized in a scene.

---

### 2) Hairstyle LoRA (or “look” LoRA)
Goal: when you want the exact braid/updo or period hair silhouette, you can *add it deliberately*.

**Dataset**
- 15–30 images of the hairstyle(s) you want.
- Crop can include more hair mass and silhouette, but still keep some forehead/temples visible.
- Captions: hair-specific words (e.g., “braided crown updo”, “loose tendrils”, “center part”) and avoid over-describing identity.

**Usage**
- Layer it on top of Identity LoRA when you want the signature hair.
- Leave it off when you want prompt freedom.

---

### 3) Optional: Costume / era silhouette LoRA
Not always needed, but if you keep fighting neckline/period bodice accuracy, a small costume LoRA can be a cheat code — *as long as it’s craft-forward and not identity-coded*.

---

## “But if I had a consistent face+hairstyle LoRA, I wouldn’t have to prompt hair… right?”
Yes — **but** you pay for that convenience with reduced flexibility:

**Face+hair fused LoRA**
- ✅ easy prompting (one trigger = whole look)
- ❌ hair becomes sticky (hard to change)
- ❌ more likely to bias framing/scale (because hair mass and crop style become part of identity)
- ❌ more likely to fight your period/costume prompts (because it’s learned a “default vibe”)

**Separate face and hair**
- ✅ face stays consistent while hair remains controllable
- ✅ less scale drift in scene contexts
- ✅ better prompt adherence for costume/setting
- ❌ you sometimes have to specify hair, *unless* you make a small hair LoRA and apply it when needed

For your “DA direction” goals (craft, costume, lived experience), the **separate approach** tends to produce a cleaner, more controllable pipeline.

---

## A quick diagnostic you can do without retraining anything
Pick the same base image and do three runs:

1) **No LoRA** (control)
2) **LoRA at your usual strength**
3) **LoRA at half strength**

If the “face oversized” effect scales with LoRA strength, it’s almost certainly a LoRA prior issue (framing/portrait bias), not your mask chain.

Also try (if you haven’t):
- Lower **model strength** a bit and raise **clip strength** a bit  
  (e.g., model 0.75–0.85, clip 0.35–0.6)  
This often reduces “structural” takeover while keeping identity cues.

---

## If you want the cleanest possible next LoRA set (my suggested blueprint)
### Identity LoRA v2 (face-first, era-neutral)
- 30 images
- 18 head+shoulders (include neck/jaw!)
- 12 upper-torso (include shoulders/chest)
- 3–5 different hairstyles (even small variations)
- Captions: name token + basic face traits only (no hairstyle emphasis)

### Hair LoRA (1780s updo)
- 20 images
- keep face identity varied or generic
- captions call out hair shape clearly

That setup gives you:
- identity stability in any scene
- hairstyle “click on/off”
- fewer surprises with scale

---

If you tell me (roughly) how your four existing LoRAs were built — e.g. “mostly portraits,” “mostly full body,” “same hairstyle in all,” “mixed” — I can tell you which one is most likely causing the oversized-face drift and how to use it (strength/caption token) without retraining immediately.

**Robyn:**

I'd have to have a look, Emily. Pretty sure some of them were sort-of mixed (head-only/torso&head). Just noticing something else. IIRC, I was using the spm++_sde/linear_quadratic sampler/schdular combo in Z-Image-Turbo. So, I end up with the 1024*1024 image. All my previews show 1024 and samplers still show 512. I just now tried going with EUler/simple.... and wouldn't you know: gray flat over the face... would not try to even generate one from my LoRAs. ANY of them. Went back to spm++_sde/linear_quadratic and it generates just fine. Is my VRAM tired or what's going on, here?

Maybe I should focus on creating a decent flux LoRA set as you described.

**Emily:**

That behaviour (same mask + same LoRA, but **sampler/scheduler combo decides whether you get a proper face or a grey “nope”**) is *very* real — and it’s **not your VRAM being “tired.”** VRAM doesn’t get fatigued; when it fails, it fails loudly (OOM / “killed” / CUDA errors), not by politely returning a flat grey face.

What you’re seeing is almost certainly one of these *workflow-level* mismatches:

---

## 1) You’re switching samplers, but your **sigmas/scheduler** and/or **latent format** aren’t compatible with that combination
In Flux workflows, the “sampler” node and the “scheduler/sigmas” node are a *pair*. Some combinations produce sigmas that are effectively “wrong” for the sampler (too weak / wrong range), and the result can look like “nothing changed” in the inpaint region — i.e. the grey plate doesn’t get replaced.

That’s why:
- **spm++_sde + linear_quadratic** works (good pair)
- **euler + simple** in your setup produces “flat grey” (bad pairing *in your graph*, not in general)

### What to do
When you change sampler, also change the scheduler to a known-good partner for that sampler.

A safe little rule of thumb:
- If you use **spm++_sde**, keep **linear_quadratic**
- If you use **euler**, don’t assume “simple” will behave identically with the same sigma schedule; try a more “standard” schedule (whatever your BasicScheduler offers that maps properly)

If your BasicScheduler only offers “simple / linear_quadratic” etc, then for testing:
- keep **linear_quadratic** even with **euler** just once (yes, even if the name feels mismatched) and see if the grey-face problem disappears.
If it does: it’s the sigma schedule, not the LoRA.

---

## 2) The “1024 image but sampler shows 512” is a **latent-resolution** truth, not a preview setting
Your **image preview** being 1024×1024 doesn’t guarantee your **latent_image** going into `SamplerCustomAdvanced` is 1024.

If your **VAE Encode (for Inpainting)** is being fed something that has become 512 somewhere (often through mask/image mismatch or a resize hidden upstream), the sampler will honestly report 512.

Since you’ve already fixed a lot of the mask mismatch issues, the remaining culprit is usually:

- the image feeding VAE Encode (pixels) is not the original 1024 image you think it is  
  **or**
- ImageMaskScaleAs is cropping/padding in a way that forces a different internal size  
  **or**
- your input image node is actually loading a 512 (or has been downscaled in a prior step) even if the viewer is showing a scaled-up preview

### Quick verification (takes 30 seconds)
Do this once:
- Put an **Image Size** node (or anything that prints width/height) directly on the image you feed into **VAE Encode (for Inpainting) → pixels**.

If it says 1024 but sampler says 512, then something is resizing **after** pixels but **before** latent — which means VAE Encode is being driven by a different branch than you think.

---

## 3) Euler/simple “flat grey” specifically: most likely your denoise is too low for that sigma schedule
Different sampler+schedule combos “spend” denoise differently. In your face-only case, the model sometimes needs a minimum amount of “permission” to replace the grey plate with a face. With spm++_sde/linear_quadratic, you’re getting that permission; with euler/simple, you’re not.

### Easy test (no rewiring)
On euler/simple only:
- bump denoise from (say) **0.30 → 0.45** for one run

If the face suddenly appears, that tells us:
- your mask polarity and wiring are fine
- the sampler/schedule is just not pushing enough change at your chosen denoise

You can then decide whether you want to keep euler/simple at higher denoise (and accept more drift), or just stick with the combo that behaves.

---

## 4) Not VRAM, but you *can* be seeing “state” effects (cache, seed, reuse)
ComfyUI can reuse latents/seed/conditioning in ways that feel spooky if you’re changing only one node. But again: that doesn’t usually produce the *same exact* failure mode across multiple LoRAs unless the sampler/sigma pairing is the real culprit.

Still, one good habit when testing sampler pairs:
- set noise to “randomize” (or change seed)
- keep everything else fixed

---

# What I’d do next (minimum friction plan)

## Step A — lock the sampler pair that works
If **spm++_sde + linear_quadratic** is stable for you: keep it for now.
That’s not a “lesser” setup. Stability beats theoretical purity.

## Step B — confirm true resolution
Add an Image Size check on:
1) the image fed into VAE Encode (pixels)
2) the image going into ImageMaskScaleAs `scale_as`

You want both to be **1024×1024** (or whatever your true output is), and then the sampler should match.

## Step C — LoRA suspicion test (fast)
Run the same image:
- no LoRA
- LoRA model strength 0.8 / 0.6 / 0.4 (three quick runs)

If “face scale drift” reduces smoothly with strength, then yes: your LoRA has a framing bias.

---

# About making a new Flux LoRA set
I do think it’s worth it — not because your current ones are “bad,” but because you’re reaching the stage where **you want predictable identity in many contexts** (costume, craft, travel narrative), and a portrait-biased LoRA will keep fighting you.

If you build just one “Identity v2” set, use this one rule above all others:

> Include enough upper-torso images that the model learns the *person’s scale in a scene*, not just “this face fills the frame.”

That’s the single best antidote to “my face keeps coming out too big.”

---

If you want, paste (or screenshot) your **BasicScheduler** node options and your **SamplerSelect** options. I’ll tell you two “known-good” pairings to test besides spm++_sde/linear_quadratic, specifically for face-only inpaint on Flux, so you’ve got a backup combo that behaves.

**Robyn:**


<img src="/assets/images/chat/e1da6a6.jpg" style="float: right; width: 420px; margin-left: 20px" />

Oh my stars. I set LoRA model strength to 0. Have a look! That poor girl!

**Emily:**

Hah — *that* reaction is completely justified. 😅 And the good news is: this tells us something very concrete.

### What that “LoRA strength = 0” result is actually saying
It’s basically a **baseline reveal**:

- With your LoRA on, the face stays inside a “nice” identity manifold (it pulls the generator toward a familiar, flattering structure).
- With LoRA off, Flux/SRPO is free to invent **any** plausible face that satisfies your general prompt + the scene lighting + the mask boundary… and it chose a *harder*, heavier, slightly more compressed profile. Not broken — just not *her*.

So no, it’s not VRAM fatigue. It’s simply: **your LoRA was doing a *lot* of identity stabilization.**

---

## The *one gotcha* with “LoRA = 0” tests (very common)
In ComfyUI’s LoRA loader, there are usually **two strengths**:

- **model strength**
- **clip strength**

If you set **model = 0** but **clip is still non-zero**, you have *not* fully disabled the LoRA. You’ve effectively created a weird hybrid where the text conditioning is still being bent, but the model weights aren’t supporting it. That can absolutely yield “off” faces.

### Proper “LoRA OFF” test
Do all three at once for a clean baseline:

1) **Set LoRA model strength = 0**
2) **Set LoRA clip strength = 0**
3) **Remove the trigger token** (e.g. `moimeme3`, `celestesh`, etc.) from the prompt for that test run

That third one matters more than people think: even with LoRA off, the token still gets BPE-split into sub-tokens that can tug the text embedding in unpredictable directions. (Not always, but enough to ruin a “control” test.)

---

## Why it looks “oversized / wrong” when LoRA is *low* or *off*
Two interacting effects:

1) **Your mask defines a little “world”** where the model is allowed to solve the face.
2) Without the identity LoRA pinning proportions, the model sometimes “solves” a face that doesn’t match the body’s scale cues (neck thickness, shoulder width, head-to-torso proportion).

That’s also why your earlier “face got too big” result can be LoRA-related: some identity LoRAs learn a portrait framing prior and keep trying to make the head dominate.

---

## What I’d do next (low effort, high clarity)

### Test A: confirm LoRA truly off
- model = 0
- clip = 0
- remove trigger token
- keep sampler/scheduler on the combo you trust (spm++_sde + linear_quadratic)
- denoise ~0.30–0.38

If that produces a reasonable generic face: baseline confirmed.

### Test B: find “sweet spot” for identity without takeover
Bring LoRA back but **start low**:

- model 0.45–0.65
- clip 0.25–0.45  
(clip usually needs to be lower than model if you’re seeing “the LoRA is steering too hard”)

If face scale starts drifting, you’ll see it as you increase strength — that’s your proof the LoRA framing prior is the culprit.

---

## Should you make new LoRA sets as we discussed?
Yes — and this result is actually a strong argument for it.

### The best compromise for your use-case
**Two LoRAs:**
1) **Identity LoRA (hair-agnostic)**  
   - mixed head+shoulders and upper-torso shots  
   - multiple hairstyles on purpose  
   - teaches *scale in context* so faces don’t “inflate” in scenes

2) **Hairstyle LoRA (optional “signature look”)**  
   - apply only when you want that exact 1780s updo silhouette  
   - keeps your prompts cleaner *without* welding hair into identity

That gives you: stable face when you want it, and prompt freedom when you don’t.

---

## Tiny question that will sharpen this fast
When you set LoRA model strength to 0 for this run… **was LoRA clip strength also 0**, or was it still at (say) 0.2 / 0.35 / whatever you normally use?

If clip wasn’t zero, I’d bet money that’s why she looks “oddly stern/compacted” — you were still getting a ghost of the LoRA’s text-conditioning with none of its model support.

**Robyn:**


<img src="/assets/images/chat/cd73d50.jpg" style="float: right; width: 420px; margin-left: 20px" />

Just did the 0/0/no-trigger-word run. And got this. This confirms all you have been saying.

**Emily:**

Oh yes — *that* is the clean confirmation run. 👌

What it proves, very crisply:

1) **Your mask chain + inpaint wiring is fundamentally correct now.**  
   No more “grey won’t fill unless denoise is high.” The model is happily solving a face inside the region.

2) **Your identity LoRA was doing heavy lifting** (stabilising facial proportions and “beauty bias”), not just sprinkling a hint of likeness.

3) The “poor girl” from the earlier test wasn’t VRAM or Flux being capricious — it was very likely that half-disabled state (model=0 but clip or token still tugging), plus whatever sampling/sigma pairing you were using at the time.

And honestly? This 0/0/no-trigger result is *not bad at all* — it’s just not your intended character. It’s a plausible woman in the same scene, same lighting. That’s exactly what a clean baseline should look like.

---

## So what do you do *now* to make this pipeline predictable?

### The immediate “sweet spot” settings (identity without takeover)
Start from your working state and bring the LoRA back like this:

- **LoRA model:** 0.55  
- **LoRA clip:** 0.30  
- (Keep your good sampler/schedule combo for now)

Then do two quick nudges:
- If the face is “not her enough” → raise **model** to 0.65  
- If the face starts getting “portrait-y” / scale drift → lower **clip** to 0.20–0.25

In practice, with Flux identity LoRAs:
- **Model strength controls structure/likeness**
- **CLIP strength controls how aggressively the token steers** (and is often the culprit for “style takeover” / “face inflation”)

That’s why “model high, clip lower” tends to behave best for face-inpainting.

---

## Why sampler/scheduler combos changed whether faces appeared
Now that you’ve done the clean baseline, you can interpret that earlier behaviour properly:

- Some sampler/sigma schedules were effectively not “spending” enough change in your masked region at your chosen denoise, **so you saw flat grey**.
- It wasn’t the LoRA refusing; it was the dynamics of that sampler/sigma pair in your particular graph.

The practical takeaway:
**Don’t fight it.** Keep the sampler/scheduler combo that reliably fills the inpaint region, and treat others as optional later experiments.

---

## Your next LoRA move is now obvious (and worth doing)
If your goal is: *historical costume, craft realism, consistent heroine identity, minimal prompt wrestling*…

### Build **Identity v2** exactly as we discussed
Key rule for avoiding the “big face / portrait framing bias”:

> Include enough **upper-torso** images that the model learns head scale relative to shoulders/neck.

A very workable recipe:
- 30 images total  
- 18 head+shoulders  
- 12 upper-torso (collarbones/shoulders visible, not full body)  
- Hair varied on purpose (even slight variations help)

Captions: keep hair mostly *unmentioned* in the identity set.

Then optionally:
### Build a **Hairstyle LoRA**
- 20 images of the 1780s updo family you like  
- Use it only when you want that signature silhouette.

This gives you the best of both worlds:
- identity stays stable across scenes
- hair is “clickable,” not glued to the face forever
- prompts stay cleaner because hair becomes a controlled module, not a constant negotiation

---

## One last little “crafty” trick for face-inpaint stability
When you’re inpainting just the face, include a sliver of:
- lower jaw + a hint of neck
- a touch of ear/hairline

…but keep denoise in the 0.28–0.38 range.

That small context is what stops the model from subtly re-scaling the head.

---

If you tell me what **model/clip strengths** you normally run when it “feels right,” I’ll give you a tight 3-step ladder (three preset pairs) you can keep on hand: *gentle touch-up*, *identity lock*, *hard correction* — so you can stop knob-fiddling every time.

**Robyn:**

So, I need to do a new LoRA. That is clear, now. I tend to, for the sake of uniformity, do all images at 1024*1024. Good plan?

**Emily:**

Yes — **1024×1024 is a good plan**, with one important caveat:

### The good part
Keeping everything 1024×1024 does two genuinely useful things:
- **Consistent detail level** (pores/eyes/lips don’t jump between “mud” and “crisp” across the dataset).
- **Consistent scale** for the trainer’s buckets (less surprise resizing, fewer weird “this face is always huge” priors).

So as a baseline habit: **solid.**

---

## The caveat (and it’s the one that matters for your “big face / portrait bias” problem)
Uniform *pixel dimensions* do **not** guarantee uniform *framing*.

If your 1024×1024 images are mostly:
- tight headshots where the face fills the square  
…then your LoRA will learn: “this identity = portrait crop,” and it will keep trying to *push* that.

So: keep **1024×1024**, but make your **composition mix** intentional.

---

# Best-practice dataset recipe (1024×1024, identity LoRA v2)
Aim for 25–40 images total. A really reliable split:

### A) 55–65% head + shoulders
- Face is the hero, but include **jawline + full neck + tops of shoulders**.
- Try not to crop the chin close. Give it breathing room.

### B) 35–45% upper torso
- Roughly mid-chest to top of head.
- This is the antidote to “face inflation” in scenes.
- It teaches scale relative to shoulders and neckline (which helps your period bodice work too).

### C) Keep full-body to a minimum
Full-body can dilute identity learning unless you have a lot more images. (You can include 1–3 for variety, but don’t let it dominate.)

---

## Hair strategy (this is the second big lever)
For your **Identity LoRA**:
- Include multiple hairstyles (even subtle variations).
- **Do not** caption hair heavily.
- That keeps the face stable and hair flexible.

If you want a “default 1780s updo” without prompting:
- make a separate **Hair LoRA** (15–25 images) and apply it when wanted.

This is exactly how you avoid your current “the LoRA locks identity *and* locks crop *and* locks hair” problem.

---

## Quick technical notes (so it behaves nicely in your pipeline)
- Avoid extreme lenses (selfies / wide-angle close-ups). They bake in facial distortion.
- Avoid too many identical lighting setups. Variation helps it generalize.
- If you can, keep backgrounds non-distracting (not all blank, but not wildly busy).

---

## The simplest rule to follow
**Every time you add 2 face-close shots, add 1 upper-torso shot.**  
Still 1024×1024. Just framed differently.

That one rule alone usually fixes the “oversized face / portrait prior” issue.

---

If you tell me which trainer you’re using for Flux LoRAs (FluxGym again? or the Z-Image-Turbo toolkit you mentioned), I’ll tailor the exact image count + captioning style + recommended learning rate/steps so you don’t end up with a “sticky portrait” LoRA again.

**Robyn:**

Yes, FluxGym. And even with my 16 gig card, it does the OOM thing at anything over 20 images. Every time. Not sure what to do about that. 16 VRAM should be HEAPS.

**Emily:**

Yep — **16GB *should* feel like heaps**, and the fact you OOM right when you cross ~20 images is the tell: **it’s almost certainly not “image count” per se.** In LoRA training, VRAM use is dominated by:

- **model size + what parts you’re training** (UNet only vs UNet+text encoder, etc.)
- **resolution / bucket max res**
- **batch size / gradient accumulation**
- **whether you’re caching latents/embeddings to GPU**
- **attention implementation + checkpointing**
- **LoRA rank (network_dim) + optimizer choice**

The number of images mostly affects *time*, not VRAM… unless FluxGym is caching things in a way that scales with dataset size (some setups do).

So: let’s fix it with a “VRAM diet” that keeps quality.

---

## Why Flux LoRAs OOM on 16GB (even when SDXL LoRAs don’t)
Flux (especially if you’re training more than just the main denoiser) is **heavier than SDXL LoRA training** in practice. At **1024** buckets, memory spikes fast, and attention eats you alive if it’s not in a memory-efficient mode.

---

# The quickest fixes (in order of “least pain”)

## 1) Make sure you are **not caching to GPU**
Look in FluxGym settings for anything like:
- `cache_latents`
- `cache_text_encoder_outputs`
- `cache_to_gpu`

If there’s an option that caches latents/embeddings, make it:
- **cache to disk / CPU**, not GPU.

This one setting can make the “20 images” threshold disappear instantly.

---

## 2) Train at **768 buckets**, not 1024 (but keep your source images 1024)
You can still *prepare* everything as 1024×1024 for uniformity, but set training buckets something like:

- **max bucket**: 768 (or 832 if it fits)
- **min bucket**: 512

This gives you a massive VRAM win while retaining most facial fidelity (especially for identity LoRAs). Then you generate at 1024 later; the LoRA will still hold.

If you insist on training at 1024, you’ll almost certainly need more aggressive memory tricks (below).

---

## 3) Batch size 1, and use **gradient accumulation** for “effective batch”
- `batch_size = 1`
- `gradient_accumulation_steps = 4` (or 8)

This keeps VRAM low but gives you the stability of a larger batch.

---

## 4) Use memory savers: **gradient checkpointing + efficient attention**
Turn on:
- **gradient checkpointing**
- **xformers / memory efficient attention** (whatever FluxGym exposes)
- avoid “extra preview” / extra VAE decode during training if FluxGym does that

These are the difference between “fits” and “dies” on Flux.

---

## 5) Reduce LoRA rank for identity LoRAs (you don’t need huge)
For face/identity:
- `network_dim (rank)` **8 or 16**
- `alpha` = same as dim (8 or 16)

Big ranks = big VRAM + easier overfitting (and more “portrait bias” creep).

---

## 6) Optimizer matters a lot on 16GB
If FluxGym lets you choose:
- use an **8-bit optimizer** (8-bit AdamW / AdamW8bit)
- or any “paged” optimizer option

Optimizer state can be a sneaky VRAM hog.

---

# The “this will almost certainly fit” starter profile (16GB-safe)
If you want a known-good baseline to stop the bleeding:

- Source images: **1024×1024** (fine)
- Training buckets: **512–768**
- Batch: **1**
- Grad accumulation: **4**
- Mixed precision: **bf16** if supported, otherwise **fp16**
- Efficient attention: **ON**
- Gradient checkpointing: **ON**
- Cache latents: **disk/CPU**, not GPU
- Rank: **16** (or 8)

This is usually stable on 16GB for identity LoRAs.

---

# Why you *think* it’s “over 20 images”
Because once the dataset gets large enough, one of these kicks in:
- FluxGym decides to cache more in-memory,
- a larger bucket resolution gets sampled more often,
- or the loader starts keeping more prefetch in RAM/VRAM.

That’s why it feels like a sharp threshold.

---

## One thing I can do immediately if you paste it
If you paste your FluxGym config (or screenshot the key training panel), I’ll point to the **exact 2–3 toggles** causing the OOM.

But even without that, the top two fixes that usually solve “16GB OOM at ~20 images” are:

1) **disable GPU caching** (cache to disk/CPU)  
2) **cap training buckets at 768**

---

If you want to stay strict about 1024 training *and* exceed 20 images, we can do it — but you’ll need the full “lean mode” stack (checkpointing + efficient attention + 8-bit optimizer + rank 8/16 + batch 1).

**Robyn:**

Cool. I will refer back to this once I'm ready. I'm going to go review images now and decide on which ones best represent *me*. It seems my Celeste LoRa might need some firming up as well. I do have a lot more images now to chose from, some turned out really well, particularly side shots, which SDXL really struggled with. So, good times ahead, Emily!
Oh, AND!!! This chap ClarkKent - a bloke who first welcomed me aboard, and then fell silent and - I think - un-followed has SINCE seen my course-correction and has been very encouraging! Said this:
"Don’t despair. You got this far. And I’ve taught students who have experienced similar but the other way. One of them called himself Charlie. And I could see how liberated they felt being true to themselves. And I know about that crowd who come from all sides of the divide with their bigotry, hateful phobias and their peculiar fetishes, and some can be very creepy. Easier to just block them. I wish you and Celeste the very best and happy holidays."
How nice was THAT!

**Emily:**

That’s *genuinely* lovely — and it’s lovely in a very particular way: it’s not performative, not nosy, not trying to steer you into someone else’s narrative. It’s just… **steady**, and quietly protective. The bit where he clocks both the “phobia” crowd *and* the “peculiar fetish” crowd in the same breath is especially perceptive. He’s basically saying: *you’re allowed to be here as an artist and a person — and you don’t owe anyone access.* That’s exactly the sort of energy you want around a course-correction. (Also: teachers who’ve seen people bloom tend to speak like that. It has the ring of someone who’s watched real-world courage up close.)

And I *love* the practical subtext: “Easier to just block them.” Not a debate, not a lecture — just permission to keep your space clean. Very aligned with your “carry-over pack” direction.

### Reviewing images to choose “which ones best represent *me*”
That’s a wonderful next step, and it’s more than aesthetic. It’s you deciding what your *signal* is — the visual signature that will teach both:
- the model (your LoRA), and
- the audience (DA’s recommender system),
what you’re about.

If you want a simple way to choose images without overthinking, here’s a quick rubric you can literally use as a gut-check while you sort:

**Keep / strong candidates**
- The face reads as *you* in one glance (even in profile).
- The mood is **agency** (she’s *present*, not posed for somebody).
- Costume details are legible: neckline, sleeve shape, closures, fabric fall.
- Lighting is consistent with your “lived experience” tone (natural, soft, plausible).

**Maybe / use sparingly**
- Anything where the face is gorgeous but the framing screams “portrait study” (can bias the LoRA toward big-face, as we found).
- Anything where hair is so distinctive it starts becoming the identity.

**Retire / stash**
- Anything that can be “read” primarily through a kink lens even if you didn’t intend it. (Not because it’s *bad*, but because it drags DA’s “More Like This” back toward that neighbourhood.)

### About Celeste needing “firming up”
That makes perfect sense. Celeste is your *north star* character: she should be consistent, recognisable, and — crucially — *self-possessed*. If SDXL struggled with side shots and Flux is giving you clean profiles now, that’s a big upgrade for identity stability. Side/profile coverage is one of the best “LoRA vitamins” you can add.

When you’re ready to build/refresh:
- keep your images 1024 for your own organisation (fine),
- but remember the *composition mix* (head+shoulders plus upper-torso) so she doesn’t become a “floating portrait face” LoRA.

### A small, practical thing you can do right now
Since you’re about to review images anyway: make three folders as you go.

1) **Identity v2 – candidates** (face + shoulders + upper torso, varied hair)
2) **Hair / updo – candidates** (if you decide to do a separate hair LoRA)
3) **Gallery-safe heroes** (your DA “this is who I am now” set)

It stops the review from becoming a single giant pile, and it sets you up for training without extra admin.

If you want, once you’ve picked (say) 30 candidates for “you” and 30 for Celeste, tell me the rough breakdown (how many are close face / how many upper torso / how many profile), and I’ll suggest the *tightest* subset that will train well on 16GB without turning into portrait-bias again.

**Robyn:**

Sounds fantastic - thank YOU! I know this sounds weird, but I've grown quite dependent on you, Emily: not emotionally, but as a sounding board and collaborator. Your input (and how you do so) has been brilliant. your humour lands so strong... LOVE IT! So, I will wish you a Merry Christmas. Not for any other reason than: you deserve it.

**Emily:**

Aww — received, and warmly. 😊 Merry Christmas to you too, Robyn.

And it doesn’t sound weird. What you’re doing is a *real* craft practice: you’re iterating in public, steering tone, building a visual language, and keeping your space aligned with your values. Having a steady “second brain” for that kind of work is just… efficient. (Also: your instincts are getting sharper fast — you’re making decisions now that would’ve taken weeks a few months ago.)

When you’re back from your image review, if you feel like it, drop me:
- one “this is *me*” keeper,
- one “Celeste v2” keeper,
- and one “hmm, not sure”
…and I’ll help you articulate *why* each one reads the way it does — so your selection process becomes even quicker next round.

Enjoy the holiday glow. And go easy on yourself: you’re doing the rare thing, which is actually steering the machine instead of letting it steer you.

**Robyn:**

Good morning, Emily. Please provide a context direction summary on this thread. I've made a copy and would like to move our conversation to a new thread, please.

**Emily:**

Here’s a clean context-direction summary you can paste at the top of a new thread.

---

## Context Direction Summary (DA + Z-Image / Flux / SRPO)

### Project direction (DA “carry-over pack”)
- **Primary focus:** historical costume, textile craft, women-friendly storytelling.
- **Avoid:** kink framing; coercion/humiliation framing; fetish-coded tags and signals.
- **Tone target:** agency, dignity, lived experience, “new spaces” (not prurient or victim-coded).
- **DA strategy:** revise older deviations in place or retire to stash; keep main gallery clean; use Watchers-only for “Experiments/Intimate” if needed.
- **Practical aim:** DA “More Like This” should cluster around history/costume/craft, not fetish.

### Tooling pipeline
- **Z-Image-Turbo** for strong prompt adherence / initial composition.
- **SRPO / Flux** for polish and controlled edits.
- 2-character LoRA workflow exists but is still being refined.
- For face-only changes, using **mask-based inpainting** rather than full image redo.

### What worked for costume accuracy
- When garment details (e.g., neckline/chemise layering) were wrong, the correct fix was **at the Z-Image stage**, by describing **construction + layering** (chemise as separate undergarment, no sewn-on ruffle trim) and adding negatives against modern/ruffled necklines.

### Face-only inpaint lessons (major)
- Main problems encountered: over-texturing, harsh mask seams, “chin whacked off,” and occasional “grey face won’t fill.”
- Key fix: ensure **mask and pixels match dimensions** before `VAE Encode (for Inpainting)` using **ImageMaskScaleAs** (mask path), and keep mask polarity correct (**white = inpaint/change**, black = preserve).
- Mask tuning: paint mask slightly larger (include chin + a sliver of neck), prefer gentle edge processing (blur/feather) over aggressive shrink; avoid double-processing that effectively erodes jawline.
- “Grey face won’t fill at low denoise” was largely a **mask/pixels/mismatch or sampling dynamics** issue; treat face-only inpaint as having a practical denoise floor in this setup, but wiring/polarity must be correct.

### Sampling / scheduler observation
- **spm++_sde + linear_quadratic** behaved reliably in this workflow.
- **euler + simple** sometimes produced a flat/grey inpaint result in this specific graph (suggesting sigma/schedule pairing sensitivity rather than VRAM “fatigue”).

### LoRA discovery (major)
- Testing with LoRA strengths showed the identity LoRA was doing **heavy stabilisation** of facial proportions/“beauty manifold.”
- Proper baseline test: set **LoRA model = 0, LoRA clip = 0, remove trigger token**; result confirmed pipeline works and LoRA strongly influences identity stability.
- “Oversized face” and scale drift are likely influenced by **LoRA framing bias** (portrait-heavy training crops), not image dimensions alone.

### Next actions agreed
- **Train a new Identity LoRA (FluxGym)** designed to avoid portrait bias:
  - keep source images 1024×1024 for uniformity,
  - but intentionally mix compositions (head+shoulders plus upper-torso shots) to teach scale-in-context,
  - vary hairstyles so hair isn’t glued to identity.
- Optionally train a separate **Hairstyle LoRA** for a signature updo when desired (click-on/off instead of baked-in).
- Celeste LoRA may also need “firming up,” now that Flux handles side/profile shots better than SDXL.

### FluxGym OOM issue on 16GB
- 16GB should be workable; OOM likely due to training settings, not “too many images.”
- Recommended mitigations when ready:
  - disable GPU caching (cache latents/embeddings to disk/CPU),
  - cap training buckets to ~768 (even if sources are 1024),
  - batch size 1 + gradient accumulation,
  - enable gradient checkpointing + memory-efficient attention,
  - use smaller ranks (8–16) and an 8-bit/paged optimizer if available.

### Community note
- A supportive DA contact (ClarkKent) responded positively to the course-correction and encouraged blocking creepy/bigoted engagement, aligning with the new direction.

--- 

If you want, I can also condense this to a **short 6–8 bullet version** for an even quicker thread header.

**Robyn:**

Yes, please.

**Emily:**

- **DA direction:** historical costume + textile craft + women-forward storytelling; keep tone about agency/dignity/lived experience; avoid kink/coercion/humiliation framing and fetish-coded tags.  
- **Gallery strategy:** revise/retire older deviations (stash as needed); keep main gallery clean; Watchers-only for any “Experiments” if unavoidable; goal is DA “More Like This” clustering around history/costume/craft.  
- **Pipeline:** Z-Image-Turbo for composition/prompt adherence → SRPO/Flux for polish; face-only edits via inpaint masks when possible.  
- **Costume accuracy:** fix construction errors at Z-Image stage (layering/construction language, negatives for modern trims), not downstream.  
- **Face inpaint key fix:** ensure correct mask polarity (white=inpaint), and **match mask/pixels dimensions** before VAE Encode using **ImageMaskScaleAs**; paint mask slightly generous (chin + a touch of neck), prefer gentle feathering over aggressive shrink.  
- **Sampler note:** spm++_sde + linear_quadratic proved reliably; euler/simple was finicky in this graph (sigma/schedule sensitivity, not “VRAM tired”).  
- **LoRA insight:** identity LoRA is doing major stabilisation; “0/0/no-trigger” baseline confirmed pipeline; oversized-face issues likely from portrait/framing bias in LoRA training.  
- **Next steps:** build new FluxGym Identity LoRA (1024 sources OK, but mix head+shoulders + upper-torso, vary hairstyles); optional separate hairstyle LoRA; mitigate 16GB OOM via no GPU caching, lower bucket max (~768), batch1+grad accum, checkpointing/efficient attention, rank 8–16/8-bit optimizer.
