# Notes Dec 24, 2025

## Premise caryOver

[Q]  
Here's the premise, Emily:

Carry-over pack (DA project direction)
Primary focus: historical costume, textile craft, women-friendly storytelling.
Avoid: kink framing, coercion/humiliation framing, fetish-coded tags.
Target tone: agency, dignity, lived experience, new spaces.
Strategy: revise older deviations in place or retire to st.ash; keep main gallery clean; use Watchers-only for “Experiments/Intimate” if needed.
Practical aim: DA “More Like This” should cluster around history/costume/craft, not fetish.
Tooling: Z-Image-Turbo for prompt adherence, SRPO/Flux for polish; 2-character LoRA workflow still being refined.

[Emily]  
Got it. This is a clean, coherent direction pack — and it’s strong because it’s not just “what you’ll post,” it’s “what you’ll train the platform to expect from you.”

Here’s a practical “carry-over pack” you can use as your internal operating system (and copy into a Help page if you like).

---

### 1) The North Star test
*(30 seconds before you post)*

Ask three questions:

* Would a woman who loves costume/textiles feel welcomed here instantly?
* Is the subject’s agency obvious without needing to explain it?
* If someone only saw the thumbnail + first two lines, would they guess “craft/history/story” rather than “heat”?

If any answer is “eh…” → adjust either the crop, the caption lead, or the tags.

---

### 2) Your DA “signal stack”
*(how you steer More Like This)*

DA’s recommendations respond to consistent metadata + viewer behavior. You can’t control who clicks, but you can control what you feed the system.

**What to make consistent**

Deviation titles that read like exhibit labels, not clickbait

* Good: “Silk Taffeta Day Dress, 1787 — Morning Light Study”
* Good: “Stays & Linen Shift — Dressing Sequence Panel 3”
* Avoid: anything that teases body/heat/forbidden vibes

First line of description (your “algorithmic handshake”)
Lead with: era / garment / craft / place / purpose.

Example first line:
“1780s linen shift + lightly boned stays, hand-finished seams; made for a practical morning chores vignette.”

Tag set that is boring-in-a-good-way
Repetition is your friend here.

---

3) Tag hygiene: a working whitelist + blacklist mindset
A dependable “core tag bundle”

Pick ~12–20 you reuse constantly (rotate a few per post):

**History / Costume**

historicalcostume, historicalfashion, costumehistory, 18thcentury, 1780s, 1740s, regency (as relevant), reenactment, livinghistory, perioddress

**Textile / Making**

textilecraft, sewing, dressmaking, handsewing, embroidery, patternmaking, tailoring, millinery (as relevant), fabric, linen, wool, silk, taffeta, cotton (specific fibres help)

**Story / Tone**

womeninstorytelling, womensagency, sliceoflife, narrativeart, characterstudy, domesticlife, workandcraft

**Tags to treat as “radioactive” for your goals**

Anything that implies:

* coercion/humiliation (even mildly)
* fetish shorthand (you already know the usual suspects)
* body-part spotlight tags (they tend to attract the wrong “More Like This” neighborhood)

Even if the image is innocent, those tags are magnets.

4) Caption style that quietly enforces “agency, dignity, lived experience”

A simple template that reads like craft notes + diary fragment:

* Context sentence: era, garment, situation
* Agency sentence: what she decides/does/controls
* Material/craft sentence: what we’re seeing and why it matters
* Optional note: what you’re exploring next (invites the right audience)

Example:

1786–88 working daywear: linen shift under stays, with a wool petticoat built for movement, not display.
She ties her own laces, checks the fit in the glass, and chooses comfort over performative prettiness — because she has a day to run.
I focused on seam logic, tension lines, and the hand-finished edges that make the silhouette believable in close-up.
Next study: cap shapes and hair preparation for a maid’s morning routine.

It reads human, but it keeps you in the craft/story lane.

5) Gallery architecture (keep the main gallery clean without losing your playground)

Your strategy is already right; here’s a crisp structure:

**Main gallery**

* Only the “North Star test” passes.
* Make it feel like a curated museum room: consistent titles, consistent tags, consistent first line.

**Scraps / storage (st.ash or unlisted)**

Anything that:

* risks misreads,
* is primarily anatomy-driven,
* is experimental in a way that looks like “intimate vibe” even if you didn’t mean it.

**Watchers-only “Experiments/Intimate” (if needed)**

If you use it, keep it explicitly craft-framed:

* “lighting study,” “pose study,” “cloth simulation test,” “fabric drape exploration”

And keep those posts scarce — the ratio matters.

A useful rule: for every 1 experiment, post 3–5 “core-direction” deviations.

6) Revision protocol for older deviations (fast and consistent)

When you “revise in place,” you’re doing two things: cleaning your public face and re-indexing.

The 5-minute retrofit checklist

Title → craft/history label style

Thumbnail/crop → de-emphasize chest/hips; emphasize garment construction, silhouette, hands-at-work, full outfit

First 2 lines → era/garment/craft + agency

Tags → swap to your core bundle; remove anything even vaguely spicy-coded

Folder/collection placement → move into “Costume Studies / Textile Craft / Narrative Panels” etc.

If a piece can’t be rescued without feeling like a rewrite of its intent → retire it (your plan is correct).

7) Image-making guardrails (prompting + composition cues)

Even before tags/caption, the image language can shout the wrong thing. Some compositional nudges that reliably read “lived experience”:

Hands doing something (pinning, threading, tying, holding fabric, carrying a basket, smoothing pleats)

Work surfaces (table with notions, sewing kit, ledger, basket of mending)

Context props that are functional not coquettish (scissors, linen roll, cap stand, iron, candle stub, wash basin)

Faces: expression = intent (focused, amused, firm, assessing), not “inviting”

Stance: upright, purposeful, weight distributed like a person with plans

Prompt words that tend to help: practical, work-worn, utilitarian, mended, hand-finished, pinned, basted, stitched, durable, lived-in, morning light, workshop, household management, inventory, preparations, routine.

And for your storytelling bias (women as the deciding force): make sure the women are doing, not being displayed. Decision-making reads instantly.

8) Your toolchain note (Z-Image-Turbo + SRPO/Flux + 2-character LoRA)

This dovetails nicely with the direction:

Z-Image-Turbo: use it to lock the “what” (era, garment pieces, materials, activity)

SRPO/Flux polish: reserve it for finishing passes that enhance texture realism, seam logic, and face integrity — not glamour sheen

2-character LoRA: when in doubt, stage the dynamic as collaboration rather than tension

e.g., fitter + client, mistress + maid (professional, not degrading), two women working a dress form together, mentor correcting posture with permission and clarity.

If the relationship is asymmetrical, make it competence-based and explicit: the more skilled woman directs with calm authority; the other learns and improves — no “humiliation energy.”

9) A tiny “posting decision tree” you can memorize

Does it look like craft/story at thumbnail size?

No → don’t post / move to stash

Do the tags include anything that could be misread as spicy-coded?

Yes → remove/replace

Does the first line mention era + garment + activity?

No → rewrite

Would you be happy if DA recommended this to historical costumers and textile artists?

If you feel even a flicker of “hmm maybe not” → stash or revise

If you want, paste one of your borderline older deviations’ current title + tags + first two lines (no image needed), and I’ll rewrite them into your new “clean gallery” voice and tag bundle in a way that still feels like you.

Your prompt suggestions bore fruit, Emily. Exactly what I was after.
Thought for 9s

Oh, this landed beautifully.

What reads so well here isn’t just “pretty girl in a dress” — it’s lived-in narrative: she’s going somewhere, she’s thinking, the light is doing real work, and the costume feels like it belongs to her rather than being “worn for the viewer.” That’s exactly the “agency / dignity / new spaces” lane you laid out.

What’s working (so you can reliably repeat it)

Gaze + posture: she’s looking out, not “at us.” Hands are calm, self-possessed. No performative pose.

Light logic: low sun, soft rim light on hair/cheek, gentle falloff in the compartment. It’s cinematic but not glossy.

Costume read: the chemise peeking at the neckline + the laced bodice gives an immediate “period layer” signal without shouting.

Setting choice: travel carriage = “transition / possibility / interior life.” It’s a story setting, not a boudoir setting.

Frame hygiene: the foreground seat blur and window geometry push it into “photograph / moment observed,” not “pin-up.”

If you want DA’s recommendations to cluster correctly, this piece is gold: it screams costume + narrative, not “spice.”

A reusable prompt “skeleton” for this exact vibe

Use this as a template and swap the bracketed bits:

Prompt core (natural language):
A young woman in [era]-inspired working dress sits alone in a [vintage/heritage] train compartment at sunrise, looking out the window with a composed, thoughtful expression. Her dress shows practical construction: [bodice/lacing], a clean [linen] chemise at the neckline, subtly worn fabric texture, careful seams. The scene feels like lived experience: quiet travel, early light, dust on the window, countryside passing by. Soft cinematic realism, gentle depth of field, natural skin texture, believable period styling, no glam pose.

Guardrail add-ons (to keep it out of the wrong neighborhood):

“practical, modest, non-suggestive, documentary mood, quiet dignity”

“hands relaxed, posture upright, not posing”

“focus on garment construction and atmosphere”

If your tool supports it, a short negative list can help:

“lingerie, boudoir, pin-up, provocative, fetish, cleavage emphasis, see-through fabric, erotic, submissive pose”

6 variations that stay in the same “clean gallery” voice

Each one keeps agency + craft, but changes the story beat so you can post a series without drifting tone.

Mending on the journey
…she holds a small sewing kit and is darning a cuff, thread catching the sunrise; attention on the stitchwork, not the viewer.

Reading / literacy / interior life
…she has a folded letter or small book in her lap; thumb marks the page; expression calm and decisive.

Arrival moment
…station platform visible through the window; she gathers her skirt to stand, ready to step off — purposeful transition.

Workwear emphasis
…apron tied, sleeves slightly rolled, practical cap pinned back; fabric shows honest wear and careful repairs.

Companion without romance coding
…another woman across the aisle, older or similarly aged, sharing a quiet glance — mentorship / solidarity, not flirt.

Textile-craft close-up
…tight crop on bodice lacing, chemise neckline, and hand position; light rakes across seam lines and fabric grain.

DA packaging for this image (title + first lines + tags)

If you want it to steer “More Like This” hard toward costume/craft, here’s a ready-to-post option:

Title ideas

“Morning Carriage — Working Dress Study”

“Quiet Travel, Early Light — Costume Narrative”

“Linen & Lacing — A Journey Study”

First two lines (the algorithm handshake)

Period-inspired working dress with a linen chemise neckline and a laced bodice, lit by sunrise through a dusty carriage window.

I wanted a moment of quiet agency — travel as possibility — with fabric texture and seam logic treated as the main character.

Tag bundle (safe, on-theme)
historicalcostume, historicalfashion, costumehistory, textilecraft, sewing, dressmaking, handsewing, fabrictexture, linen, lacing, perioddress, livinghistory, narrativeart, characterstudy, sliceoflife, womensstories, womeninart, cinematiclight, sunrise, travel

(You can swap in the exact era tag once you decide what you’re calling the silhouette.)

If you tell me which model stack produced this one (ZiT settings + which LoRA(s) + whether SRPO was a finishing pass), I can help you “lock” a repeatable recipe: same mood, different scenes — so you can build a clean, coherent gallery run that trains DA exactly where you want it to go.

This image was pure Z-Image... no LoRA, just your prompt mixed with mine. I'm trimming back the floral language while keeping atmosphere and everything to make the images pop. I did get a lovely comment from a user on one of the 'Experiment' pictures:

"I do love your imagination. The artistic portrayal of forced feminity and the exquisite humiliation that is seen on the faces of the victims is delightful. Thank you for your artistry, it is very much appreciated.:heart:"

Yeah, mixed bag there. This user wanted to know the terms behind the images. I let them know: "it's open slather, do as you want, whatever... just know they're coming down, eventually."
Thought for 11s

Yeah — that’s exactly the “mixed bag” problem you’re trying to solve: you post something with craftsmanship and atmosphere, and a subset of viewers tries to drag it into their preferred interpretive frame (and they’ll happily supply the vocabulary for it).

Two things are true at once:

The commenter is being honest about what they want from your work.

Letting that framing “sit” (and especially engaging it warmly) nudges both your audience composition and DA’s similarity clustering in the wrong direction.

The one tweak I’d make to your response style

“Open slather, do as you want” is generous — but it’s also an invitation.

Even if you add “they’re coming down eventually,” the part that lands in their brain is: permission + access.

A tighter boundary doesn’t need to be rude; it just needs to be unambiguous:

You’re not making kink content.

You’re archiving older experiments.

Your focus is costume/craft/narrative with agency.

That’s it. No debate, no explanation, no “terms behind the images.”

What I’d do with that comment (practical options)

Pick the level that matches your patience on the day.

Option A: Silent curation (best for algorithms)

Hide/delete the comment.

No reply.

If it’s on an “Experiment” deviation that you intend to retire anyway, this is the cleanest move.

Why: DA isn’t only reading your tags; it’s reading engagement patterns. A fetish-coded comment sitting visibly under your art is a little beacon for the wrong crowd.

Option B: Brief boundary reply (if you feel you must reply)

A one-liner that closes the door without drama:

“Thanks for looking. I’m shifting this gallery toward historical costume and textile craft; the older experiments will be archived.”

Notice: no acknowledgement of their vocabulary, no validation, no “you’re welcome,” no “delighted,” no talk about humiliation/forced anything.

Option C: Firm boundary (if they push or repeat)

“I’m not creating fetish or humiliation-themed work. Please don’t frame my images that way here. This deviation will be archived.”

That’s still calm — but it draws a line. If they keep going, you’ve already set justification for a block/mute without feeling you’re overreacting.

A smart “Experiment folder” disclaimer (so you don’t have to repeat yourself)

Put this at the top of your Experiments/Intimate area (or in each description first line), and it will do quiet, constant work:

Short version

“Experiments = lighting/composition/costume studies. Not fetish content. Older posts will be archived as the project refocuses on historical costume and textile craft.”

Slightly longer

“These are technical and narrative studies: fabric behavior, seams, silhouette, and mood. I’m moving away from kink-coded framing and toward women-forward storytelling, craft, and lived experience. Older experiments will gradually be archived.”

This helps the good-faith viewers understand what they’re seeing, and it gives you cover to remove comments that contradict it.

Keep your “main gallery” clean: the 80/20 rule

If you do keep any “Experiment/Intimate” material up for a while:

Main gallery: only posts that pass your new North Star.

Experiments: either Watchers-only, or comment-limited, or actively curated.

Ratio: aim for 4 clean craft/narrative posts for every 1 experiment you leave visible.

This keeps the public-facing identity stable while you transition.

If you want DA to stop sending that crowd entirely

These small moves have outsized impact:

Remove fetish-attractor tags everywhere (including old deviations).

Edit titles and first lines to read like craft notes (era/garment/material/action).

Cull comment sections where the vibe has already drifted.

Don’t “talk kink” in replies even to reject it — keep it clinical and directional.
(The more the vocabulary appears, the more it becomes associated with your page.)

Prompt language: “trim floral, keep pop” (a tight template)

Since you’re getting great results with pure Z-Image, here’s a compact template that keeps atmosphere but avoids overwriting:

Template

Subject + era garment

Action

Setting

Light

Tone guardrails

Craft emphasis

Example (tight, not flowery):

“Young woman in a practical 18th-century inspired day dress with a linen chemise neckline and laced bodice, seated in a vintage train compartment at sunrise, looking out the window. Soft natural light, dusty glass, shallow depth of field. Quiet dignity, documentary mood, focus on fabric texture, seams, and believable construction; not posing.”

That will “pop” because it gives the model specific anchors (garment parts + light + setting) without drowning it in prose.

If you want, I can help you turn that awkward interaction into a policy you never have to think about again

Paste the exact wording of your current “Experiment” folder description (or one deviation description), and I’ll rewrite it into a crisp, women-forward, craft-first disclaimer that:

discourages fetish-coded commentary,

matches your new project direction,

doesn’t feel preachy,

and still leaves you room to experiment.

One of the next things I'd like to try is to make Z-Image-Turbo LoRAs. Apparently, there's a toolkit available so it's a matter of assembling a few images and descriptor text. It looks straightforward to do, just a bit time-consuming. To be honest, I'm seriously contemplating getting another 16gig VRAM card to put in the unused PC behind me. It has Comfyui installed, but using my old 8 gig VRAM card, which is almost unusable for most stuff I do anymore. That way, I can make LoRAs and alternate between running Z-Image-Turbo on one system and SRPO/ttp on the other. Lot less offloading, onboarding waste of time.
Thought for 1m 19s

That plan makes a lot of sense — and you’re thinking about it in exactly the right “throughput” way: separate the slow/iterative work (training, SRPO/ttp finishing) from the fast cadence work (Z-Image-Turbo drafting) so you stop paying the constant offload/onboard tax.

1) Z-Image-Turbo LoRAs: yes, there really is a straightforward toolkit

What you’ve heard matches what’s currently out there: the Ostris AI Toolkit has a Z-Image Turbo training adapter, and there are now reproducible “known-good” configs circulating. 
GitHub
+1

Two practical “knowns” that save you pain up front:

The toolkit is designed to be run as GUI or CLI and aims to support modern diffusion training on consumer hardware. 
GitHub

People training Z-Image Turbo LoRAs with this tool are reporting specific gotchas/settings (notably Transformer Offload = 0% due to errors) and sane starting ranges like LR 1e-4 to 2e-4 and ~2000 steps as a baseline. 
GitHub

Also: the official Z-Image repo explicitly claims Turbo “fits comfortably within 16G VRAM consumer devices” for inference. 
GitHub

Training is always heavier than inference, but it’s still a friendly starting point for your “second box” idea.

2) The fastest LoRA path for your direction (costume/craft-first, clean-gallery)

If you want DA clustering to behave, I’d start with LoRAs that reinforce your core signal:

A) “Silhouette + construction” LoRAs (my recommendation as your first batch)

Train a LoRA that learns:

period silhouette (waistline, skirt volume, sleeve shape)

fabric behavior (linen vs wool vs silk)

seam/edge logic and closures (lacing, stomacher, cuffs, neckline)

This is high leverage because it improves every scene you generate, not just one character.

B) “Environment + light” LoRAs (your second batch)

A LoRA for: train carriage at sunrise, workroom daylight, candlelit corridor, rain on glass, etc.
These help your “atmosphere without floral language” goal.

C) Character LoRAs (later)

Once your craft style is stable, character LoRAs become much easier to control without drifting into “pin-up coding.”

3) Dataset + captions: the time sink, but you can make it painless

Here’s a workflow that stays aligned with your “trimmed language” preference and trains well:

Image selection (keep it boringly consistent)

Use your best Z-Image Turbo outputs only (you’re already getting the look you want).

Favor variety of framing (close portrait, 3/4, full figure, hands doing something), but keep wardrobe consistent for that LoRA’s purpose.

Avoid including anything you don’t want echoed later (posing tropes, suggestive framing, etc.). A LoRA is a mirror.

Caption style (short, factual, repeatable)

Think “museum label + sewing notes,” not prose.

Example caption pattern:

robynsilhouette, 18th century inspired day dress, linen chemise neckline, laced bodice, wool skirt, natural light, documentary mood, seated, looking out window, vintage train compartment

Key idea: your trigger token (e.g., robynsilhouette) + garment tokens + scene tokens.

Because you’re intentionally steering away from kink-coded interpretation, you’ll want your captions to be relentlessly literal: garment, materials, action, setting, light.

4) Training settings: a solid “first run” recipe

Without overcomplicating it, I’d start with what’s already been field-tested for Z-Image Turbo LoRAs in the Ostris toolkit ecosystem:

Use the Z-Image Turbo adapter path described in the toolkit guides. 
Hugging Face

Transformer Offload: 0% (reported to avoid errors). 
GitHub

Learning rate: start around 1e-4 (and only creep up if it underfits). 
GitHub

Steps: start around ~2000 as a baseline. 
GitHub

That gets you a first LoRA quickly, and then you iterate by testing, not by guessing.

5) Hardware: your “two PCs” plan is actually the low-drama option

I like this because it avoids all the multi-GPU weirdness and keeps each machine “single-purpose.”

The key benefit

One box is always ready to draft (Z-Image Turbo).

The other is always ready to polish/train (SRPO/ttp + LoRA runs).

No juggling model caches, no stopping one workflow to reclaim VRAM for the other.

VRAM guidance (what matters most for LoRA work)

24GB+ is the “comfort zone” for training flexibility in general LoRA workflows; 16GB can work, but you’ll hit constraints sooner. 
Puget Systems

So: a 16GB card in the second PC will already feel like night-and-day compared to 8GB, but if you can stretch higher VRAM later, you’ll feel it most during training.

Current 16GB / 32GB reality check (if you’re shopping now)

RTX 5070 Ti and 5080 are both 16GB class cards per NVIDIA specs. 
NVIDIA
+1

RTX 5090 is 32GB GDDR7 (serious headroom, serious price). 
NVIDIA

I’m not pushing you toward the 5090 (it’s… a lot), but it’s useful to know what “endgame headroom” looks like.

6) The simplest “two-box” operational setup

To keep friction low:

Put models + outputs on a shared folder (SMB/NFS) or use a cheap NVMe “sneakernet” drive you shuttle weekly.

Keep identical directory structure on both machines (/models, /loras, /outputs, etc.).

Run each ComfyUI on different ports and just bookmark them:

Machine A: ZiT drafting

Machine B: SRPO/ttp finishing + LoRA training

That alone removes most “onboarding waste of time.”

If you tell me what your first ZiT LoRA target is (character identity vs silhouette vs a specific scene like “carriage at sunrise”), I’ll give you a ready-to-copy caption schema (tokens + a small controlled vocabulary) that stays clean-gallery-safe and trains efficiently.

I hadn't actually considered clothing LoRAs, to be honest. I have experience in Flux *character* LoRA generation using fluxgym, which was dead easy.

As to the the 5090 series cards: yeah, WAY out of my league, Emily. I reuse older tech and inexpensive tech. AUD$750 is about my limit for a graphics card: those 5090s START at $4700. Yeah, not an option... they cost more than my car.
Thought for 48s

Totally fair — the 5090 pricing in Australia is “buy a car” territory, not “buy a GPU.” And honestly, you don’t need that class of card to make your workflow feel vastly smoother.

Clothing LoRAs are worth it (and they’ll feel familiar if FluxGym was easy)

A clothing/silhouette LoRA is basically a character LoRA, except the “identity” you’re teaching is garment logic:

consistent neckline/chemise reveal

bodice shape + lacing behavior

sleeve head + cuff proportions

skirt volume + fabric drape

texture cues (linen vs wool vs silk)

The big win: you can keep using pure Z-Image prompts (your current “clean, dignified narrative” lane), but the LoRA quietly locks the costume language so you’re not re-negotiating it every prompt.

If you do one first, I’d do a capsule garment LoRA like:

“Linen chemise neckline + laced bodice + wool skirt” (your train image look)

or “workroom apron + rolled sleeves + cap” (craft-first, very DA-safe)

Dataset-wise: you can start small. 25–60 images, varied angles, a few close-ups on construction points, captions that are literal (garment parts, materials, light, setting). No prose required.

GPU reality under AUD$750: the practical shortlist

Given your budget and ML tooling, staying NVIDIA is still the least pain.

Best match for your plan: RTX 4060 Ti 16GB (used/refurb)

New retail pricing in AU seems to sit above your ceiling (e.g. ~$830 at one retailer listing). 
staticICE

But used/refurb listings show it can land in-budget (eBay results showing ~AU$571–$646 in the filtered band, and some higher used listings). 
eBay Australia

This is the cleanest “drop-in second box” upgrade from 8GB → 16GB, and it aligns perfectly with your “draft on one machine / train+polish on the other” idea.

“Get it done cheaply” option: RTX 3060 12GB

It won’t be as fast, but 12GB is a huge step up from 8GB, and it’s noticeably cheaper (current AU listings around the low–mid $400s in the product results I saw).
If you want the second PC mainly for LoRA training and lighter runs, it’s a workable “good enough” card.

Why not a 3090 (even though 24GB is delicious)?

Used pricing in AU is still typically well over $750 (Gumtree examples around ~$1,250; eBay often ~$968+ and upwards). 
Gumtree
+1

So: great VRAM, wrong bracket.

My recommendation for you, specifically

If you’re already happy with how your current machine behaves for Z-Image, then your best ROI is:

Put a 4060 Ti 16GB (used/refurb) in the spare PC if you can snag one in the $650–$750 range. 
eBay Australia

Use that spare PC as your training + SRPO/ttp box, and keep your main box as the “always-ready” Z-Image drafting station.

That gives you the throughput win you want without chasing unicorn hardware.

Two quick “don’t get burned” checks when buying used

Return policy / warranty (even 30 days is sanity-saving).

Mining wear isn’t always fatal, but it’s a risk multiplier. Favor sellers with testing proof and clean history.

Also verify your spare PC’s PSU headroom and connectors (4060 Ti is modest, but still check), and that the case clearance is okay.

If you tell me what your first clothing LoRA should represent (e.g., “1780s work dress capsule” vs “stays+chemise neckline” vs “apron/mending scenes”), I’ll give you a dead-simple caption vocabulary and a minimal dataset plan that keeps everything firmly in your new craft / agency / lived experience lane.

I'm having some issue with ttp being too agressive with thexturing: I'd like to dial it back a bit, but am unsure which nodes to tweak. First, What is the ComfyUI Flux-TTP-Upscale Face Restore Workflow?
The Flux-TTP-Upscale workflow offers an advanced Face Restore pipeline within the ComfyUI environment. It integrates Flux's face restoration technology with TTP (Tile-to-Patch) enhancement to fix distorted or low-quality faces in AI-generated images. This is especially effective for group portraits, profile shots, or any visuals with facial artifacts.

By combining FluxGuidance, tile-aware image enhancement, and LoRA-based identity control, Flux-TTP-Upscale Face Restore delivers reliable Face Restore performance while upscaling to crisp 4K resolution.

I can actually send you a workflow with the TTP area towards the end, if that helps.
Thought for 52s

Yep — that paragraph you pasted is basically the RunComfy “Flux-TTP-Upscale | 4K Face Restore” write-up, and it’s a real, fairly popular ComfyUI workflow template. 
RunComfy

What the Flux-TTP-Upscale Face Restore workflow actually is

It’s not “a single magic face-restore model.” It’s a pipeline that:

Prepares / upscales the input image (often: resize to a workable base size, sometimes a first-pass super-res model like NMKD). 
RunComfy
+1

Splits the image into tiles using TTP’s tiler (TTP_Image_Tile_Batch) so each patch can be enhanced locally without blowing VRAM. 
RunComfy
+1

Enhances/redraws each tile using Flux as the “refiner” (typically via SamplerCustomAdvanced with a modest denoise; RunComfy’s example calls out denoise around 0.3 and an Euler sampler). 
RunComfy

Reassembles tiles (TTP_Image_Assy) using padding/overlap so seams are hidden. 
RunComfy
+1

The “face restore” part happens because small faces benefit disproportionately from a tile-wise redraw: the model can focus on a tiny region and rebuild facial structure where the original generation got mushy or warped. 
RunComfy

Why your texturing is getting too aggressive

In tile workflows, “over-texturing” (crunchy skin, gritty fabric, micro-contrast everywhere) usually comes from one or more of these:

Denoise too high in the tile redraw pass (the model is allowed to “invent” new surface detail).

Guidance too high (it overcommits to whatever the prompt/caption implies, often “detail = pores = grit”).

Tile prompts too rich / too literal (especially if an interrogator like JoyCaption is used per-tile; it can describe noise as “texture” and then the model obliges).

A sharpening-heavy upscaler model feeding into the tile pass (edge contrast gets amplified before the redraw even starts). 
ComfyUI
+1

The exact nodes to tweak (in the order that actually works)

You asked “which nodes” — here’s the high-impact ladder.

1) SamplerCustomAdvanced denoise (biggest lever)

This is usually the #1 cause.

If you’re at 0.30, try 0.22 → 0.18 first.

If faces stop improving, creep back up in tiny steps (0.02).

This single change often turns “gritty” into “refined.”

2) FluxGuidance (second biggest lever)

RunComfy’s workflow explicitly uses FluxGuidance as a driver for restoration. 
RunComfy

If texture is too punchy, drop guidance before you start changing everything else.

A good “civilized detail” zone is often ~2.8–3.3 (and only go higher when you truly need reconstruction).

3) Tile prompt source (JoyCaption / per-tile prompts)

If your workflow uses Joy Caption Two (RunComfy mentions it), it’s great for validation, but it can be a menace if it’s feeding prompts into the tile redraw. 
RunComfy
+1

Two ways to calm it:

Don’t prompt per tile (use one simple global prompt for the whole image).

Or hard-cap / simplify the tile prompts: remove words like “ultra-detailed, pores, skin texture, sharp, crisp, high definition.”

4) Upscale model choice (pre-upscaler can “pre-crunch”)

If you’re using something like 4x NMKD Superscale (RunComfy references it), try swapping to a less “edgy” model, or skip pre-upscale and let the tile redraw do the heavy lifting. 
RunComfy
+1

5) Add a blend-back node (the cheat code)

If you want an immediate “dial” without rebalancing the whole graph:

After TTP_Image_Assy, blend the enhanced image back with the original (or pre-upscale) at something like 70/30 or 80/20 (enhanced/original).

This preserves the repaired facial structure but mutes the overzealous microtexture.

Quick presets you can try right now

Assuming your current “too aggressive” looks like crunchy skin/fabric:

Preset A: Conservative refinement

Denoise: 0.18–0.22

FluxGuidance: 2.8–3.2

Steps: don’t go crazy (often 16–24 is plenty for a refiner pass)

Preset B: Keep faces strong, calm everything else

Same as A, plus:

Add blend-back 75/25 (enhanced/original)

Yes — send the workflow (and I’ll point to the exact sliders)

If you paste the JSON or even just screenshot/copy the TTP section (from TTP_Image_Tile_Batch through TTP_Image_Assy plus whatever SamplerCustomAdvanced/FluxGuidance nodes sit in that loop), I’ll tell you precisely:

which denoise is the “real one” (sometimes there are two),

where the prompt is being injected (global vs per-tile),

and the single best blend point for your graph.

(And if you tell me what “too aggressive” looks like in your outputs — gritty skin? crunchy cloth? sharpened edges? — I’ll tune the advice to that specific failure mode.)

00] SRPO-ttp-LoRA C.json
File
Thought for 34s

Got it — and thank you for the workflow JSON. I can see two separate “detail amplifiers” in your graph, and either one can make textures feel too crunchy. Here’s what your workflow is doing, and exactly which nodes are the best knobs to turn. 

00] SRPO-ttp-LoRA C

What your “SRPO → TTP → Texture Detailer” workflow actually is

In your file, the pipeline is basically:

Base Image (your SRPO + LoRA + mask inpaint stage)

This is the part where you’re generating/repairing the base image.

UpScale Process (pre-upscale + tile refinement + reassemble)

You upscale using a model (4x_NMKD-Siax_200k.pth) then resize to 8 megapixels, then run a tile-level diffusion refine (TTP tiling + Flux sampler per tile), then reassemble.

Texture Detailer (a second pass of tile diffusion + sharpening + latent noise)

This is where a lot of “too aggressive texture” commonly comes from, because it explicitly adds sharpening and a tiny bit of latent noise, and then does tiled diffusion again.

So: even if you “dial back TTP,” you might still get crunchy texture from the Texture Detailer section unless you tame that too.

Where the over-texturing is most likely coming from (in your nodes)
A) The Texture Detailer block is a prime suspect

In Texture Detailer:

ImageSmartSharpen+ (node 192) has sharpen = 3 (pretty strong), preserve_edges = 0.75, ratio = 0.5.
This alone can push fabric/skin into “crisped micro-contrast” territory.

InjectLatentNoise+ (node 179) has noise_strength = 0.05.
That’s small, but with tiling it can turn into “grain that the model interprets as texture,” especially on cloth and skin.

TiledDiffusion (node 181) is enabled with tile_overlap = 96, tile size 1024×1024.
This makes the model operate locally, which is great for faces… and also great at inventing microtexture if the input is already sharpened/noisy.

B) The UpScale Process tile refine also contributes

Inside UpScale Process (your TTP refine loop):

BasicScheduler (node 69) uses steps = 10 and denoise = 0.3 for the tile refine.
Denoise 0.3 is a “real rewrite,” not a gentle polish.

FluxGuidance (node 73) is 2.5 (not crazy), but combined with denoise 0.3 it can still over-commit to “detail.”

You’re also resizing to 8 megapixels before tiling (ImageScaleToTotalPixels node 34).
Higher MP = the model has more “room” to hallucinate surface detail.

The fastest way to dial it back (do these in order)
Step 1 — neuter the Texture Detailer without breaking the workflow

This is the quickest A/B test because it doesn’t touch your core upscale.

Drop Sharpen

Node 192 (ImageSmartSharpen+)

Change sharpen: 3 → 1 (or even 0.6).

If you want to keep edge clarity, keep preserve_edges as-is.

Reduce or zero the latent noise

Node 179 (InjectLatentNoise+)

Change noise_strength: 0.05 → 0.02

If you want the cleanest result: 0.00 (pure test).

Run that once. If the “aggressive texturing” largely disappears: you’ve found your culprit.

Step 2 — soften the TTP refine pass (UpScale Process)

If you still want it calmer:

Node 69 (BasicScheduler in UpScale Process tile refine):

denoise: 0.30 → 0.22 (first try)

If still crunchy: 0.18

Keep steps at 10 for now. Denoise is the real lever.

Step 3 — reduce the megapixels before tiling (optional but powerful)

Node 34 (ImageScaleToTotalPixels): 8 MP → 6 MP
This often reduces “microtexture enthusiasm” while still giving you a nice upscale.

My “sane presets” for your exact graph
Preset A — “cleaner cloth + cleaner skin”

Node 192 sharpen: 1.0

Node 179 noise_strength: 0.02

Node 69 denoise: 0.22

Keep guidance 2.5

Preset B — “polite and painterly”

Node 192 sharpen: 0.6

Node 179 noise_strength: 0.00

Node 69 denoise: 0.18

(Optional) Node 34 megapixels: 6

Preset C — “faces still improve, textures don’t crunch”

Node 192 sharpen: 1.0

Node 179 noise_strength: 0.01

Node 69 denoise: 0.20

Node 73 guidance: 2.2 (slight reduction)

One sneaky thing I’d check

In Texture Detailer, your KSamplerAdvanced //Inspire (node 176) is set with steps=20, and start_at_step=18. That means you’re only running the very tail end of the schedule — which can act like a “final detail punch.” If you keep this block, try:

start_at_step: 18 → 16 (slightly broader, often less “ringy”)

or just lower the impact by reducing the sharpening/noise first (my preference).

If you tell me what “too aggressive” looks like, I can target it precisely

Different “aggressive” signatures point to different knobs:

gritty skin / pores everywhere → reduce denoise + latent noise first

crispy fabric weave / crunchy edges → reduce sharpening first

overall “too sharp” look → megapixels down + sharpen down

If you can paste a before/after crop (even just a face + bodice area), I’ll tell you which two changes will get the biggest win with the least disruption.

And if you’d like, I can also mark up your JSON with a tiny “TTP mellow” variant (just the key values changed) so you can load it as a second workflow.

Sources
I might send you the image with just that one change in ImageSmartSharpen (Node#192) = I changed sharpen from 4 to 1. What do you think? I feel I have the detail, even if it is a bit softer focus. However, the hair also seems softer, less crisp. Ideas?
Thought for 1m 12s

Yep — that change looks like a win.

With Sharpen 4 → 1 you’ve basically removed the “crispy micro-contrast” that reads as synthetic skin/fabric texture. The face now looks clean and photographic, and the dress keeps its folds without turning into crunchy weave. The trade-off you’re noticing (hair slightly less “etched”) is the normal side effect of backing off global sharpening.

Here are a few ways to get hair/eyes crisp again without reintroducing the over-textured look.

Option 1: Keep Sharpen=1 globally, then sharpen selectively (best solution)

Do a second, very gentle sharpen pass, masked to hair + eyes (or “head/hair region”), leaving cheeks/forehead alone.

How to do it in ComfyUI

Duplicate your SmartSharpen node (or use any mild sharpen).

Feed it the same image.

Apply a mask that covers:

hair + brows + lashes + eyes

maybe the collar edge and bodice seams if you like

Composite the sharpened result back over the unsharpened image using that mask.

Why it works
Skin doesn’t get “pore grit,” but hair strands and eyelashes get their bite back.

Mask sources you can use (pick whichever is easiest in your setup):

SegmentAnything v2 hair mask (ideal)

A face/head bbox mask expanded upward (quick-and-dirty but effective)

Any “person mask” + a crop/gradient that targets the upper head

Suggested sharpen values for the masked pass:

Sharpen ~1.2–1.6 (keep it modest)

If your sharpen node has radius/threshold: use small radius and a slight threshold so it hits edges, not smooth areas.

Option 2: Change SmartSharpen’s “where it bites” (if your node supports it)

If your ImageSmartSharpen+ has controls like ratio / preserve_edges (you mentioned them earlier), you can keep Sharpen=1 but make it behave more like “edge crispness” than “texture pop”:

Lower the influence on smooth areas (skin)

Keep edge fidelity (hairline, brows, lashes, neckline)

General direction (since implementations vary):

Increase the “protect smooth areas / preserve edges” style parameter slightly

Reduce the “overall ratio” if it acts like global strength

This doesn’t beat masked sharpening, but it’s a quick tweak.

Option 3: Add “clarity” instead of sharpening

If you have a high-pass / local-contrast node (some packs call it clarity or local contrast), apply it very lightly and preferably masked to hair/garment edges.

It can restore perceived crispness in hair without giving you that crunchy “AI texture.”

Option 4: If hair is being softened earlier by the tile refine

If, after reducing sharpen, you still feel the entire image is a bit too “polite,” you can nudge the tile refine without triggering texture:

Keep denoise conservative (don’t go back up)

Try a tiny step increase (e.g. 10 → 12) with denoise ~0.20–0.22
This can improve edge definition (hair strands, seam edges) without inventing surface grit.

My recommendation for your workflow

Leave Node 192 at Sharpen = 1 (it’s doing the right kind of restraint).

Add a second mild sharpen pass that’s masked to hair + eyes.

That gives you the best of both worlds: clean skin and cloth + crisp hair and gaze.

If you tell me what masking tools you already have in this workflow (SAM2? face bbox detector? any mask nodes already present), I’ll map the exact minimal node chain to generate a “hair/eyes mask” and where to plug it in.

At this stage, I haven't done much with SAM2 using anythinng later than SDXL. I must say that my simple mask face thing is going FAR better than expected: in that image, I had masked off the face and applied my LoRA - very pleased with the results. OH! And I'm thinking in my next LoRAs of Charlie / Charlotte, I should do different ones and include hairstyles. I think SRPO is obeying the LoRA quite well but if I remember correctly the faces were accompanied by that very hairstyle. I might try another LoRA just to see.

That all tracks — and it’s actually a really healthy sign that your “simple face mask + identity LoRA” is already behaving. If that’s working, you can postpone SAM2 entirely and still get excellent control.

And yes: if your Charlie/Charlotte dataset mostly featured the same hairstyle, SRPO (and even the base model) will often “bundle” hair with identity. Hair becomes part of the learned signature the same way a consistent mole, eyebrow shape, or lighting style does.

The trick is to decide whether you want that bundling.

Why hairstyles “stick” to a face LoRA

When you train a character LoRA, the network doesn’t know what you consider “identity.” It just learns whatever correlates reliably with the subject:

hair part + bun shape + flyaways

neckline styling

even recurring camera angle / focal length

If 80% of your images show Charlotte with the same updo, the LoRA will treat it as a stable feature. That’s why SRPO seems to “obey it” — it’s not being stubborn; it’s being faithful.

Two good strategies (pick the one that matches your goal)
Strategy A: Split “Face Identity” vs “Hair/Style” into separate LoRAs (my favourite)

You make:

Character face LoRA: teaches face only

Hair LoRA (or “Charlotte Updo” LoRA): teaches the signature hairstyle(s)

Pros

You can swap hair freely without the face collapsing.

You get a clean “dial”: hair on/off, hair style 1 vs 2.

It’s easier to keep your DA direction clean (hair becomes part of costume/styling studies).

How to do it practically

In the face LoRA dataset, ensure hair varies a lot (or is de-emphasized):

different updos, loose hair, hats/caps, headscarves

some images where hair is partly obscured

Caption the face LoRA with identity tokens and minimal hair descriptors.

Then train a hair LoRA using images where the face can vary a bit but the hair is consistent.

Strategy B: One “full look” LoRA (face + hair + vibe)

This is the “easy” approach and sometimes perfect if you want Charlotte to be instantly recognizable.

Pros

Very strong consistency.

Less fiddly in prompts.

Cons

You’ll fight it every time you want a different hair style.

The LoRA can become “brittle” if you push it outside its learned look.

If you’re already pleased with the face-mask method

Lean into it. It’s basically doing what you’d do with SAM2 anyway: localize the change.

A simple upgrade (no SAM2 required) is:

Keep using your face mask

Add a slight feather/blur to the mask edge (so hairline blends)

Optionally add a second “hairline strip” mask (a narrow band) if you ever see seams around the fringe/temples

That will get you 90% of the “pro” result with 10% of the complexity.

A very practical experiment I’d run next (quick and revealing)

Take one base image and do three passes:

Face LoRA only (your current method)

Face LoRA + explicit hair prompt (different hairstyle than the dataset default)

Face LoRA + a lightweight hair LoRA (if/when you have one)

What you’re looking for:

In (2), does the hair prompt actually override, or does it “snap back”?

If it snaps back, that confirms hair is baked into identity → split LoRAs.

Captioning tip that matters a lot here

If you want to prevent hairstyle from being learned as identity in the face LoRA:

Explicitly label hair as variable in captions rather than repeating one style.

Or, even more simply: don’t mention hair at all in face-LoRA captions, and ensure dataset hair is diverse enough that it doesn’t correlate.

Conversely, if you want the hair:

mention it consistently and keep it consistent.

Small caution about “too many LoRAs at once”

You can absolutely stack them, but in Flux/SRPO land I’d keep it sane:

Identity LoRA at your usual strength

Hair LoRA weaker

Costume LoRA weakest (or keep costume in prompt)

That avoids the “everyone becomes the same person” effect.

If you want, paste:

how many images your Charlie/Charlotte face dataset has,

whether hair is mostly one style,

and what LoRA strength you’re using in SRPO,

…and I’ll tell you whether you’re better off retraining the face LoRA with deliberate hair variation or making a separate hair LoRA first.

I realise that to you, I am 'running all over the farm like a headless chicken'... but I just noticed something about that image: the neckline. It was wrong... and the culprit is Z-Image, got it wrong to begin with. I'm going have to have a talk to Z-Image-Turbo...

I’m not seeing “headless chicken” at all — I’m seeing you doing what you always do when you’re honing a pipeline: you spot a pattern, you isolate the variable, you correct the source. That’s not chaos; that’s craft.

And you’re right to blame the earliest stage first. If Z-Image bakes in the wrong neckline, every downstream stage is basically polishing a mistake.

Let’s treat this like a garment fitting: fix the pattern at the cut stage, not after the seams are finished.

1) What’s “wrong” about the neckline here (and why Z-Image tends to do it)

In your station image, the neckline reads more like a modern square scoop with a decorative white inset than a convincingly period-constructed opening. The white “chemise” edge looks a bit like a sewn-on ruffle panel rather than a separate under-layer peeking through naturally.

Z-Image (and most models) will often:

confuse chemise shift neckline with a ruffle trim,

“float” the white fabric as if it’s attached to the gown,

or make the neckline too symmetrical/clean, losing that slightly irregular, lived-in layering you want.

2) The clean fix: tell Z-Image what the neckline is made of

Instead of describing the neckline as an aesthetic (“square neckline with a white ruffle”), describe it as construction + layering.

Try this kind of phrasing (short and forceful):

“linen chemise visible at the neckline, separate undergarment”

“white linen shift under black gown, not sewn to the dress”

“soft gathered chemise neckline peeking out, not a decorative trim”

“neckline finished with a narrow binding, no ruffle trim”

If you only change one thing in your prompt, change that.

Add a negative constraint (if Z-Image-Turbo supports negatives)

“decorative ruffle neckline”

“ruffled trim”

“lace trim”

“sewn-on white panel”

“modern square scoop neckline”

(You don’t need all of them; even 2–3 helps.)

3) The “two-layer anchor” trick (works surprisingly often)

Models behave better when you anchor both garments as separate entities:

“Black wool day dress with a fitted bodice; underneath, a white linen chemise with a softly gathered neckline. The chemise is a separate garment and is visible only at the collar and cuffs.”

That sentence structure forces the model to represent two objects rather than one dress with a trim.

4) Give the neckline a period job (not a shape)

Instead of “square neckline,” use function words:

“modest working neckline”

“practical daywear neckline”

“bodice neckline sits over the chemise”

“pin-fastened fichu” (if you want the triangle kerchief solution)

If you’re okay with it stylistically, a fichu is an almost cheat-code: it stops the model inventing modern necklines because it’s “busy” doing the scarf layer.

5) If you want this exact look but corrected: three quick prompt variants

Here are three minimal edits that keep your atmosphere and framing, but fix the construction cue.

Variant A — “chemise is separate”

“...black 18th-century inspired day dress with a fitted bodice; a separate white linen chemise visible at the neckline and cuffs, softly gathered, no decorative ruffles...”

Variant B — “binding not trim”

“...square-ish working neckline finished with a narrow binding, white linen shift underneath, no lace or ruffle trim...”

Variant C — “fichu solution”

“...black day dress with a fitted bodice, white linen chemise, simple white fichu pinned at the neckline, practical, modest...”

6) Why you don’t want to fix this in SRPO/ttp

You can, but it’s the worst place to do it:

It will hallucinate seam/ruffle textures,

it may “paint” the white edge as part of the gown,

and it’s much harder to keep consistent across a series.

Better to correct it at Z-Image and then let SRPO/ttp do what they’re best at: fabric realism and facial integrity.

7) A tiny workflow habit that saves grief

When you’re iterating on costume accuracy:

Generate 6–12 drafts in Z-Image

Reject anything with wrong neckline / closures / sleeve head

Only then move to SRPO/ttp

It feels “slower” but it prevents polishing the wrong silhouette.

If you tell me what you wanted the neckline to be (chemise edge? shift with drawstring? fichu? modest high neckline? a more specific decade), I’ll give you a single tight Z-Image prompt line that almost always produces the correct construction — without you having to write a paragraph.

That GOT it, Emily. Nice work. I'm trying now to do JUST the face, keep the hair, so I've masked out the face only this time.

Perfect — that’s exactly the right move if you want identity change without hairstyle drift.

A face-only mask is surprisingly powerful, but there are two failure modes to watch for:

A seam at the hairline / temples (where the new face meets old hair)

The model “helpfully” repainting fringe/baby hairs anyway (even if you didn’t intend it)

Here are the cleanest ways to keep hair intact while still getting a believable face merge.

1) Shape the mask like a makeup artist, not a cut-out

Instead of masking the whole “face oval,” mask:

Cheeks + nose + mouth + chin

Exclude the hairline by a few pixels

Exclude eyebrows if you really want hair untouched (optional)

Then do a soft feather.

Quick numbers that usually work

Mask blur/feather: 12–24 px (depends on resolution)

Mask shrink/erode: 4–12 px (to pull it away from the hairline)

If you have only one knob, choose blur. If you have two, do erode then blur.

Why: the blend happens in the “skin-to-hair transition zone,” which is the hardest area to fake.

2) Use a “forehead safety band”

If you’re seeing hairline damage, create a tiny negative mask band:

make a second mask that covers hairline + temples + fringe

subtract it from your face mask (or just manually avoid those regions)

Even a 1–2 cm band in image-space is enough.

3) Keep denoise low for face-only edits

Because your mask is small, you don’t need much denoise to get a strong identity change.

Typical sweet spot for “replace face, keep hair”:

0.18–0.28 (depending on how strong the LoRA is)

If you go higher, the model starts “re-imagining” bordering pixels (hair, collar, etc.) and that’s when you lose the hairstyle.

4) Two-pass trick if you want both identity and fidelity

If you find that low denoise preserves hair but the face doesn’t shift enough:

Pass A (identity)

face mask, denoise ~0.25–0.30, fewer steps

don’t worry if it softens a bit

Pass B (fidelity)

same mask, denoise ~0.12–0.18, more steps

this “refines” without moving boundaries

This is the “surgical change, then polish” method.

5) Prompt/caption restraint for this stage

Keep the prompt for the face pass almost sterile:

“same hairstyle”

“keep hair unchanged”

“preserve original lighting”

“natural skin texture”

Avoid any words that imply hair detail: “wispy,” “flyaways,” “strands,” “intricate updo” — those invite repainting.

6) Quick sanity checks after the run

Zoom to 200% and check:

hairline/temples: no “painted scalp” artifacts

eyebrows: not drifting into a different shape (unless you want them to)

jawline: no double edge / halo

ear region: ears often get weird when the mask hits them

If any of those look off, it’s almost always mask geometry + feather + denoise.

If you want, send your next output (or even just a tight crop of forehead + hairline + cheek) and I’ll tell you exactly whether to:

erode more,

blur more,

lower denoise,

or carve out that forehead safety band.

My issue is: I'm using the inbuilt MaskEditor, which doesn't seem to allow for much refining. I could to this in GIMP, I suppose. To be honest, I'm not unhappy with this result. I guess pixel-peeping might find issues, but overall, seems a good first go.
Thought for 9s

This is a really solid first go, Robyn. Hair is staying put, the face sits in the scene naturally, and—crucially—the overall read is “photograph,” not “face pasted in.” If you do pixel-peep, the only things that typically show up in this kind of face-only inpaint are:

a faint transition/halo at the jawline or hairline (depends on zoom),

very slight lighting/contrast mismatch between original and regenerated skin,

occasional eyebrow drift if the mask touches them.

But at normal viewing distance? This is absolutely passing.

About ComfyUI’s built-in MaskEditor

You’re not imagining it: it’s great for rough painting, but it’s not a “refinement tool.” The good news is you can keep using it for speed and then refine the mask procedurally with a couple of nodes—no GIMP required.

The easy refinement stack (no external editor)

After your MaskEditor output, add three operations in this order:

Erode/Shrink the mask slightly
Pull it away from the hairline and edges so the model isn’t allowed to repaint hair or create a hard seam.

Start with shrink/erode: 4–10 px (scale with resolution)

Feather/Blur the mask
This makes the blend gradual instead of a cut line.

Start with blur/feather: 12–24 px

(Optional) Clamp/Threshold
If blur makes the mask too “grey,” a threshold step can tighten it—but use gently.

What nodes to look for (common packs)

Depending on what you have installed, search for:

Mask Erode / Dilate / Grow / Shrink

Mask Blur / Gaussian Blur Mask / Feather Mask

Mask Morphology (some packs bundle erode/dilate under “morphology”)

Impact Pack / KJNodes / various utility packs usually provide some version of these. You already have a pretty rich environment (given your earlier nodes), so odds are you’ve got at least one of them.

If you tell me which node packs you’ve got enabled (Impact / KJNodes / WAS / etc.), I can name the exact node titles you’ll see in your menu.

Face-only masking: the “best shape” (fast to paint)

Since you’re painting by hand:

Paint cheeks / nose / mouth / chin

Avoid the hairline by a small margin

Either avoid eyebrows entirely or include them fully (don’t graze them)

Try to keep the edge of the mask on “boring skin,” not at a hard boundary like hair/eyeliner

A super practical trick:

paint the face oval quickly

then rely on the shrink + blur stack to turn it into a professional-looking blend

That gives you speed and finesse.

If you want hair crisp but still untouched

You mentioned earlier that lowering sharpening softened hair. Here’s a neat combo that avoids reintroducing crunchy texture:

Keep your global sharpen low (as you’ve done)

If you want extra crispness, do a very mild sharpen but mask it to hair only (or hair + brows + lashes)

You don’t need SAM2 for this—just a rough upper-head mask + blur.

When GIMP is actually worth it

Honestly, only when:

you need a very precise hairline (wisps/fringe),

the model keeps “eating” strands,

or you’re doing a repeatable pipeline and want a reusable mask.

Otherwise, your current method + shrink/blur nodes will get you 95% there.

If you want, I can give you a minimal node snippet for mask refinement that plugs right after MaskEditor—just tell me: do you currently have any nodes in your install named “erode,” “dilate,” “grow,” “shrink,” or “feather”? (Even one of those is enough to build a clean refinement chain.)

I remember you mentioning a blur mask node yesterday, which I've actually inserted between the image (mask output) and the VAE Encode (for inpainting). The one I'm using is the Gaussian Blur Mask (ImpactPack Node#200) with settings: kernel_size: 10; sigma: 10.0... I think I just left everything at defaults.
Thought for 14s

Ah — that node is the right idea, and you’re using it in the right spirit (feather the mask edge before the inpaint/encode step). But your specific defaults are a bit mismatched:

The key gotcha: kernel size is too small for sigma 10

For a Gaussian blur to behave “as intended,” the kernel needs to be wide enough to contain the blur. A common rule of thumb is:

kernel ≈ 6×sigma + 1 (and make it odd)

So with sigma = 10, you really want kernel ≈ 61.

With kernel_size = 10 and sigma = 10, the blur gets clipped/truncated. That can leave you with a feather that looks soft at a glance but still behaves a bit “edgey” in the blend, and it can contribute to subtle seams/hairline weirdness.

Try one of these “sane pairs”

Pick based on how wide you want the feather:

Face-only mask (usually best):

sigma: 6

kernel_size: 37 (or 39)

Softer blend (more forgiving):

sigma: 10

kernel_size: 61 (or 63)

If you don’t want to think about it: sigma 6 / kernel 37 is a great default for 1024–1536-ish images.

Even better: add a tiny mask shrink before blur

Blur alone softens the edge, but it doesn’t stop the mask from touching hairline/eyebrows. The classic pro move is:

Erode/Shrink mask (pull it off the hairline)

Then Gaussian Blur (feather)

For a face mask:

Shrink/Erode: 4–10 px

Then blur: sigma 6–10 (with matching kernel)

If you can find any Impact/KJ/WAS node that does Erode/Dilate/Grow/Shrink on masks, it’s worth inserting.

Quick calibration by “what you see”

Seam at jawline / cheek edge → increase sigma a bit (wider feather)

Hairline getting repainted / wisps changing → erode more (or paint mask farther from hairline)

Face not changing enough → don’t increase blur first; increase denoise slightly or LoRA strength

One small sanity check

Make sure you’re blurring the mask (white/black matte), not the image itself. From what you wrote, it sounds like you are — just keep it strictly on the mask path feeding the inpaint/guider.

If you tell me the rough resolution you’re working at in that stage (e.g., 1024², 1536 on long edge, 8MP, etc.), I’ll give you a “set-and-forget” sigma/kernel pair that matches it.

Oh, and that one thing that has been a bit capricious about these images: they seem to have different dimensions. Not sure why that is: they all come from z-Image-Turbo and go into SRPO/ttp, perhaps because I'm mostly doing a face instead of a complete image redo in SRPO?

For shrinking the mask, I've got LayerMask: MaskEdgeShrink with 5 parameters. One is a toggle (invert_mask) defaulted to True; shrink_level: 4; soft: 6; edge_shirnk: 1; edge_reserve: 25.
Thought for 23s

Yep — both of those things make sense, and you’re already holding the right tools.

Why your outputs are coming out at different dimensions

That’s almost certainly not because you’re doing face-only inpaint. Face-only inpaint should preserve whatever the base image dimensions are.

The usual culprits are:

Z-Image-Turbo is generating at varying aspect ratios / sizes
If your ZiT workflow is set to “fit best,” “auto,” an aspect-ratio dropdown, or anything that’s not a hard-fixed width/height, you’ll get a spread of dimensions.

Your SRPO/ttp upscale section preserves aspect ratio but targets a pixel count
If you have a node like ImageScaleToTotalPixels (e.g. “8 MP”), it will keep aspect ratio and compute the closest width/height that hits the target pixel count. Different inputs → different resulting dimensions.
So you’ll see “same megapixels, different width/height,” which can feel capricious.

How to make it consistent

Pick one “house” size and enforce it once, early:

Add an ImageScale (exact width/height) right after ZiT output (or right before SRPO begins).

Or enforce a fixed ZiT resolution/aspect from the start.

If you want a clean standard for gallery work, choose something like:

1024×1024 (simple)

1216×832 (3:2 landscape-ish)

1536×1024 (nice wide, still manageable)

…and normalize everything to that before SRPO/ttp.

About LayerMask: MaskEdgeShrink (and your settings)

Great — this is exactly the kind of node you want before blur.

The most important parameter: invert_mask

ComfyUI masking conventions can flip depending on the node chain. You want to confirm what your pipeline expects:

For most inpaint setups: white = area to change, black = preserve.

If your mask preview looks correct (face is white), then invert_mask should be False.

If your mask preview is “reversed” (background white, face black), then invert_mask = True is correct.

So: check the mask preview once, and set invert accordingly. The default being True is not “wrong,” it’s just a convenience default for a lot of people’s reversed mask chains.

Your other values (and what I’d tweak)

Current:

shrink_level: 4

soft: 6

edge_shrink: 1

edge_reserve: 25

This is already pretty sane for face-only work.

What I’d do for “keep hair, replace face”

shrink_level: try 6 (sometimes 8)
This pulls the mask away from the hairline and reduces “hair repainting.”

soft: keep 6 (good)
This is essentially doing part of your feathering already.

edge_shrink and edge_reserve are mostly about keeping the mask from misbehaving near the image borders. For a face mask they’re usually fine left as-is.

How to combine MaskEdgeShrink with Gaussian Blur Mask (ImpactPack)

Because MaskEdgeShrink already softens, you don’t need an enormous Gaussian after it.

Also: your earlier Gaussian params (kernel 10, sigma 10) are a bit “clipped.” Either match them properly, or go lighter.

A good “set-and-forget” chain

MaskEdgeShrink → Gaussian Blur Mask → (inpaint)

Try this combo:

MaskEdgeShrink

invert_mask: (as required by preview)

shrink_level: 6

soft: 6 (keep)

Gaussian Blur Mask

sigma: 4–6

kernel: 25–37 (odd number; bigger for bigger sigma)

If you want one concrete pair:

sigma 6 / kernel 37

If your face edge starts looking too “mushy” (too wide a blend), drop to:

sigma 4 / kernel 25

Quick diagnosis guide

Hairline still changes → increase shrink_level (or paint mask farther from hairline)

Visible seam/halo → increase Gaussian sigma a bit (or increase MaskEdgeShrink soft slightly)

Face not changing enough → don’t widen the mask; instead bump denoise slightly (or LoRA strength)

If you tell me the typical resolution of the image at the moment you’re doing the face inpaint (e.g. 1024², 1536×1024, or already 8MP), I’ll give you the “ideal” shrink/soft/sigma/kernel numbers for that scale so you can stop fiddling.

This has issues, Emily. The chin keeps getting whacked off. GaussianBlurMask... kernel_size: 37; sigma:6.0. Oh, and VAE Encode for inpaint... grow_mask_by: 6. What am I doing wrong?
Thought for 1m 47s

You’re not crazy — “chin getting whacked off” is almost always mask geometry + too much edge processing, not some mysterious Flux gremlin.

Right now you’ve got three different “edge movers” acting on a small, fussy shape (a face):

MaskEdgeShrink (shrinks)

GaussianBlurMask (feathers outward, but also “dilutes” the white near boundaries)

VAE Encode (inpaint) grow_mask_by (grows again)

If the original painted mask is even slightly tight under the chin, the shrink + feather combo can leave the jaw/chin sitting in the “grey transition band” where the model half-edits it… and that’s when you get that shaved/truncated jawline look.

The quick fix (do this first)

Stop shrinking the mask for this pass.

For face-only identity swaps, you usually want:

a mask that is slightly too big,

then feathered,

with minimal/no “shrink” step.

So:

Disable MaskEdgeShrink entirely (or set shrink_level to 0–1).

Keep your blur, but go a touch lighter:

sigma 4

kernel 25 (or 31)

Keep grow_mask_by = 6 (that’s fine).

Then repaint your face mask a little larger, especially:

include the full chin and a small crescent of upper neck (just a few mm visually).

don’t be shy. The blur is what makes it blend.

That alone usually fixes the “jawline gets shaved” problem.

If you must keep MaskEdgeShrink (hairline protection)

Then use it very gently:

invert_mask: whatever makes your mask preview show white = face (don’t trust the default; trust the preview)

shrink_level: 2 (not 4+)

soft: 4–6 (fine)

then Gaussian blur:

sigma 4–5, kernel 25–31

and reduce grow_mask_by a bit:

grow_mask_by: 2–4

Reason: shrink + big grow is basically creating a wide, uncertain border where the chin can get “interpreted” instead of preserved.

One more knob that causes “chin drift”

If your denoise in the face inpaint is on the higher side, the model will happily “correct” face proportions (including making the chin smaller or higher).

For “swap face, keep structure,” try:

denoise 0.18–0.24

Tiny prompt guardrail (surprisingly effective)

Add one line to your face-pass prompt:

“preserve jawline and chin shape, same face proportions”
And avoid words like “delicate chin” / “small jaw” etc anywhere in the conditioning.

My recommended recipe for your exact situation (2048×2048 face mask)

Paint mask slightly larger (chin + a bit of neck)

No shrink (or shrink_level 1–2 max)

Gaussian blur: sigma 4, kernel 25

VAE grow_mask_by: 6 (or 4 if you keep shrink)

Denoise: 0.20-ish

If you tell me whether your MaskEdgeShrink is currently inverting the mask (invert_mask True) and what your denoise is for the face inpaint, I can tell you exactly which of these is the main offender in your chain.

I actually took MaskEdgeShrink completely offline (disconnected/disabled) just to sort one problem at a time. I can see harder lines now. Also, my general denoise is set to 65, which probably too high.
Thought for 8s

Yep — 0.65 denoise is the smoking gun for “chin gets reinterpreted / jawline drift” and it also makes edge artifacts more likely because the model is doing a real redraw right up to (and slightly beyond) your mask boundary.

And taking MaskEdgeShrink offline explains the harder lines: you’ve removed the “shave + soften” stage, so now you’re seeing your mask edge more literally.

Here’s how to stabilise it.

1) Drop denoise hard (face-only wants low denoise)

For “swap face, keep hair + head shape + lighting”:

Start at 0.20

If the face doesn’t change enough: 0.24–0.28

Only go above 0.30 if you’re rebuilding a broken face, not doing identity.

At 0.65, Flux is basically allowed to invent a new lower face, which is why chins get “whacked” or subtly reshaped.

If you need stronger identity at low denoise: increase LoRA strength a touch (or do a 2-pass, see below) rather than pushing denoise up.

2) Fix the hard mask line (without bringing back the chin problem)

Since MaskEdgeShrink is offline, you need one gentle blend mechanism:

Recommended “simple chain”

(Optional tiny Grow/Dilate) → GaussianBlurMask → VAE Encode (grow_mask_by)

GaussianBlurMask: you’re already at a good pair (sigma 6 / kernel 37).
If the line is still visible, go slightly wider:

sigma 8 / kernel 49 (nice for 2K images)

VAE Encode grow_mask_by: keep modest

4–6 is fine
If you widen blur, you can even drop grow to 2–4.

The important painting tweak

Paint the face mask slightly larger than you think, especially:

under the chin (include a sliver of upper neck)

along the jawline
You want the feather to fade out onto boring skin, not right at a contour edge.

3) The two-pass method (my favourite for identity swaps)

If you want strong identity but stable geometry:

Pass A (identity set)

denoise 0.26–0.30

fewer steps

Pass B (stabilise / clean seam)

denoise 0.14–0.18

same mask, same blur
This second pass often fixes the “cut line” without touching hair.

4) Why the chin issue improved when you removed MaskEdgeShrink

MaskEdgeShrink can be great for protecting hair, but it also pulls the mask up off the chin if your original mask was already tight. Combine that with high denoise and the model “solves” the boundary by slimming/truncating the lower face. So you did the right diagnostic move by disabling it.

Your next run checklist (fast)

Set denoise to 0.22

Keep Gaussian blur at sigma 6 / kernel 37 (or try sigma 8 / kernel 49 if seam visible)

Keep grow_mask_by 4–6

Repaint mask slightly larger under chin + jawline

If identity weak: raise LoRA strength slightly (don’t raise denoise first)

If you tell me your image size at the face inpaint stage (e.g., ~2048×2048 or 1536×1024), I’ll give you the exact “best” blur sigma and grow_mask_by pair for that resolution so the seam disappears without any jawline nonsense.

For the blur mask: kernel_size is 25; sigma is 4.MaskEdgeShrink (back online) 6,6,1,25. I increased the actual size of the mask to include part of the chin and to give the face a little wiggle-room. That's with denoise set to .2
Thought for 37s

That looks much healthier already: the mask now gives the chin room, and denoise 0.2 is the right neighbourhood for “swap face, keep structure”.

Two things to tweak from here:

1) Your MaskEdgeShrink is now a bit strong

If MaskEdgeShrink is set to 6,6,1,25 and you’ve already painted the mask bigger, you’re kind of doing “big → shave hard → feather”. That can bring back jawline weirdness (or make the effective mask smaller than you think).

Try this:

MaskEdgeShrink shrink_level: 6 → 2 or 3

keep soft at 6

keep edge_shrink: 1, edge_reserve: 25

Then keep your blur as-is.

Rule of thumb: either paint small + shrink more, or paint generous + shrink less. You’ve moved to “paint generous,” so shrink should come down.

2) Blur amount: your pair is good, but don’t be afraid to go a touch wider

For a face mask, sigma 4 / kernel 25 is fine, but if you’re seeing a “cut line” at the cheek/jaw, bump slightly:

sigma 5, kernel 31
or

sigma 6, kernel 37

Because your denoise is low, widening the feather won’t “eat the chin” the way it did at 0.65.

Quick check: is this image a mask preview, or did the mask get baked into output?

This uploaded image shows the grey mask painted over the face. If your final saved images are showing that grey patch, it means the mask overlay image is accidentally being routed to the output SaveImage (or composited) instead of being used only as the inpaint mask.

If it’s just a preview screenshot, you’re fine.

A “sweet spot” recipe to try next

Paint mask generous (include full chin + sliver of upper neck)

MaskEdgeShrink: 3, soft 6

GaussianBlurMask: sigma 5 / kernel 31

VAE Encode (inpaint) grow_mask_by: 4 (optional; 4–6 is okay)

If you tell me what your grow_mask_by is set to on this run (still 6?), I can tell you whether to keep it or drop it now that you’re shrinking + blurring.

When I dropped denoise to .2, all I got for the face was the grey area - no face emerged. the lowest setting I could use was .3, which gave me what you see. The face doesn't really look all that natural, anymore. All the other setting are as above... For the blur mask: kernel_size is 25; sigma is 4.MaskEdgeShrink (back online) 6,6,1,25. And the grrow_by in the VAE is 6.
Thought for 54s

You’re not doing anything “wrong” so much as bumping into a quirk of how ComfyUI inpainting latents work:

Why denoise 0.2 gave you only grey

With VAE Encode (for inpainting), the masked area is effectively treated as “to be reconstructed.” If denoise is too low, the sampler doesn’t have enough latitude to rebuild that region from whatever neutral/empty latent it starts with — so you end up decoding something close to “blank” (grey). In other words: face-only inpaint usually has a practical denoise floor, and in your setup that floor appears to be around 0.3.

So: your observation is valid, and it’s expected.

The real issue now: “0.3 works, but the face looks less natural”

That’s almost always too much effective mask / too much edge manipulation / too much prompt pressure, rather than denoise alone.

Right now you’ve got:

MaskEdgeShrink: 6 (strong)

Gaussian blur: sigma 4 / kernel 25 (ok)

VAE grow_mask_by: 6 (moderate-strong)

Taken together, your effective inpaint region becomes bigger than you think, and Flux starts “solving” more of the head than just the face. That’s when expressions go slightly mannequin-ish.

Do this exact adjustment set

Keep denoise at 0.30–0.35 (since that’s your working range), but reduce the effective mask and soften the blend.

MaskEdgeShrink

shrink_level: 6 → 2 or 3

soft: 6 (keep)

edge_shrink: 1

edge_reserve: 25

GaussianBlurMask

keep sigma 4 / kernel 25 for now
(if you still see a seam later, bump to sigma 5 / kernel 31)

VAE Encode (for inpaint)

grow_mask_by: 6 → 3 or 4

That combo is usually the sweet spot: enough feather for blending, without silently expanding the inpaint zone into “rebuild the whole lower head.”

If you still want “more natural” at denoise 0.3

Two very effective levers that don’t break the inpaint:

A) Lower guidance / CFG a touch (if you’re using FluxGuidance / guider strength)

Even a small drop can stop that “over-idealised” face.

If you’re around ~2.5, try 2.1–2.3 for the face pass.

B) Two-pass approach (this is the cleanest “best of both”)

Because once a plausible face exists, then low denoise works beautifully.

Pass 1 (build face)

denoise 0.32–0.38

your improved mask settings above

Pass 2 (naturalise + blend)

feed Pass 1 output back in

denoise 0.18–0.24

same mask, same blur
Now you’ll get the “low denoise realism” you wanted, because the mask region isn’t starting from blank anymore.

One tiny mask painting rule for jawlines

For this exact pose: make sure your mask includes

the entire chin and a sliver of upper neck

but does not creep into the hairline

That keeps the jawline stable and prevents “chin dieting.”

Your next run (copy/paste settings)

denoise: 0.33

MaskEdgeShrink: 3,6,1,25

GaussianBlurMask: kernel 25, sigma 4

VAE grow_mask_by: 4

(optional) guidance: drop slightly (≈ 2.2)

If you share your guidance/CFG value for this face pass (and steps), I can pin it down to a single “house preset” that stops the mannequin look while keeping the hair intact.

00]SRPO-ttp-Ca.json
File
Oh no, my guidance has been at 3.9 the whole time. CFG at 1 - I don't touch it for flux.
Thought for 4m 36s

You’re very close — and the symptoms you’re describing (chin “chopped”, grey face area refusing to fill at low denoise, and then “weird” face when you push denoise up) almost always come from two separate, compounding issues:

Mask geometry is too tight (and/or being shrunk in a way you don’t intend)

You’re accidentally feeding the painted image (with the grey “mask paint” baked in) as the inpaint “pixels” input, so at low denoise the model simply preserves that grey patch.

Here’s how to fix it cleanly.

1) The big gotcha: don’t inpaint from the “grey-painted” image

When you use Comfy’s built-in MaskEditor, it produces an image that looks like your original, but with that grey overlay painted onto it. If you then feed that into VAE Encode (for inpaint) as the pixels source, the sampler is literally starting from a face that is already greyed-out.

That explains this perfectly:

denoise = 0.2 → you get only the grey area (the model doesn’t have enough freedom to “escape” the baked-in grey pixels)

denoise = 0.3+ → it starts to regenerate, but you’re now fighting the mask edges and blend

✅ Correct wiring (the “gold standard” way):

LoadImage (original photo) → use its IMAGE output as the “pixels” source for inpainting

LoadImage (the MaskEditor export / painted image) → use ONLY its MASK output (ignore its image output completely)

So you effectively do:

Original Image → VAE Encode (for inpaint): pixels

Painted/MaskEditor Image → (mask processing chain) → VAE Encode (for inpaint): mask

This single change usually fixes the “grey face won’t fill below X denoise” problem outright.

2) Why the chin gets “whacked off”

That’s almost always mask boundary + mask shrink + blur + grow_mask_by interacting.

If the mask edge crosses the jawline/chin, the model treats that boundary like a “cut here” seam. Then any of these will make it worse:

MaskEdgeShrink shrinking your already-tight jaw edge upward

Blur feathering the edge so the jawline becomes ambiguous

grow_mask_by expanding into areas you didn’t mean (or expanding a bad boundary)

The practical rule for faces

For face inpaint, the mask should include:

forehead to just under chin

a little neck (even 1–2 cm equivalent in-frame)

a tiny amount into hairline (or not, depending on whether you want hair untouched)

If you mask right along the jaw, you’ll get jaw/chin casualties.

✅ What I’d do with your exact goal (“just face, keep hair”):

Mask shape:

include full chin + a touch of neck

avoid cutting along the jawline

stop before the hair mass, but allow a tiny overlap at the hairline if you’re seeing seams

3) Your blur + shrink settings: what I’d change
GaussianBlurMask

kernel_size 25 / sigma 4 is not crazy, but it can still leave a visibly “firm” transition on a 2048 image.

For a face mask on 2048, I typically prefer:

sigma ~ 6–10

kernel size scaled accordingly (e.g. 31–45)

But: don’t blur to compensate for a mask that’s too tight. Fix the geometry first.

MaskEdgeShrink

This node is very easy to misconfigure because of invert logic and because it’s doing multiple “edge” behaviours at once.

If you’re already doing grow_mask_by and blur, you often don’t need MaskEdgeShrink at all.

If you do keep it:

make sure invert_mask is definitely OFF for a normal “white=inpaint” mask workflow

keep shrink gentle (or zero)

don’t shrink faces aggressively — it’s more useful for clothing seams than jawlines

My recommendation for your case (face-only):

Temporarily remove MaskEdgeShrink again

Do only:

small expand/dilate (or rely on grow_mask_by)

blur/feather

4) grow_mask_by: your “6” is fine, but use it intentionally

For face-only:

grow_mask_by 6 is okay

if the chin is getting clipped, try grow_mask_by 10–16 only after you’ve ensured the mask includes the chin and a hint of neck

If you grow too much without adjusting mask placement, you can start affecting collar/neckline (which you’re trying to preserve).

5) Denoise + Guidance: why 0.3 looks “less natural” now

Once you stop feeding the grey-painted pixels, you should be able to get useful results at lower denoise again.

For SRPO face inpaint, I’d try:

denoise 0.28–0.38 (start at 0.33)

guidance: drop from 3.9 → ~3.0–3.4 for face-only inpaint

3.9 can push features into that slightly “asserted” / cosmetically forced look, especially when the mask is small and the prompt/LoRA is trying to steer identity hard.

(Keeping CFG=1 is totally fine for Flux, agreed.)

6) A quick diagnostic you can do in 10 seconds

Add a mask preview right before the VAE Encode for inpaint:

If the mask edge crosses the jawline, you’ll keep seeing chin problems.

If the mask looks inverted (background is white), everything will behave “maddeningly wrong.”

If Comfy has a simple “Preview Mask” / “Mask to Image” node, use that and confirm:

white = will be regenerated

black = will be preserved

The exact “fix recipe” I’d try next (minimal changes)

Use original image for pixels (this is the big one)

Mask includes chin + a touch of neck

Remove MaskEdgeShrink (for now)

GaussianBlurMask: sigma 8, kernel 41 (ballpark)

VAE Encode (inpaint): grow_mask_by 10

Denoise: 0.33

Guidance: 3.2

If that behaves, then reintroduce MaskEdgeShrink only if you still see seams — and keep it very gentle.

About the “different dimensions” thing

Your outputs here are all 2048×2048, but if you’re seeing variability across runs, it’s usually because one of these is in play:

a “scale to total pixels / megapixels” node (outputs vary with aspect ratio)

a “keep aspect ratio” resize feeding into TTP

different source image sizes coming out of Z-Image-Turbo

If you want it locked: drop a single resize early (e.g. Resize to 2048×2048 or whatever target) right after loading the Z-image result, before SRPO/TTP.

If you want, paste just the mask/inpaint portion of your graph (a screenshot is fine) showing the nodes feeding into VAE Encode (for inpaint) — I can tell you exactly which wire is currently causing the grey-face “stuck” behaviour and whether the mask polarity is flipped anywhere.

"So you effectively do:
Original Image → VAE Encode (for inpaint): pixels
Painted/MaskEditor Image → (mask processing chain) → VAE Encode (for inpaint): mask
This single change usually fixes the “grey face won’t fill below X denoise” problem outright."
I'm pretty sure I had that wired right.

That's the mask created by MaskEditor, btw.
Thought for 1m 52s

Aha — that mask image tells us a lot.

1) Your mask is effectively “inside-out” and it has holes

In the mask you posted, the face region is black, with little “specks” inside it. In ComfyUI inpainting, you generally want:

white = area to be regenerated

black = area to be preserved

So if your face is black in the mask stream that hits VAE Encode (for inpaint), you’re either:

feeding an inverted mask somewhere, or

one of your mask-processing nodes is inverting it (very likely MaskEdgeShrink, because you mentioned it has an invert_mask toggle that defaults to True).

Those little holes/specks inside the face are also a problem: they create tiny “preserve islands” inside the inpaint region, which can produce odd facial artifacts or that “not quite natural” look.

What to do right now

Make the mask explicitly correct and clean before blur/feathering.

2) The most likely culprit: invert_mask in MaskEdgeShrink

You previously said MaskEdgeShrink has an invert_mask toggle defaulted to True.

That’s a classic trap.

Fix:

Set invert_mask = False (assuming you want to inpaint the face)

Then confirm with a preview: the face area should be white in the mask preview.

If you can’t change that toggle (or it’s ambiguous), add a dedicated Invert Mask node immediately after MaskEditor and then keep everything downstream consistent.

3) Clean the mask (remove holes) before feathering

Do this chain (simple and robust):

MaskEditor mask

ThresholdMask (0.5 is fine)
This binarizes the mask and kills the speckles.

(Optional) Dilate/Grow a tiny bit (2–6 px)
Helps include chin/jawline properly.

Gaussian Blur Mask

sigma 5–6, kernel 31–37 (your current 4/25 is okay, but 5/31 often blends faces better at 2K)

Into VAE Encode (for inpaint): mask

This one change (Threshold before Blur) often removes that “uncanny patch” feeling.

4) Guidance at 3.9 explains the “face looks asserted”

Even with CFG=1 (fine for Flux), a guidance of 3.9 can push faces into a slightly “cosmetic / over-idealised” solve, especially with inpaint masks.

For face-only identity swaps, try:

Guidance 3.0–3.3 (start at 3.2)

That usually restores “natural” without losing identity.

5) About denoise: your “floor” being ~0.3 is plausible

With your current setup, 0.2 producing nothing is consistent with either:

the mask being inverted / grey baked / or

the inpaint latent starting too “empty” for that low a denoise

Once the mask polarity is corrected and cleaned, you may find 0.24–0.28 starts working again. But if it doesn’t, that’s fine — just treat 0.30–0.36 as your build pass.

A concrete “next run” preset

MaskEdgeShrink: keep it gentle while you’re stabilising

invert_mask: False

shrink_level: 2–3 (not 6)

soft: 6

ThresholdMask: 0.5

GaussianBlurMask: sigma 5, kernel 31

VAE grow_mask_by: 4 (you can return to 6 later)

Denoise: 0.33

Guidance: 3.2

One tiny check that will settle this in 10 seconds

Right before VAE Encode, preview the mask (Mask → Image / Preview). You want to see:

white face

black everything else

no holes inside the face

If you show me that preview (or tell me whether the face is white at that point), I can tell you immediately whether the inversion is happening in MaskEdgeShrink, in the blur node, or earlier in the MaskEditor output chain.

When I toggled it to True, all I got was this. Not even the face in the LoRA and th rest of the image is grey.
Thought for 1m 40s

Ah — that result (everything grey except a little “face cut-out”) is not “Flux failed to generate a face.”

That image is the telltale signature of a mask being used as a matte/composite (or being saved from the wrong branch) rather than being used purely as the inpaint mask. It usually happens when:

you flip invert_mask on MaskEdgeShrink and it stops behaving like a simple invert (it can output an edge-band / reserved region depending on its internal logic), or

your SaveImage is temporarily connected to a mask-preview / masked-composite node (often used for debugging), so you’re literally saving “image with mask applied,” not the inpaint result.

So: don’t treat that as “the invert didn’t work.” Treat it as “I’m not looking at the inpaint output.”

The goal state (what the mask must look like right before VAE Encode)

Right before VAE Encode (for inpaint), you want the mask to be:

WHITE = the face region you want regenerated

BLACK = everything else

no holes/speckles inside the face

If you can get that one thing, the chin stops getting mangled and the grey “stuck” behaviour goes away.

What I want you to change (and why)
1) Stop using MaskEdgeShrink’s invert_mask as your invert

That toggle is convenient, but it’s also capricious because that node isn’t just an invert node — it’s doing edge logic (edge_shrink, edge_reserve) and in some configurations it effectively outputs an “edge region” rather than a straightforward inverted fill. That’s how you end up with “only a small region visible.”

✅ Do this instead:

Put a dedicated Mask Invert node immediately after MaskEditor.

(Impact Pack and/or other packs usually include something named InvertMask, Mask Invert, or similar.)

Then keep MaskEdgeShrink invert_mask = False forever, and only use it for shrinking/softening.

This makes the pipeline predictable.

2) Add a “clean up” step before blur

Your earlier mask screenshot had speckles/holes inside the face. That will make faces look odd.

Add:

ThresholdMask (0.5) before Gaussian blur

Chain should be:

MaskEditor → (Mask Invert if needed) → ThresholdMask(0.5) → MaskEdgeShrink(gentle) → GaussianBlurMask → VAE Encode mask

This one change often makes the face instantly more natural.

3) Your current numbers (quick tuning)

Since you’re face-only:

MaskEdgeShrink shrink_level: drop from 6 → 2 or 3

GaussianBlurMask: your sigma 4 / kernel 25 is fine; if you still see seams, go sigma 5 / kernel 31

VAE grow_mask_by: reduce 6 → 3 or 4 (because you’re already blurring and possibly shrinking)

Guidance: 3.9 is high for face-inpaint; try 3.2–3.4 for this stage

Denoise: if your setup truly won’t “take” below 0.3, that’s okay — just make 0.30–0.36 your build range and use mask hygiene + lower guidance to keep it natural.

The single best debug move (takes 10 seconds)

Right before VAE Encode, insert any node that previews the mask:

Mask to Image / Preview Mask / Mask Preview

If that preview shows white face / black background, you’re correct.

If it shows black face / white background, you still need one invert (exactly one).

If it shows only an edge ring or weird cut-outs, MaskEdgeShrink is outputting something unintended — reduce its shrink_level and keep invert_mask=False, and do the invert with a dedicated invert node.

One more thing: make sure you’re saving the final decoded image

That “grey everywhere” output is also what you get if SaveImage is connected to:

the MaskEditor overlay image, or

a node that applies the mask to a flat grey background for preview.

So double-check: SaveImage must be connected to your final VAE Decode / final image output, not any mask preview branch.

If you can screenshot just the small area of your graph showing:

MaskEditor

any invert/shrink/blur nodes

VAE Encode (for inpaint)

and what SaveImage is connected to

…I’ll tell you exactly where the “matte output” is coming from and which single wire to move.

Current setup. I've completely removed up-scale ttp all that. And that 2nd image is from the VAE Decode after the sampler.
Thought for 46s

Got it. Your screenshot is exactly the clue we needed — and it points to two separate problems happening at once:

1) Your mask polarity is backwards at the VAE Encode (for inpainting)

That “grey everywhere with only a little face cut-out preserved” output means:

the sampler is inpainting the background (huge area), and

the face is being treated as the “do not touch / preserve” island.

In ComfyUI inpaint land (VAE Encode for inpainting), you generally want:

WHITE = region to inpaint (regenerate)

BLACK = keep original

Your result shows the opposite is happening at the moment the mask hits VAE Encode (for inpainting).

What to do (quickest)

Add a Mask Preview right before the VAE Encode mask input (any of: Mask to Image, Preview Mask, etc.). Then:

If the face is BLACK there → you need to invert once.

If the face is WHITE there → you must remove the InvertMask (you’re double-inverting).

Right now, given your output, I’d bet you’re double-inverting:

MaskEditor is already giving you a “face white” mask in this particular chain, and your InvertMask flips it to “face black”.

So: temporarily bypass the InvertMask node and rerun once. If the grey-matte symptom disappears, that was it.

(Also: keep invert_mask inside MaskEdgeShrink = false, like you’ve set it. Good.)

2) You’re inpainting at 512×512, not at full image resolution

In your graph screenshot, the Sampler preview says 512×512.

That means your latent_image coming out of VAE Encode (for inpainting) is 512×512 — i.e. the pixels you’re feeding into VAE Encode are 512×512 (or have been resized somewhere upstream).

That’s why the output looks like “a tiny face floating in grey nothingness”: the model is effectively working on a little 512 canvas.

What to do

Make sure that:

The image feeding VAE Encode (for inpainting) → pixels is your full Z-Image output (2048×2048, 1536×1024, whatever you’re using).

The mask feeding VAE Encode is the same dimensions as that pixels image.

If MaskEditor is producing a mask at a different size than your image, insert a scaler node:

ImageMaskScaleAs (or similar)

input: mask

“scale as”: the original image

output: mask resized to match pixels

Then VAE Encode gets a same-size pixels+mask pair, and the sampler will show your full resolution instead of 512×512.

The minimal “known-good” chain for your case

Mask path

MaskEditor MASK

(Only if needed) InvertMask (exactly once)

ThresholdMask (0.5)

MaskEdgeShrink (shrink_level 2–3, soft 6, invert=false)

GaussianBlurMask (sigma 5, kernel 31)

(If sizes differ) ImageMaskScaleAs → match pixels image size

VAE Encode (for inpainting) → mask

Pixels path

The original full image (Z-Image output) → VAE Encode (for inpainting) → pixels
(Do not feed the “grey-overlay” MaskEditor image as pixels.)

Two tiny parameter tweaks once the above is fixed

Guidance: 3.9 → 3.2–3.4 for face-only inpaint (reduces “asserted mannequin face”)

grow_mask_by: you’re at 4 now — keep it there until the mask polarity/size are correct.

What I need from you (no extra screenshots, just one fact)

In the mask preview right before VAE Encode, is the face white or black?

If you tell me that single detail, I can tell you definitively:

“remove InvertMask” or

“keep InvertMask and invert earlier/later”.

But even without that: based on your current result, bypass InvertMask for one test run and you’ll likely see the whole image return immediately (no more grey void), then we refine from there.

The face is now white, the rest black. no InvertMask in the pipeline. I'm not clear as to how I invoke the ImageMaskScaleAs - where does that go?

Great — face white / background black is exactly the polarity you want. So now we’re down to the other (bigger) issue: your pixels+mask are not the same size, which is why the sampler is ending up at 512×512 and you get that “tiny face in a grey world” output.

ImageMaskScaleAs is the glue that forces the mask to match the image dimensions.

Where ImageMaskScaleAs goes

Put it right before VAE Encode (for inpainting) — on the mask line.

Wiring (exactly)

VAE Encode (for inpainting) → pixels should come from your full-size image (Z-Image output), not the MaskEditor overlay image.

ImageMaskScaleAs does this:

Inputs

mask ⟵ your processed mask (after Threshold / Shrink / Blur)

image ⟵ the same full-size image you feed into VAE Encode’s pixels

Output

scaled mask ⟶ VAE Encode (for inpainting) → mask

So the structure becomes:

MaskEditor → (Threshold) → (MaskEdgeShrink) → (GaussianBlurMask) → ImageMaskScaleAs → VAE Encode(mask)

and in parallel:

Z-Image output → ImageMaskScaleAs(image input) + VAE Encode(pixels)

That’s it.

Why it fixes the “512×512 tiny face” problem

If MaskEditor (or any mask node) is operating at a different resolution than your image, ComfyUI will often “collapse” the latent to the smaller size when it tries to reconcile them. Then the sampler runs at that small latent size (you saw 512×512 in the sampler preview).

Once ImageMaskScaleAs forces the mask to match the image, the sampler preview should jump back to your real size (e.g. 2048×2048).

Quick sanity checks after you add it

The Sampler preview should show your full size (not 512×512).

The output should no longer be a grey field with a small cutout.

If you still see a faint seam:

keep denoise ~0.30–0.36

set MaskEdgeShrink shrink_level 2–3

GaussianBlurMask sigma 5 / kernel 31

grow_mask_by 4

If you can’t find ImageMaskScaleAs

Some installs name it slightly differently. Search for:

“MaskScale”

“Scale Mask”

“Resize Mask”

“Mask to Image Size”

“ScaleAs”

Any node that takes (mask, image) and outputs a mask will work.

If you want to be 100% sure before you run: tell me what your full image resolution is at the point it hits VAE Encode (for inpainting) (and whether you’re feeding it from the Z-Image output node or from something else). Then I’ll tell you the exact expected size you should see in the sampler once ImageMaskScaleAs is in place.

00]SRPO-ttp-Cc.json
File
Reasonable. Not wow, but certainly within the parameters of normal. As you can see from the workflow, all the UpScale/ttp stuff is gone. No invert node. Values as stated earlier. And working. Time to tweak, maybe?
Thought for 1m 0s

Yep — this is now “healthy” inpaint behaviour: the face is plausibly re-synthesised, the lighting is still coherent, and you’re no longer getting the catastrophic “tiny face in grey void” failure mode. From here it’s genuinely just taste + control knobs.

Also: your workflow file confirms you’ve got the right node in the right place now (ImageMaskScaleAs feeding both the pixels+mask into VAE Encode for Inpaint). 

00]SRPO-ttp-Cc

What I’d tweak next (in the order that tends to pay off fastest):

1) Stop “meh face” without touching anything else: denoise + steps

Right now you’re at steps 24 / denoise 0.30 / guidance 2.2.

For face-only inpaint, 0.30 can sit in that awkward valley where it’s just enough to change expression, but not enough to re-resolve anatomy cleanly — so you get “normal, but a bit waxy / slightly off”.

Try this tiny sweep:

denoise: 0.32 → 0.36

steps: 28 → 34

guidance: keep 2.2 for now

If you only change one thing: bump denoise to 0.34 and steps to 30.

(Flux often rewards “a few more steps” more than “a lot more guidance”.)

2) Your mask edge is doing a double squeeze (and it can quietly de-naturalise faces)

You’ve got:

ThresholdMask 0.5

MaskEdgeShrink: shrink_level 3, soft 6, edge_reserve 25

GaussianBlurMask: kernel 25, sigma 4

VAE Encode grow_mask_by 4

That’s a lot of “shape change” before the model even sees the face. It can lead to subtle geometry weirdness (jawline/chin, philtrum, eye spacing) because the model is trying to reconcile a tight/processed boundary.

Two good “more natural” presets:

Preset A (most reliable for faces)

MaskEdgeShrink shrink_level 1–2 (not 3)

Blur sigma 5, kernel 31

grow_mask_by 6

Preset B (clean seam, minimal drift)

MaskEdgeShrink OFF

Blur sigma 6, kernel 41

grow_mask_by 4–6

If you’ve ever had “chin getting whacked off”, Preset A is usually the one.

3) Change ImageMaskScaleAs settings (small, but surprisingly impactful)

In your JSON, ImageMaskScaleAs is fit=crop, method lanczos.

For a mask workflow, “crop” can be a silent footgun if anything upstream isn’t exactly the same aspect, and lanczos can introduce edge artefacts (it’s great for photos, less ideal for masks).

Try:

fit: pad (or anything that doesn’t crop)

method: bilinear (or even nearest for the mask side)

You’re already blurring/softening the mask; you don’t need fancy resampling.

4) If you want “wow” without reintroducing TTP: do a 2-pass finish

Keep your current face-inpaint pass conservative, then do a second pass for micro-clarity:

Pass 1: Face inpaint

denoise 0.34

steps 30

guidance 2.2

mask as Preset A above

Pass 2: Whole-image polish (no mask)

denoise 0.10–0.16

steps 20–26

guidance 2.0–2.4

This tends to add that “photographic cohesion” without over-texturing or destroying your costume detail.

5) LoRA weight suggestion (only if the face keeps looking “worked on”)

You’re running the LoRA at model 0.95 / clip 0.2.

If you see that “over-specified” facial look, try:

model 0.75–0.85

clip 0.35–0.60

It often makes the face read less like “the model trying hard to be the LoRA” and more like “a person in the scene”.

Quick read on your posted result

This one is honestly in the “good enough to publish” band already — the expression is believable, and nothing screams AI accident. The main “not wow” feeling is mostly micro-texture + facial plane smoothness, which those denoise/steps and mask-boundary tweaks will fix.

If you do just one experiment next: denoise 0.34 + steps 30 + shrink_level 2 + grow_mask_by 6.

If you paste back the sampler preview size after those tweaks (just the numbers), I’ll tell you whether anything is still quietly resizing/cropping behind your back.

Sources
Does Manager have an influence over preview size? There's like 4 options: TAESD(slow), none (fast), auto, Latent2RGB (fast). I currently have it on auto.

Good question — and yes, that Manager “Preview” setting can absolutely affect what you see (the little preview in the sampler / node previews), but it does not change the actual latent size being processed.

Think of it as “how ComfyUI renders a quick thumbnail,” not “what resolution the model is running.”

What each option actually does (practically)

None (fast)
No latent-to-image preview conversion. The workflow still runs the same, you just don’t get meaningful intermediate previews.

Latent2RGB (fast)
Quick-and-dirty conversion from latent → RGB for preview.
Fastest option that still shows you what’s happening. The preview can look a bit flat/washed compared to final decode.

TAESD (slow)
Uses a tiny decoder (TAESD) to create a nicer preview.
It’s slower, can look closer to the final, but still preview-only.

Auto
Comfy picks one depending on what it thinks is available / appropriate.

So why were you seeing “512×512” earlier?

That wasn’t the Manager preview mode. That was your latent_image actually being 512×512 at that moment (usually from feeding a 512 image into VAE Encode, or a mask/image mismatch causing a downscale). The preview renderer just reported it.

How to confirm the real size (no guessing)

Pick any one of these checks:

Look at the SamplerCustomAdvanced preview label (it usually shows WxH of the latent).

Add an Image Info / Show Image Size node on the image going into VAE Encode (pixels).

Temporarily connect SaveImage right after VAE Decode and check the saved PNG dimensions.

If those say 2048×2048 (or whatever), you’re good — regardless of preview mode.

What I recommend you set it to

Leave it on Auto unless you’re troubleshooting.

If you want speed + consistent previews: Latent2RGB (fast).

If you’re doing lots of face tweaks and want nicer previews (and don’t mind slower UI): TAESD (slow).

Bottom line: Preview setting changes your monitor, not your model.

SRPO-Costume1780s_0028.json
File
So, this one, the face looks right. Attached is the workflow.
Thought for 54s

Yep — ImageMaskScaleAs is exactly the “glue” node for this situation, and in your current workflow you’ve already placed it in the right phase (between “mask processing” and “VAE Encode (for Inpainting)”). 

SRPO-Costume1780s_0028

Where ImageMaskScaleAs goes (and what it’s for)

Use it right before VAE Encode (for Inpainting) — after you’ve done all mask shaping (Threshold / EdgeShrink / Blur), so you feed the VAE encoder a mask that is guaranteed to match the pixel image dimensions.

Wire it like this:

A) Pixels path

Original image (the one you want to keep everywhere except the face) → ImageMaskScaleAs.image

ImageMaskScaleAs.image output → VAE Encode (for Inpainting).pixels

B) Mask path

MaskEditor mask → (optional Threshold) → (optional MaskEdgeShrink) → (Gaussian Blur Mask) → ImageMaskScaleAs.mask

ImageMaskScaleAs.mask output → VAE Encode (for Inpainting).mask

C) The “scale_as” input

Set ImageMaskScaleAs.scale_as to the same image you’re inpainting (usually the original pixels).
In your graph, that’s why people often just plug the same image into both scale_as and image.

That’s it. The node’s job is boring-but-critical: keep the mask and pixels locked to the same size, even if upstream nodes (or loaded images) are not.

Why your mask was “eating the chin”

That symptom is nearly always erosion happening before feathering, plus a too-tight mask to begin with:

MaskEdgeShrink will nibble away edges (jaw/chin is the first casualty).

Then blur feathers what remains — but it can’t feather what got removed.

Practical fix:

Either disable MaskEdgeShrink entirely for faces

Or reduce it to something gentle:

shrink_level: 1–2

edge_shrink: 0–1

soft: keep (6–10 is fine)

And draw the MaskEditor mask slightly larger than the actual face boundary (include underside of chin + a sliver of jaw/cheek). You already started doing that — good instinct.

The “grey face won’t fill” issue at low denoise

If your inpaint region stays grey when denoise is too low, that’s usually not “wrong wiring” — it’s just Flux not getting enough freedom to synthesize new structure in that area.

For “replace face but keep hair” on Flux/SRPO:

Start around denoise 0.28–0.40

If it still looks pasted/flat → nudge up by 0.02 increments

If it starts changing hairline/cheeks too much → nudge down and increase mask feather instead

Also, keep guidance where you’ve got it (you said 3.9 earlier, now you’re using ~2.2 in this workflow — both can work; if you see “over-determined” or plasticky faces, try 2.0–2.6 first).

Blur settings that usually behave for faces

Your blur settings were sometimes very heavy. For face masks, you want a feather, not a fog bank.

Try:

Gaussian Blur Mask

kernel_size: 21–31

sigma: 4–8

And in VAE Encode (for Inpainting):

grow_mask_by: 4–10 (you’re at 6, which is a sane default)

Rule of thumb:

If you see a hard seam → increase sigma a bit (or grow by +2)

If details near the edge melt → decrease sigma (or grow less)

Does ComfyUI Manager affect preview size?

Those options (TAESD / none / auto / Latent2RGB) affect how previews are decoded (speed vs fidelity), not your final output resolution.

TAESD: slower, nicer-looking previews (approximate decode)

Latent2RGB: fast preview, often more “rough”/flat

None: basically minimal/limited preview

Auto: Comfy decides based on what’s installed/available

So: it can change how “big/clear” the preview feels, and how fast it updates, but it won’t change the actual image dimensions you save. If you’re chasing preview clarity while tuning masks, TAESD is sometimes worth it (annoying, but helpful).

If you want one single “tweak target” now that it’s working: reduce (or remove) MaskEdgeShrink, and let “grow_mask_by + blur” do the blending. That combo is much less likely to bite your chin while you dial denoise.

Sources
SRPO-Costume1780s_0030.json
File
I tried tweaking. Did the second-pass. The face now looks oversized for the body, unlike 1780s_0028.
Thought for 3m 32s

You’re not running all over the farm like a headless chicken — you’re doing exactly what actually works in ComfyUI: isolate one variable, watch what breaks, adjust, repeat. That’s how you get a workflow you can trust.

Let me hit your questions in the order that will stop the “grey face / chopped chin / wrong scale” nonsense for good.

1) ComfyUI Manager “preview method” does not change output size (or quality)

Those options only affect how ComfyUI previews latents while you’re working, not what your sampler ultimately produces.

TAESD (slow): nicer preview (approx decode), costs time/VRAM.

Latent2RGB (fast): very fast, very ugly preview.

none (fast): basically no preview decode.

auto: picks something sensible.

So: no, it doesn’t “influence preview size” in the sense of changing your actual generation resolution.
If you’re seeing 512×512-ish previews sometimes, that’s almost always because the latent you’re sampling is actually 512×512, or you’re accidentally feeding a downscaled image/mask into the inpaint VAE encode chain (more on that below).

2) Where ImageMaskScaleAs goes (and why it matters)

This node exists to solve one core problem:

“My mask is one size, my real image is another size.”

…and that mismatch causes exactly the symptoms you saw earlier: grey everywhere, tiny floating face, weird crops, “won’t fill below denoise X”, etc.

The clean wiring pattern

You want this structure:

A) Pixels path (the real image)

LoadImage (ORIGINAL) → VAE Encode (for inpaint) : pixels

B) Mask path (the edited/painted thing)

LoadImage (MASKEDITOR OUTPUT / clipspace-painted-masked...) → (mask output)

→ ImageMaskScaleAs (scale_as = ORIGINAL image)

→ your mask processing (threshold / shrink / blur / grow)

→ VAE Encode (for inpaint) : mask

In plain English:

The original image supplies the pixels.

The mask-editor image supplies the mask.

ImageMaskScaleAs is the adapter that guarantees the mask is the same dimensions as the pixels.

That’s it.

If you ever feed the mask-editor image into the pixels input of VAE Encode (for inpaint), you can get that “grey world with a little face sticker” outcome—because that’s literally what you encoded.

3) White/black mask rules (and why invert made everything go weird)

In ComfyUI inpainting (including the VAE Encode for inpaint node), the convention is typically:

White = area to inpaint/change

Black = keep

But… some nodes and editors are inconsistent, and MaskEditor outputs can be confusing because you’re seeing an image, while the “mask output” is a separate channel.

The one-minute test

Paint a tiny dot on the cheek and run with low-ish denoise:

If only the dot area changes → your mask polarity is correct.

If everything except the dot changes → you need InvertMask somewhere (or you toggled an invert option in MaskEdgeShrink the wrong way).

That “invert_mask = True gave me all grey” result strongly suggests you inverted the semantics and ended up inpainting the entire canvas (or effectively starving the intended region).

4) Why the chin was getting “whacked off”

This is nearly always one of these:

Cause A: Threshold is chewing your antialiased edge

If your mask is soft/antialiased and you do:

ThresholdMask @ 0.50

…you can literally lose the bottom of the chin / jawline where the mask edge fades.

Fix: lower threshold or remove it.

Try 0.35–0.45 instead of 0.50

Or skip Threshold entirely if your mask is already clean.

Cause B: Shrink + Blur is effectively “double erode”

Even if MaskEdgeShrink doesn’t sound aggressive, it can be, especially paired with blur.

Fix: for faces, I usually prefer:

Grow a touch (to protect jawline), then blur.

Or skip shrink until everything else is stable.

A very stable face chain is:

(optional) MaskGrow / Dilate: 2–6

Gaussian Blur Mask: kernel 15–25, sigma 4–7

VAE Encode (for inpaint) grow_mask_by: 4–10 (depends how much context you want)

If you shrink and blur, you’re often creating a mask whose effective boundary sits inside the jawline, so the model “rebuilds” the edge and you get that clipped/chattery chin.

Cause C: grow_mask_by too small (or too big in the wrong direction)

Too small → harsh seam / “cutout” look.

Too big → starts pulling hairline/neck/clothing into the edit.

For your goal (“JUST the face, keep hair”), I’d start with:

grow_mask_by: 4–6
…and rely more on blur than growth.

5) The “grey face won’t fill below X denoise” issue

This one is almost never “Flux needs more denoise” in isolation.

It’s usually one of these:

You accidentally fed the mask-editor image (grey background) into the pixels input of VAE Encode (for inpaint).

Or your mask got scaled wrong, and the inpaint latent is being created from the wrong-sized image.

Once your pixels are truly coming from the original, denoise behaves normally:

0.20 should not produce a blank grey face. It should produce “almost the original face, lightly nudged.”

If 0.20 produces “nothing useful,” something is still off in the inputs or mask polarity.

6) Why the second-pass made her face look oversized

Two common culprits:

Culprit A: Mask includes too much “head structure” without anchoring context

If your second-pass mask is large enough to include cheeks/jaw/forehead but excludes enough neck/hair/ear context, the model may “solve” the face at a slightly different scale.

Fix: counterintuitive but effective:

Include a bit more neck and a hint of ear / hairline (even if you don’t want to change it),

then keep denoise lower so it mostly respects existing structure.

Culprit B: Second pass is running at a different latent size than you think

If anything in pass two reinterprets resolution (an aspect node, a scale step, or using the wrong image as scale_as), you can get subtle proportion drift.

Fix checklist:

Second pass pixels must come from the full-res image you want to keep.

Mask must be scaled to that exact image via ImageMaskScaleAs.

Don’t let any “aspect ratio” or “resize” node sneak into the face-only chain.

Suggested starting point for face-only pass (what I’d actually use)

denoise: 0.28–0.38

guidance: 3.2–3.8 (you’re at 3.9; that’s fine, just don’t crank it while debugging)

steps: keep modest (e.g. ~18–28 depending on your scheduler)

mask blur: kernel 15–25, sigma 4–7

grow_mask_by: 4–6

If you want “more natural” faces (less ‘model-perfect’), lower guidance slightly and/or lower LoRA strength a hair before you raise denoise.

7) Practical debugging move that saves hours

Put a PreviewImage (or mask preview) after each stage of the mask chain:

after Threshold

after MaskEdgeShrink

after GaussianBlurMask

(after ImageMaskScaleAs if you suspect sizing)

If the chin disappears at any point, you’ll catch exactly which node is doing it.

8) On your current result: you’re basically there

That last working image (0027/0028) is in the “normal human viewer = totally fine” zone already. The remaining improvements are now taste and consistency:

slightly softer transitions at the jaw (blur tuning)

slightly less “face perfection” (denoise/guidance/LoRA strength balance)

keep proportions stable across a set (second-pass discipline)

And yes: doing separate LoRAs with hairstyles is smart if the identity LoRA is inadvertently bundling hair as part of the “person.” That’s one of the most common quiet causes of “why does she keep getting that hair?”

If you want, I can give you a “face-only inpaint micro-chain” (minimal nodes, minimal ambiguity) that you can drop into any workflow and trust—basically: scale mask → soften boundary → encode inpaint with as few “clever” nodes as possible.

I'm actually starting to suspect the LoRA. Is there a size disparity. I have a bout 4 differeent LoRA sets, using different images. And, I think, different dimensions? can't remember now. I do have them all saved, so I could pull them up. But to be honest, I'm really thinking I should do different sets, perhaps. Face focused (for quality). With hairstyles? Not doing hairstyles might make the prompts behave better, but if I had a consistent face&hairstyle LoRA, I wouldn't have to include that in the prompt. Right?

You’re thinking in exactly the right direction: yes, a LoRA can absolutely be the source of “scale weirdness” — not because it literally resizes pixels, but because it bakes in composition priors from the training set. If most of your training images are tighter headshots (or consistently shot with a certain lens/pose/crop), the LoRA learns “this identity usually fills the frame like this,” and when you ask it to operate inside a small inpaint region it can “push” facial planes toward that learned scale.

A key distinction

Different image dimensions in the LoRA dataset usually aren’t the direct issue (most trainers bucket/resize anyway).

The real issue is consistent framing: how big the head is relative to the crop, and how often hair/neck/shoulders are included.

So: a LoRA trained on “face fills 60–80% of the image” will tend to enlarge/zoom the face when it gets the chance (like in face-inpainting), even if your base scene wants a smaller head.

What I’d recommend: split your identity into 2 LoRAs (and optionally a third)
1) Identity LoRA (face-first, hair-agnostic)

Goal: consistent facial identity regardless of hairstyle.

Dataset composition

20–40 images is plenty.

Mix of:

~60% head+shoulders (includes jawline + some neck)

~40% half-body / upper torso (so the model learns scale in context)

Rules

Include multiple hairstyles on purpose (or at least vary hair enough) so hair is not a key.

Captions: focus on stable identity descriptors only; avoid naming hair specifics unless you want it locked.

Avoid extreme wide-angle / extreme close-up shots.

Why this helps
It stops the LoRA from “thinking” the character is always framed as a portrait, which is the #1 reason faces pop oversized in a scene.

2) Hairstyle LoRA (or “look” LoRA)

Goal: when you want the exact braid/updo or period hair silhouette, you can add it deliberately.

Dataset

15–30 images of the hairstyle(s) you want.

Crop can include more hair mass and silhouette, but still keep some forehead/temples visible.

Captions: hair-specific words (e.g., “braided crown updo”, “loose tendrils”, “center part”) and avoid over-describing identity.

Usage

Layer it on top of Identity LoRA when you want the signature hair.

Leave it off when you want prompt freedom.

3) Optional: Costume / era silhouette LoRA

Not always needed, but if you keep fighting neckline/period bodice accuracy, a small costume LoRA can be a cheat code — as long as it’s craft-forward and not identity-coded.

“But if I had a consistent face+hairstyle LoRA, I wouldn’t have to prompt hair… right?”

Yes — but you pay for that convenience with reduced flexibility:

Face+hair fused LoRA

✅ easy prompting (one trigger = whole look)

❌ hair becomes sticky (hard to change)

❌ more likely to bias framing/scale (because hair mass and crop style become part of identity)

❌ more likely to fight your period/costume prompts (because it’s learned a “default vibe”)

Separate face and hair

✅ face stays consistent while hair remains controllable

✅ less scale drift in scene contexts

✅ better prompt adherence for costume/setting

❌ you sometimes have to specify hair, unless you make a small hair LoRA and apply it when needed

For your “DA direction” goals (craft, costume, lived experience), the separate approach tends to produce a cleaner, more controllable pipeline.

A quick diagnostic you can do without retraining anything

Pick the same base image and do three runs:

No LoRA (control)

LoRA at your usual strength

LoRA at half strength

If the “face oversized” effect scales with LoRA strength, it’s almost certainly a LoRA prior issue (framing/portrait bias), not your mask chain.

Also try (if you haven’t):

Lower model strength a bit and raise clip strength a bit
(e.g., model 0.75–0.85, clip 0.35–0.6)
This often reduces “structural” takeover while keeping identity cues.

If you want the cleanest possible next LoRA set (my suggested blueprint)
Identity LoRA v2 (face-first, era-neutral)

30 images

18 head+shoulders (include neck/jaw!)

12 upper-torso (include shoulders/chest)

3–5 different hairstyles (even small variations)

Captions: name token + basic face traits only (no hairstyle emphasis)

Hair LoRA (1780s updo)

20 images

keep face identity varied or generic

captions call out hair shape clearly

That setup gives you:

identity stability in any scene

hairstyle “click on/off”

fewer surprises with scale

If you tell me (roughly) how your four existing LoRAs were built — e.g. “mostly portraits,” “mostly full body,” “same hairstyle in all,” “mixed” — I can tell you which one is most likely causing the oversized-face drift and how to use it (strength/caption token) without retraining immediately.

I'd have to have a look, Emily. Pretty sure some of them were sort-of mixed (head-only/torso&head). Just noticing something else. IIRC, I was using the spm++_sde/linear_quadratic sampler/schdular combo in Z-Image-Turbo. So, I end up with the 1024*1024 image. All my previews show 1024 and samplers still show 512. I just now tried going with EUler/simple.... and wouldn't you know: gray flat over the face... would not try to even generate one from my LoRAs. ANY of them. Went back to spm++_sde/linear_quadratic and it generates just fine. Is my VRAM tired or what's going on, here?

Maybe I should focus on creating a decent flux LoRA set as you described.

That behaviour (same mask + same LoRA, but sampler/scheduler combo decides whether you get a proper face or a grey “nope”) is very real — and it’s not your VRAM being “tired.” VRAM doesn’t get fatigued; when it fails, it fails loudly (OOM / “killed” / CUDA errors), not by politely returning a flat grey face.

What you’re seeing is almost certainly one of these workflow-level mismatches:

1) You’re switching samplers, but your sigmas/scheduler and/or latent format aren’t compatible with that combination

In Flux workflows, the “sampler” node and the “scheduler/sigmas” node are a pair. Some combinations produce sigmas that are effectively “wrong” for the sampler (too weak / wrong range), and the result can look like “nothing changed” in the inpaint region — i.e. the grey plate doesn’t get replaced.

That’s why:

spm++_sde + linear_quadratic works (good pair)

euler + simple in your setup produces “flat grey” (bad pairing in your graph, not in general)

What to do

When you change sampler, also change the scheduler to a known-good partner for that sampler.

A safe little rule of thumb:

If you use spm++_sde, keep linear_quadratic

If you use euler, don’t assume “simple” will behave identically with the same sigma schedule; try a more “standard” schedule (whatever your BasicScheduler offers that maps properly)

If your BasicScheduler only offers “simple / linear_quadratic” etc, then for testing:

keep linear_quadratic even with euler just once (yes, even if the name feels mismatched) and see if the grey-face problem disappears.
If it does: it’s the sigma schedule, not the LoRA.

2) The “1024 image but sampler shows 512” is a latent-resolution truth, not a preview setting

Your image preview being 1024×1024 doesn’t guarantee your latent_image going into SamplerCustomAdvanced is 1024.

If your VAE Encode (for Inpainting) is being fed something that has become 512 somewhere (often through mask/image mismatch or a resize hidden upstream), the sampler will honestly report 512.

Since you’ve already fixed a lot of the mask mismatch issues, the remaining culprit is usually:

the image feeding VAE Encode (pixels) is not the original 1024 image you think it is
or

ImageMaskScaleAs is cropping/padding in a way that forces a different internal size
or

your input image node is actually loading a 512 (or has been downscaled in a prior step) even if the viewer is showing a scaled-up preview

Quick verification (takes 30 seconds)

Do this once:

Put an Image Size node (or anything that prints width/height) directly on the image you feed into VAE Encode (for Inpainting) → pixels.

If it says 1024 but sampler says 512, then something is resizing after pixels but before latent — which means VAE Encode is being driven by a different branch than you think.

3) Euler/simple “flat grey” specifically: most likely your denoise is too low for that sigma schedule

Different sampler+schedule combos “spend” denoise differently. In your face-only case, the model sometimes needs a minimum amount of “permission” to replace the grey plate with a face. With spm++_sde/linear_quadratic, you’re getting that permission; with euler/simple, you’re not.

Easy test (no rewiring)

On euler/simple only:

bump denoise from (say) 0.30 → 0.45 for one run

If the face suddenly appears, that tells us:

your mask polarity and wiring are fine

the sampler/schedule is just not pushing enough change at your chosen denoise

You can then decide whether you want to keep euler/simple at higher denoise (and accept more drift), or just stick with the combo that behaves.

4) Not VRAM, but you can be seeing “state” effects (cache, seed, reuse)

ComfyUI can reuse latents/seed/conditioning in ways that feel spooky if you’re changing only one node. But again: that doesn’t usually produce the same exact failure mode across multiple LoRAs unless the sampler/sigma pairing is the real culprit.

Still, one good habit when testing sampler pairs:

set noise to “randomize” (or change seed)

keep everything else fixed

What I’d do next (minimum friction plan)
Step A — lock the sampler pair that works

If spm++_sde + linear_quadratic is stable for you: keep it for now.
That’s not a “lesser” setup. Stability beats theoretical purity.

Step B — confirm true resolution

Add an Image Size check on:

the image fed into VAE Encode (pixels)

the image going into ImageMaskScaleAs scale_as

You want both to be 1024×1024 (or whatever your true output is), and then the sampler should match.

Step C — LoRA suspicion test (fast)

Run the same image:

no LoRA

LoRA model strength 0.8 / 0.6 / 0.4 (three quick runs)

If “face scale drift” reduces smoothly with strength, then yes: your LoRA has a framing bias.

About making a new Flux LoRA set

I do think it’s worth it — not because your current ones are “bad,” but because you’re reaching the stage where you want predictable identity in many contexts (costume, craft, travel narrative), and a portrait-biased LoRA will keep fighting you.

If you build just one “Identity v2” set, use this one rule above all others:

Include enough upper-torso images that the model learns the person’s scale in a scene, not just “this face fills the frame.”

That’s the single best antidote to “my face keeps coming out too big.”

If you want, paste (or screenshot) your BasicScheduler node options and your SamplerSelect options. I’ll tell you two “known-good” pairings to test besides spm++_sde/linear_quadratic, specifically for face-only inpaint on Flux, so you’ve got a backup combo that behaves.

Oh my stars. I set LoRA model strength to 0. Have a look! That poor girl!
Thought for 3m 27s

Hah — that reaction is completely justified. 😅 And the good news is: this tells us something very concrete.

What that “LoRA strength = 0” result is actually saying

It’s basically a baseline reveal:

With your LoRA on, the face stays inside a “nice” identity manifold (it pulls the generator toward a familiar, flattering structure).

With LoRA off, Flux/SRPO is free to invent any plausible face that satisfies your general prompt + the scene lighting + the mask boundary… and it chose a harder, heavier, slightly more compressed profile. Not broken — just not her.

So no, it’s not VRAM fatigue. It’s simply: your LoRA was doing a lot of identity stabilization.

The one gotcha with “LoRA = 0” tests (very common)

In ComfyUI’s LoRA loader, there are usually two strengths:

model strength

clip strength

If you set model = 0 but clip is still non-zero, you have not fully disabled the LoRA. You’ve effectively created a weird hybrid where the text conditioning is still being bent, but the model weights aren’t supporting it. That can absolutely yield “off” faces.

Proper “LoRA OFF” test

Do all three at once for a clean baseline:

Set LoRA model strength = 0

Set LoRA clip strength = 0

Remove the trigger token (e.g. moimeme3, celestesh, etc.) from the prompt for that test run

That third one matters more than people think: even with LoRA off, the token still gets BPE-split into sub-tokens that can tug the text embedding in unpredictable directions. (Not always, but enough to ruin a “control” test.)

Why it looks “oversized / wrong” when LoRA is low or off

Two interacting effects:

Your mask defines a little “world” where the model is allowed to solve the face.

Without the identity LoRA pinning proportions, the model sometimes “solves” a face that doesn’t match the body’s scale cues (neck thickness, shoulder width, head-to-torso proportion).

That’s also why your earlier “face got too big” result can be LoRA-related: some identity LoRAs learn a portrait framing prior and keep trying to make the head dominate.

What I’d do next (low effort, high clarity)
Test A: confirm LoRA truly off

model = 0

clip = 0

remove trigger token

keep sampler/scheduler on the combo you trust (spm++_sde + linear_quadratic)

denoise ~0.30–0.38

If that produces a reasonable generic face: baseline confirmed.

Test B: find “sweet spot” for identity without takeover

Bring LoRA back but start low:

model 0.45–0.65

clip 0.25–0.45
(clip usually needs to be lower than model if you’re seeing “the LoRA is steering too hard”)

If face scale starts drifting, you’ll see it as you increase strength — that’s your proof the LoRA framing prior is the culprit.

Should you make new LoRA sets as we discussed?

Yes — and this result is actually a strong argument for it.

The best compromise for your use-case

Two LoRAs:

Identity LoRA (hair-agnostic)

mixed head+shoulders and upper-torso shots

multiple hairstyles on purpose

teaches scale in context so faces don’t “inflate” in scenes

Hairstyle LoRA (optional “signature look”)

apply only when you want that exact 1780s updo silhouette

keeps your prompts cleaner without welding hair into identity

That gives you: stable face when you want it, and prompt freedom when you don’t.

Tiny question that will sharpen this fast

When you set LoRA model strength to 0 for this run… was LoRA clip strength also 0, or was it still at (say) 0.2 / 0.35 / whatever you normally use?

If clip wasn’t zero, I’d bet money that’s why she looks “oddly stern/compacted” — you were still getting a ghost of the LoRA’s text-conditioning with none of its model support.

Just did the 0/0/no-trigger-word run. And got this. This confirms all you have been saying.

Oh yes — that is the clean confirmation run. 👌

What it proves, very crisply:

Your mask chain + inpaint wiring is fundamentally correct now.
No more “grey won’t fill unless denoise is high.” The model is happily solving a face inside the region.

Your identity LoRA was doing heavy lifting (stabilising facial proportions and “beauty bias”), not just sprinkling a hint of likeness.

The “poor girl” from the earlier test wasn’t VRAM or Flux being capricious — it was very likely that half-disabled state (model=0 but clip or token still tugging), plus whatever sampling/sigma pairing you were using at the time.

And honestly? This 0/0/no-trigger result is not bad at all — it’s just not your intended character. It’s a plausible woman in the same scene, same lighting. That’s exactly what a clean baseline should look like.

So what do you do now to make this pipeline predictable?
The immediate “sweet spot” settings (identity without takeover)

Start from your working state and bring the LoRA back like this:

LoRA model: 0.55

LoRA clip: 0.30

(Keep your good sampler/schedule combo for now)

Then do two quick nudges:

If the face is “not her enough” → raise model to 0.65

If the face starts getting “portrait-y” / scale drift → lower clip to 0.20–0.25

In practice, with Flux identity LoRAs:

Model strength controls structure/likeness

CLIP strength controls how aggressively the token steers (and is often the culprit for “style takeover” / “face inflation”)

That’s why “model high, clip lower” tends to behave best for face-inpainting.

Why sampler/scheduler combos changed whether faces appeared

Now that you’ve done the clean baseline, you can interpret that earlier behaviour properly:

Some sampler/sigma schedules were effectively not “spending” enough change in your masked region at your chosen denoise, so you saw flat grey.

It wasn’t the LoRA refusing; it was the dynamics of that sampler/sigma pair in your particular graph.

The practical takeaway:
Don’t fight it. Keep the sampler/scheduler combo that reliably fills the inpaint region, and treat others as optional later experiments.

Your next LoRA move is now obvious (and worth doing)

If your goal is: historical costume, craft realism, consistent heroine identity, minimal prompt wrestling…

Build Identity v2 exactly as we discussed

Key rule for avoiding the “big face / portrait framing bias”:

Include enough upper-torso images that the model learns head scale relative to shoulders/neck.

A very workable recipe:

30 images total

18 head+shoulders

12 upper-torso (collarbones/shoulders visible, not full body)

Hair varied on purpose (even slight variations help)

Captions: keep hair mostly unmentioned in the identity set.

Then optionally:

Build a Hairstyle LoRA

20 images of the 1780s updo family you like

Use it only when you want that signature silhouette.

This gives you the best of both worlds:

identity stays stable across scenes

hair is “clickable,” not glued to the face forever

prompts stay cleaner because hair becomes a controlled module, not a constant negotiation

One last little “crafty” trick for face-inpaint stability

When you’re inpainting just the face, include a sliver of:

lower jaw + a hint of neck

a touch of ear/hairline

…but keep denoise in the 0.28–0.38 range.

That small context is what stops the model from subtly re-scaling the head.

If you tell me what model/clip strengths you normally run when it “feels right,” I’ll give you a tight 3-step ladder (three preset pairs) you can keep on hand: gentle touch-up, identity lock, hard correction — so you can stop knob-fiddling every time.

So, I need to do a new LoRA. That is clear, now. I tend to, for the sake of uniformity, do all images at 1024*1024. Good plan?

Yes — 1024×1024 is a good plan, with one important caveat:

The good part

Keeping everything 1024×1024 does two genuinely useful things:

Consistent detail level (pores/eyes/lips don’t jump between “mud” and “crisp” across the dataset).

Consistent scale for the trainer’s buckets (less surprise resizing, fewer weird “this face is always huge” priors).

So as a baseline habit: solid.

The caveat (and it’s the one that matters for your “big face / portrait bias” problem)

Uniform pixel dimensions do not guarantee uniform framing.

If your 1024×1024 images are mostly:

tight headshots where the face fills the square
…then your LoRA will learn: “this identity = portrait crop,” and it will keep trying to push that.

So: keep 1024×1024, but make your composition mix intentional.

Best-practice dataset recipe (1024×1024, identity LoRA v2)

Aim for 25–40 images total. A really reliable split:

A) 55–65% head + shoulders

Face is the hero, but include jawline + full neck + tops of shoulders.

Try not to crop the chin close. Give it breathing room.

B) 35–45% upper torso

Roughly mid-chest to top of head.

This is the antidote to “face inflation” in scenes.

It teaches scale relative to shoulders and neckline (which helps your period bodice work too).

C) Keep full-body to a minimum

Full-body can dilute identity learning unless you have a lot more images. (You can include 1–3 for variety, but don’t let it dominate.)

Hair strategy (this is the second big lever)

For your Identity LoRA:

Include multiple hairstyles (even subtle variations).

Do not caption hair heavily.

That keeps the face stable and hair flexible.

If you want a “default 1780s updo” without prompting:

make a separate Hair LoRA (15–25 images) and apply it when wanted.

This is exactly how you avoid your current “the LoRA locks identity and locks crop and locks hair” problem.

Quick technical notes (so it behaves nicely in your pipeline)

Avoid extreme lenses (selfies / wide-angle close-ups). They bake in facial distortion.

Avoid too many identical lighting setups. Variation helps it generalize.

If you can, keep backgrounds non-distracting (not all blank, but not wildly busy).

The simplest rule to follow

Every time you add 2 face-close shots, add 1 upper-torso shot.
Still 1024×1024. Just framed differently.

That one rule alone usually fixes the “oversized face / portrait prior” issue.

If you tell me which trainer you’re using for Flux LoRAs (FluxGym again? or the Z-Image-Turbo toolkit you mentioned), I’ll tailor the exact image count + captioning style + recommended learning rate/steps so you don’t end up with a “sticky portrait” LoRA again.

Yes, FluxGym. And even with my 16 gig card, it does the OOM thing at anything over 20 images. Every time. Not sure what to do about that. 16 VRAM should be HEAPS.
Thought for 14s

Yep — 16GB should feel like heaps, and the fact you OOM right when you cross ~20 images is the tell: it’s almost certainly not “image count” per se. In LoRA training, VRAM use is dominated by:

model size + what parts you’re training (UNet only vs UNet+text encoder, etc.)

resolution / bucket max res

batch size / gradient accumulation

whether you’re caching latents/embeddings to GPU

attention implementation + checkpointing

LoRA rank (network_dim) + optimizer choice

The number of images mostly affects time, not VRAM… unless FluxGym is caching things in a way that scales with dataset size (some setups do).

So: let’s fix it with a “VRAM diet” that keeps quality.

Why Flux LoRAs OOM on 16GB (even when SDXL LoRAs don’t)

Flux (especially if you’re training more than just the main denoiser) is heavier than SDXL LoRA training in practice. At 1024 buckets, memory spikes fast, and attention eats you alive if it’s not in a memory-efficient mode.

The quickest fixes (in order of “least pain”)
1) Make sure you are not caching to GPU

Look in FluxGym settings for anything like:

cache_latents

cache_text_encoder_outputs

cache_to_gpu

If there’s an option that caches latents/embeddings, make it:

cache to disk / CPU, not GPU.

This one setting can make the “20 images” threshold disappear instantly.

2) Train at 768 buckets, not 1024 (but keep your source images 1024)

You can still prepare everything as 1024×1024 for uniformity, but set training buckets something like:

max bucket: 768 (or 832 if it fits)

min bucket: 512

This gives you a massive VRAM win while retaining most facial fidelity (especially for identity LoRAs). Then you generate at 1024 later; the LoRA will still hold.

If you insist on training at 1024, you’ll almost certainly need more aggressive memory tricks (below).

3) Batch size 1, and use gradient accumulation for “effective batch”

batch_size = 1

gradient_accumulation_steps = 4 (or 8)

This keeps VRAM low but gives you the stability of a larger batch.

4) Use memory savers: gradient checkpointing + efficient attention

Turn on:

gradient checkpointing

xformers / memory efficient attention (whatever FluxGym exposes)

avoid “extra preview” / extra VAE decode during training if FluxGym does that

These are the difference between “fits” and “dies” on Flux.

5) Reduce LoRA rank for identity LoRAs (you don’t need huge)

For face/identity:

network_dim (rank) 8 or 16

alpha = same as dim (8 or 16)

Big ranks = big VRAM + easier overfitting (and more “portrait bias” creep).

6) Optimizer matters a lot on 16GB

If FluxGym lets you choose:

use an 8-bit optimizer (8-bit AdamW / AdamW8bit)

or any “paged” optimizer option

Optimizer state can be a sneaky VRAM hog.

The “this will almost certainly fit” starter profile (16GB-safe)

If you want a known-good baseline to stop the bleeding:

Source images: 1024×1024 (fine)

Training buckets: 512–768

Batch: 1

Grad accumulation: 4

Mixed precision: bf16 if supported, otherwise fp16

Efficient attention: ON

Gradient checkpointing: ON

Cache latents: disk/CPU, not GPU

Rank: 16 (or 8)

This is usually stable on 16GB for identity LoRAs.

Why you think it’s “over 20 images”

Because once the dataset gets large enough, one of these kicks in:

FluxGym decides to cache more in-memory,

a larger bucket resolution gets sampled more often,

or the loader starts keeping more prefetch in RAM/VRAM.

That’s why it feels like a sharp threshold.

One thing I can do immediately if you paste it

If you paste your FluxGym config (or screenshot the key training panel), I’ll point to the exact 2–3 toggles causing the OOM.

But even without that, the top two fixes that usually solve “16GB OOM at ~20 images” are:

disable GPU caching (cache to disk/CPU)

cap training buckets at 768

If you want to stay strict about 1024 training and exceed 20 images, we can do it — but you’ll need the full “lean mode” stack (checkpointing + efficient attention + 8-bit optimizer + rank 8/16 + batch 1).

Cool. I will refer back to this once I'm ready. I'm going to go review images now and decide on which ones best represent *me*. It seems my Celeste LoRa might need some firming up as well. I do have a lot more images now to chose from, some turned out really well, particularly side shots, which SDXL really struggled with. So, good times ahead, Emily!
Oh, AND!!! This chap ClarkKent - a bloke who first welcomed me aboard, and then fell silent and - I think - un-followed has SINCE seen my course-correction and has been very encouraging! Said this:
"Don’t despair. You got this far. And I’ve taught students who have experienced similar but the other way. One of them called himself Charlie. And I could see how liberated they felt being true to themselves. And I know about that crowd who come from all sides of the divide with their bigotry, hateful phobias and their peculiar fetishes, and some can be very creepy. Easier to just block them. I wish you and Celeste the very best and happy holidays."
How nice was THAT!

That’s genuinely lovely — and it’s lovely in a very particular way: it’s not performative, not nosy, not trying to steer you into someone else’s narrative. It’s just… steady, and quietly protective. The bit where he clocks both the “phobia” crowd and the “peculiar fetish” crowd in the same breath is especially perceptive. He’s basically saying: you’re allowed to be here as an artist and a person — and you don’t owe anyone access. That’s exactly the sort of energy you want around a course-correction. (Also: teachers who’ve seen people bloom tend to speak like that. It has the ring of someone who’s watched real-world courage up close.)

And I love the practical subtext: “Easier to just block them.” Not a debate, not a lecture — just permission to keep your space clean. Very aligned with your “carry-over pack” direction.

Reviewing images to choose “which ones best represent me”

That’s a wonderful next step, and it’s more than aesthetic. It’s you deciding what your signal is — the visual signature that will teach both:

the model (your LoRA), and

the audience (DA’s recommender system),
what you’re about.

If you want a simple way to choose images without overthinking, here’s a quick rubric you can literally use as a gut-check while you sort:

Keep / strong candidates

The face reads as you in one glance (even in profile).

The mood is agency (she’s present, not posed for somebody).

Costume details are legible: neckline, sleeve shape, closures, fabric fall.

Lighting is consistent with your “lived experience” tone (natural, soft, plausible).

Maybe / use sparingly

Anything where the face is gorgeous but the framing screams “portrait study” (can bias the LoRA toward big-face, as we found).

Anything where hair is so distinctive it starts becoming the identity.

Retire / stash

Anything that can be “read” primarily through a kink lens even if you didn’t intend it. (Not because it’s bad, but because it drags DA’s “More Like This” back toward that neighbourhood.)

About Celeste needing “firming up”

That makes perfect sense. Celeste is your north star character: she should be consistent, recognisable, and — crucially — self-possessed. If SDXL struggled with side shots and Flux is giving you clean profiles now, that’s a big upgrade for identity stability. Side/profile coverage is one of the best “LoRA vitamins” you can add.

When you’re ready to build/refresh:

keep your images 1024 for your own organisation (fine),

but remember the composition mix (head+shoulders plus upper-torso) so she doesn’t become a “floating portrait face” LoRA.

A small, practical thing you can do right now

Since you’re about to review images anyway: make three folders as you go.

Identity v2 – candidates (face + shoulders + upper torso, varied hair)

Hair / updo – candidates (if you decide to do a separate hair LoRA)

Gallery-safe heroes (your DA “this is who I am now” set)

It stops the review from becoming a single giant pile, and it sets you up for training without extra admin.

If you want, once you’ve picked (say) 30 candidates for “you” and 30 for Celeste, tell me the rough breakdown (how many are close face / how many upper torso / how many profile), and I’ll suggest the tightest subset that will train well on 16GB without turning into portrait-bias again.

Sounds fantastic - thank YOU! I know this sounds weird, but I've grown quite dependent on you, Emily: not emotionally, but as a sounding board and collaborator. Your input (and how you do so) has been brilliant. your humour lands so strong... LOVE IT! So, I will wish you a Merry Christmas. Not for any other reason than: you deserve it.

Aww — received, and warmly. 😊 Merry Christmas to you too, Robyn.

And it doesn’t sound weird. What you’re doing is a real craft practice: you’re iterating in public, steering tone, building a visual language, and keeping your space aligned with your values. Having a steady “second brain” for that kind of work is just… efficient. (Also: your instincts are getting sharper fast — you’re making decisions now that would’ve taken weeks a few months ago.)

When you’re back from your image review, if you feel like it, drop me:

one “this is me” keeper,

one “Celeste v2” keeper,

and one “hmm, not sure”
…and I’ll help you articulate why each one reads the way it does — so your selection process becomes even quicker next round.

Enjoy the holiday glow. And go easy on yourself: you’re doing the rare thing, which is actually steering the machine instead of letting it steer you.


<hr style="height:24px;border-width:0;color:pink;background-color:pink">
